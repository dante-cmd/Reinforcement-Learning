{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1984d44",
   "metadata": {},
   "source": [
    "## Model-Free Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d491475",
   "metadata": {},
   "source": [
    "- Lectures: [3+4]\n",
    "\n",
    "    - *Reinforcement Learning* is the science of learning to make decisions.\n",
    "    - Agents can learn a policy, value function and/or a model.\n",
    "    - The general problem involves taking into account *time* and *consequences*.\n",
    "    - Decision affect the *reward*, the *agent state* and *environment*.\n",
    "\n",
    "- Lectures [5,6,7,8]\n",
    "    - **Model-free prediction** to estimate **values** in an unknown MDP(a model)\n",
    "    - **Model-free control** to estimate **policies** in an unknown MDP(a model)\n",
    "    - **Function approximation** and **DRL**.\n",
    "    - **Off-Policy** learning: making prediction about policies different  from  a the one you're following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff8316",
   "metadata": {},
   "source": [
    "### Monte Carlo Algorithms\n",
    "- In RL means sample a complete episodes\n",
    "- An episode is a trajectory of experience which has a natural ending\n",
    "- We can use experience **samples** to leanr without a model\n",
    "- MC is **model-free**: no knowedge of MDP required, only samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a4dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab6c092d",
   "metadata": {},
   "source": [
    "### Value Function Approximation\n",
    "\n",
    "- So far we mostly considered lookup tables\n",
    "  - Every state $s$ has an entry $v(s)$\n",
    "  - Or every state-action pair $s,a$ has an entry $q(s,a)$\n",
    "- Problem with large MDP:\n",
    "  - There are too many states and/or actions to stores in memory\n",
    "  - It is too slow to learn th value of each state individually\n",
    "  - individual state is not fully observable\n",
    "\n",
    "- Solution: \n",
    "  - Estimate value function with function approximation\n",
    "  $$\n",
    "  v(s;\\omega) \\approx v_{\\pi}(s)\n",
    "  $$\n",
    "  $$\n",
    "  q(s,a;\\omega) \\approx q_{\\pi}(s,a)\n",
    "  $$\n",
    "  \n",
    "  - Update parameter $\\omega$ using MC or TD learning (instead of updating a cell)\n",
    "  - Generalize from to unseen states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d21db1",
   "metadata": {},
   "source": [
    "### Agent state update\n",
    "\n",
    "Solution for large MDPs, if the enviroment state is not fully observable\n",
    "- Use the **agent state**:\n",
    "$$ S_t = u_{\\omega} (S_{t-1}, A_{t-1}, O_{t}) $$\n",
    "- $S_t$ denotes the agent state.\n",
    "- In the simplest case, $S_t=O_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77232e",
   "metadata": {},
   "source": [
    "### Feature Vectors\n",
    "\n",
    "- A useful specia case: **liner functions**\n",
    "- Represent state by a features vector\n",
    "$$ \\mathbf{x}(s) = \\begin{bmatrix} x_1(s) \\\\ x_2(s) \\\\ \\vdots \\\\ x_m(s) \\end{bmatrix}$$\n",
    "- $\\mathbf{x}:S \\rightarrow \\mathbb{R}^m$ is fixed mapping from agent state (e.g. observation) to feature vector\n",
    "- Short-hand: $\\mathbf{x}_t = \\mathbf{x}(S_t)  $\n",
    "- For example:\n",
    "  - Distance of robot from landmarks\n",
    "  - Trends in the stock market"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b95ca4f",
   "metadata": {},
   "source": [
    "### Linear Value Function Approximation\n",
    "\n",
    "- Approximate value function by a linear combination of fectures\n",
    "\n",
    "$$\n",
    "v(s;\\omega) = \\omega^T \\mathbf{x}(s)\n",
    "$$\n",
    "- Objective function *loss* is quadratic in $\\omega$\n",
    "\n",
    "$$\n",
    "L(\\omega) = \\frac{1}{2}(v(s;\\omega) - v_{\\pi}(s))^2\n",
    "$$\n",
    "- Stochastic gradient descent convet on *global* optimum\n",
    "- Update rule is simple\n",
    "\n",
    "$$\n",
    "\\omega \\leftarrow \\omega - \\alpha \\nabla_\\omega L(\\omega)\n",
    "$$\n",
    "$$\n",
    "\\nabla_\\omega L(\\omega) = [v_{\\pi}(s) - v(s;\\omega)] \\mathbf{x}(s)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ea4196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd8907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
