{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "By Hao Dong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **agent** and **enviroment**\n",
    "\n",
    "The environment is an entity that the agent can interact with, e.g. Pong game.\n",
    "\n",
    "The agent controls the paddle to hit the ball back and forth. An agent can ‚Äúinteract‚Äù with the environment by using a predefined **action set**: $A = \\{A_1, A_2, ... \\}$ (all possible actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p>The goal of reinforcement learning algorithms is to teach the agent how to interact ‚Äúwell‚Äù with the environment so that the agent is able to obtain a good score under a predefined evaluation metric</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent receive a reward $r$ of $1$ when the ball hits the wall on the opposite side. \n",
    "\n",
    "At an arbitrary time step (a point at which observations can be made), $t$, the agent first observes the current state of the environment, $S_t$, and the corresponding reward value, $R_t$.\n",
    "\n",
    "The agent then decides what to do next based on the state and reward information. The action the agent intends to perform, $A_t$, gets fed back into the environment such that we can obtain the new state $S_{t+1}$ and reward $R_{t+1}$.\n",
    "\n",
    "$$(S_t,R_t) \\rightarrow A_t \\rightarrow (S_{t+1},R_{t+1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation of the environment state $s$($s$ is a general representation of state regardless of time step $t$) from the agent‚Äôs perspective does not always contain all the information about the environment. \n",
    "\n",
    "If the observation only contains partial state information, the environment is *partially observable*. Nevertheless, if the observation contains the complete state information of the environment, the environment is *fully observable*.\n",
    "\n",
    "The action $a$ ($a$ is a general representation of action regardless of time step $t$) is usually conditioned on the state $s$ to represent the behavior of the agent (Under assumption of fully observable environments.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide feedback from the environment to the agent, a reward function $R$ generates an immediate reward $R_t$ according to the environment status and sends it to the agent at every time step. $R_t=R(S_t)$.\n",
    "\n",
    "> Trajectory: \n",
    "> \n",
    "> A **trajectory** is defined: \n",
    "> \n",
    "> $\\tau = (S_0, R_0,  A_0, S_1, R_1, A_1,...)$\n",
    "> \n",
    "> A *trajectory*, being referred to also as an *episode*, is a sequence that goes from the initial state to the terminal state (for finite cases)\n",
    "\n",
    "The initial state in a trajectory, $S_0$, is randomly sampled from the *start-state distribution*, denoted by $œÅ_0$, in which:\n",
    "\n",
    "$$S_0 \\sim œÅ_0(.)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transition from a state (after taking an action) to the next state can be either *deterministic transition process* or *stochastic transition process*\n",
    "\n",
    "For the *deterministic transition*, the next state $S_{t+1}$ is governed by a deterministic function:\n",
    "\n",
    "$$S_{t+1} = f(S_t, A_t)$$\n",
    "\n",
    "For the *stochastic transition* process, the next state $S_{t+1}$ is described as a probabilistic distribution:\n",
    "\n",
    "$$S_{t+1} \\sim p(S_{t+1}|S_t, A_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploitation** means maximizing the agent performance using the existing knowledge, and its performance is usually evaluated by the expected reward. Given the actual knowledge, the agent doesn't take risk to explore.\n",
    "\n",
    "The policy he took here is the **greedy policy**, which means the agent constantly performs the action that yields the highest expected reward based on current information, rather than taking risky trials which may lead to lower expected rewards.\n",
    "\n",
    "**Exploration** means increasing existing knowledge by taking actions and interacting with the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov process (MP) is a *discrete stochastic process* with *Markov property*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"assets/markov-process-example.png\" height=360>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph simulates how a person works on two tasks and goes to bed in the end.\n",
    "\n",
    "If I'm doing the \"Task 1\" and exists the `30%` probability of stoping and go to play a \"Game\" and then, the probability of returning to do the \"Task 1\" is only `10%` and the probability of keep playing is `90%`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphical Model of a Markov Process\n",
    "<center>\n",
    "<img src=\"assets/graphical-model-mp.png\" height=80> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a \\rightarrow b$ indicates the variable $b$ is depended on a vaiable $a$\n",
    "\n",
    "The probabilistic graphical model can help us to have a more intuitive sense of the relationships between variables in reinforcement learning, as well as providing rigorous references when we derive the gradients with respect to different variables along the MP chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MP follows the assumption of **Markov chain** (*memoryless property*) where the next state $S_{t+1}$\n",
    "is only dependent on the current state $S_t$, with the probability of a state\n",
    "jumping to the next state described as follows:\n",
    "\n",
    "$$P(S_{t+1}|S_{t}) = P(S_{t+1}|S_0, ..., S_{t})$$\n",
    "\n",
    "Also the *time homogeneus property*\n",
    "\n",
    "$$P(S_{t+1}|S_{t}) = P(S_{t+2}|S_{t+1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a finite *state set* $S$, we can have *state transition matrix* $P$\n",
    "\n",
    "$ Ôº≥= \\{g, t_1, t_2, r, p, b\\}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"assets/matrix-transition.PNG\" height=170>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $P_{i,j}$ represents the probability of transferring the current state $S_i$ to the next state $S_j$.\n",
    "\n",
    "$$ P(s=t_1|s=g)=10\\%$$\n",
    "\n",
    "The sum of each row must be equal to $1$ and the $P$ is always a square matrix.\n",
    "\n",
    "A Markov Process can be represented by a tuple $<Ôº≥, P>$\n",
    "\n",
    "The next state is sample from $P$:\n",
    "\n",
    "$$S_{t+1} \\sim P_{S_t} $$\n",
    "\n",
    "For continuous case a finite matrix can not be used to represent the transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Reward Process\n",
    "\n",
    "We need to add feedback from the enviroment to the agent, so we extent\n",
    "\n",
    "$<Ôº≥, P>$ to $<Ôº≥, P, R, \\gamma>$, in which $R$ represent the *reward function* and $\\gamma$ *reward discount factor*.\n",
    "\n",
    "$$R_t = R(S_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Graphical Model of a Markov Process is updated to \n",
    "<center>\n",
    "<img src=\"assets/graphical-model-mp-reward.png\" height=170>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, if we are considering to move to the next state, we should take account the rewards of the following next states that we will take in order to reach the $T$.\n",
    "\n",
    "If a single trajectory $\\tau$ has $T$ time steps, and the current time step $t=0$, then **the return** is the cummulative reward discounted by $\\gamma \\in (0,1)$  of a trajectory.\n",
    "\n",
    "$$G_{t=0:T} = G_{t=0}^{(T)} = R(\\tau)_{t=0}^{(T)} =  R_1 + \\gamma R_2 + ... + \\gamma^{T-1} R_T$$\n",
    "\n",
    "<!-- $\\gamma R_1 + ... + \\gamma^{T} R_T$ is the discount factor of the future rewards that we get from the current time step $t=0$ following a set of actions ($a_1, a_2, ..., a_{T}$) -->\n",
    "\n",
    "where $R_t$ is the **immediate reward** at time step $t$, and $T$ represents the time step of the terminal state and $r$ as a general representation of immediate reward value.\n",
    "\n",
    "The discounted factor is especially critical when handling with infinite MRP cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function $V(s)$ represents the expected return from the state $S_0$ that take the state value $s$ until final state.\n",
    "\n",
    "<!-- $$V(s) = E[R_t|S_0=s]$$ -->\n",
    "$$V(s) = E[G_{t=0}^{(T)}|S_0=s]$$\n",
    "\n",
    "A simple way to estimate the $V(s)$ is Monte Carlo method, we can randomly sample a large number of trajectories starting from state $s$ according to the given state transition matrix $P$.\n",
    "\n",
    "If the agent acts according to the policy $œÄ$, we denote the *value function* as $V^œÄ(s)$\n",
    "\n",
    "\n",
    "<!-- tasks = ['Bed', 'Game', 'Pass', 'Rest', 'Task1', 'Task2']\n",
    "rewards = {'Bed':0, 'Game':-1, 'Pass':10, 'Rest':1, 'Task1':-2, 'Task2':-2}\n",
    "\n",
    "transition = np.array(\n",
    "    [\n",
    "        [1.0, 0.0, 1.0, 0.0, 0.0, 0.3],\n",
    "        [0.0, 0.9, 0.0, 0.0, 0.3, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.6],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.1],\n",
    "        [0.0, 0.1, 0.0, 0.1, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.9, 0.7, 0.0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for tk in  tasks:\n",
    "    n_iter = 1\n",
    "    print(\"V(S={})=\".format(tk), end='')\n",
    "    while n_iter<=10_000:    \n",
    "        array = []\n",
    "        task = tk\n",
    "        target = 'Bed'\n",
    "        G = 0\n",
    "        gamma = 0.9\n",
    "        t = 0\n",
    "        while True:\n",
    "            idx = tasks.index(task)\n",
    "            reward = rewards[task]\n",
    "            G = G + (gamma** t)*reward\n",
    "            t+=1\n",
    "            prob = transition[:, idx].copy()\n",
    "            task = np.random.choice(tasks, p=prob)\n",
    "            if task == target:\n",
    "                array.append(G)\n",
    "                n_iter+=1\n",
    "                break\n",
    "\n",
    "    print(\"{:.3f}\".format(sum(array)/len(array)))} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "tasks = ['Bed', 'Game', 'Pass', 'Rest', 'Task1', 'Task2']\n",
    "rewards = {'Bed':0, 'Game':-1, 'Pass':10, 'Rest':1, 'Task1':-2, 'Task2':-2}\n",
    "value_function = {'Bed':0, 'Game':0, 'Pass':0, 'Rest':0, 'Task1':0, 'Task2':0}\n",
    "\n",
    "transition = np.array(\n",
    "    [\n",
    "        [1.0, 0.0, 1.0, 0.0, 0.0, 0.3],\n",
    "        [0.0, 0.9, 0.0, 0.0, 0.3, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.6],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.1],\n",
    "        [0.0, 0.1, 0.0, 0.1, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.9, 0.7, 0.0]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(task:str, target:str):\n",
    "    def trajectory(task:str, target:str, data:list):\n",
    "        data.append(task)\n",
    "        if task == target:\n",
    "            return data\n",
    "        else:\n",
    "            idx = tasks.index(task)\n",
    "            next_task =np.random.choice(tasks, p=transition[:, idx])\n",
    "            return trajectory(next_task, target, data)\n",
    "\n",
    "    return trajectory(task, target, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tasks:\n",
    "    n_iter = 10_000\n",
    "    v = 0\n",
    "    while n_iter > 0:\n",
    "        # 1. Simulating trajectory\n",
    "        simulated_trajectory = get_trajectory(task, 'Bed')\n",
    "\n",
    "        # 2. Simulating rewards\n",
    "        simulated_rewards = np.array([rewards[x] for x in simulated_trajectory])\n",
    "\n",
    "        # 3. Computing return\n",
    "        calculated_return = np.sum(\n",
    "            np.cumprod(\n",
    "                np.ones_like(simulated_rewards)*gamma)/gamma *simulated_rewards)\n",
    "        v = v + calculated_return\n",
    "        n_iter -= 1\n",
    "    n_iter = 10_000\n",
    "    value_function[task] = v/n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bed': 0.0,\n",
       " 'Game': -5.922879947584755,\n",
       " 'Pass': 10.0,\n",
       " 'Rest': 3.9446244226534253,\n",
       " 'Task1': -1.199615908058622,\n",
       " 'Task2': 3.7855726236614347}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent policy usually selects the next state with higher value.\n",
    "\n",
    "E.g. if we are in the *Game* state, we only have, two options: *Game* or *Task1*, we should choice *Task1* (since the higher value of $V(S=\\text{Task1})$). And if we are in th *Task1* state, we would have two options: *Game* or *Task2*.  We should choice *Task2* (since the higher value of $V(S=\\text{Task2})$) and so on.\n",
    "\n",
    "$$\\text{Game} \\rightarrow \\text{Task1} \\rightarrow \\text{Task2} \\rightarrow \\text{Pass} \\rightarrow \\text{Bed}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Decision Process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actions taken above only depend on the expected value of discounted reward  given a state. The action that maximize this expected value is taken. \n",
    "\n",
    "But we do more granularity this expected value and compute at level of action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tuple $$< Ôº≥, P, R, \\gamma>$$ we add $Ôº°$\n",
    "\n",
    "$$< Ôº≥, Ôº°, P, R, \\gamma>$$\n",
    "\n",
    "$$ P(s'|s,a) = P(S_{t+1} = s'|S_{t}=s, A_t=a)$$\n",
    "\n",
    "Where $Ôº°$ represent *finite action set* $\\{a_0, a_1, a_2, a_3, ..., a_{T-1}\\}$ and the immediate reward becomes:\n",
    "\n",
    "$$R_{t+1} = R(S_t=s, A_t=a, S_{t+1}=s')$$\n",
    "\n",
    "<!-- Reward Function ($ R(s, a, s') $):  -->\n",
    "The immediate reward received when transitioning from state $ s $ to state $ s' $ after action $ a $.\n",
    "\n",
    "\n",
    "A *policy* $\\pi$ represents the way in which the agent behaves based on its observations of the enviroment.\n",
    "\n",
    "$$\\pi(a|s) = p(A_t=a|S_t = s)$$\n",
    "\n",
    "**Expected return** is the expectation of returns over all possible trajectories under a policy. Therefore, **the goal of reinforcement learning is to find the higher expected return by optimizing the policy**.\n",
    "\n",
    "The probability of the T-step trajectory for MDP is:\n",
    "Based on a behavior of the agent will generate a path or trajectory.\n",
    "\n",
    "$$p(\\tau|\\pi)_{t=0}^{T-1} = p_0(S_0) \\prod_{t=0}^{T-1} p(S_{t+1}|S_t, A_t)\\pi(A_t|S_t)$$\n",
    "\n",
    "If is given the initial state $S_0$, the probability of the T-step trajectory for MDP will updated to:\n",
    "\n",
    "$$ p(\\tau|\\pi)_{t=0}^{T-1} = \\prod_{t=0}^{T-1} p(S_{t+1}|S_t, A_t)\\pi(A_t|S_t)$$\n",
    "\n",
    "\n",
    "Given state $S_t$ and action $A_t$, if only exists way to go state $S_{t+1}$ the probability of the T-step trajectory is reduced to:\n",
    "\n",
    "$$ \\prod_{t=0}^{T-1}p(S_{t+1}|S_t, A_t)\\pi( A_t|S_t) =  \\prod_{t=0}^{T-1}p(S_{t+1}|S_t, A_t)$$\n",
    "\n",
    "<!-- $$p(S_{t+1}|S_t, A_t)\\pi( A_t|S_t) = \\prod_{t=0}^{T-1} \\pi( A_t|S_t)$$ -->\n",
    "\n",
    "Given the reward function $R$ and all possible trajectories $œÑ$, the expected return $J(œÄ)$ starting from time step $t=0$ is defined as follows\n",
    "\n",
    "\n",
    "$$J(\\pi) = \\sum_{\\tau} P(\\tau|\\pi)_{t=0}^{T-1} R(\\tau)_{t=0}^{T} = E_{\\tau \\sim \\pi}[R(\\tau)_{t=0}^{T}]$$\n",
    "\n",
    "\n",
    "The RL optimization problem is to improve the policy for maximizing the expected return with optimization methods. \n",
    "\n",
    "The **optimal policy** $œÄ^‚àó$ can be expressed as\n",
    "\n",
    "$$\\pi^* = \\argmax_{\\pi} J(\\pi)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given policy $\\pi$, the **value function** $V(s)$ can be defined as:\n",
    "\n",
    "$$V^{\\pi} (s) = E_{\\tau \\sim \\pi}[R(\\tau)_{t}^{T}|S_{t}=s]$$\n",
    "\n",
    "where $œÑ‚Äâ‚àº‚ÄâœÄ$ means the trajectories $œÑ$ are sampled given the policy $œÄ$\n",
    "\n",
    "In MDP, given an action, we have the **action-value function** $Q^\\pi(s, a)$, which depends on both the state and the action just taken\n",
    "\n",
    "$$Q^{\\pi} (s, a) = E_{\\tau \\sim \\pi}[R(\\tau)_{t}^{T}|S_{t}=s, A_{t}=a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to keep in mind that the $Q^œÄ(s, a)$ depends on $œÄ$, as the\n",
    "estimation of the value is an expectation over the trajectories by the policy\n",
    "$œÄ$. This also indicates if the $œÄ$ changes, the corresponding $Q^œÄ(s, a)$ will also change.\n",
    "\n",
    "We therefore usually call the value function estimated with a specific policy *the on-policy value function* (with lower case $q$), for the distinction from\n",
    "the optimal value function estimated with the optimal policy.\n",
    "\n",
    "$$q_{\\pi}(s,a) = E_{\\tau \\sim \\pi}[R(\\tau)_{t}^{T}|S_t=s, A_t=a]$$\n",
    "\n",
    "$$v_{\\pi}(s) = E_{a \\sim \\pi}[q_{\\pi}(s,a)] $$\n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_{a} \\pi(a|s)  q_{\\pi}(s,a) $$\n",
    "<!-- =E_{\\tau \\sim \\pi}[R(\\tau)|S_0=s] -->\n",
    "\n",
    "We can use *Monte Carlo method* to estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman Equation for **Value Function**\n",
    "\n",
    "Given state $s$ in time step $t$ ($s'$ in time step $t+1$) with a policy $\\pi$, and the all trajectories $\\tau$ upto time step $T$ with respective rewards, we can define  the value function  $v^{\\pi} (s)$\n",
    "\n",
    "$$v^{\\pi} (s) = E_{\\tau \\sim \\pi}[R(\\tau)_{t=t}^{T}|S_{t}=s] = \\sum_{\\tau} P(\\tau|\\pi)_{t=t}^{T-1} R(\\tau)_{t=t}^{T} $$\n",
    "\n",
    "\n",
    "\n",
    "We can split the above into \n",
    "1.  $P(\\tau|\\pi)_{t=t}^{T-1} =  P(s'|s, a) \\pi(a|s) P(\\tau|\\pi)_{t=t+1}^{T-1}$\n",
    "\n",
    "2. $R(\\tau)_{t=t}^{T} = R_{t+1} + \\gamma R(\\tau)_{t=t+1}^{T}$ (*Return* after take action $a$ in the time step $t$)\n",
    "\n",
    "\n",
    "$$v^{\\pi} (s) = \\sum_{\\tau} P(s'|s,a) \\pi(a|s) P(\\tau|\\pi)_{t=t+1}^{T-1} [R_{t+1} + \\gamma R(\\tau)_{t=t+1}^{T}]$$\n",
    "\n",
    "$$v^{\\pi} (s) = \\sum_{s'} P(s'|s,a) \\sum_{a} \\pi(a|s) \\sum_{\\tau} P(\\tau|\\pi)_{t=t+1}^{T-1} [R_{t+1} + \\gamma R(\\tau)_{t=t+1}^{T}]$$\n",
    "\n",
    "$$v^{\\pi} (s) = \\sum_{s'} P(s'|s,a) \\sum_{a} \\pi(a|s)[R_{t+1} + \\gamma \\sum_{\\tau} P(\\tau|\\pi)_{t=t+1}^{T-1} R(\\tau)_{t=t+1}^{T}]$$\n",
    "\n",
    "$$v^{\\pi} (s) = \\sum_{s'} P(s'|s,a) \\sum_{a} \\pi(a|s)[R_{t+1} + \\gamma v^{\\pi} (s')]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman Equation for **Value Action Function**\n",
    "\n",
    "Let be state $s$ in time step $t$ (state $s'$ and action $a'$ in time step $t$) with a policy $\\pi$, and the all trajectories $\\tau$ upto time step $T$ with respective rewards, we can define  the value function  $q^{\\pi} (s, a)$\n",
    "\n",
    "$$q^{\\pi} (s, a) = E_{\\tau \\sim \\pi}[R(\\tau)_{t=t+1}^{T}|S_{t}=s, A_{t}=a] = \\sum_{\\tau} P(\\tau|\\pi)_{t=t+1}^{T-1} R(\\tau)_{t=t+1}^{T} $$\n",
    "\n",
    "\n",
    "\n",
    "We can split the above into \n",
    "1.  $P(\\tau|\\pi)_{t=t+1}^{T-1} =  P(s'|s,a) \\pi(a|s) P(\\tau|\\pi)_{t=t+1}^{T-1}$\n",
    "\n",
    "2. $R(\\tau)_{t=t+1}^{T} = R_0 + \\gamma R(\\tau)_{t=t+1}^{T}$\n",
    "\n",
    "3. Since $s$  and $a$ are given, $\\pi(a|s) = 1$\n",
    "\n",
    "\n",
    "$$q^{\\pi} (s,a) = \\sum_{\\tau} P(s'|s,a) \\pi(a|s) P(\\tau|\\pi)_{t=t+1}^{T-1} [R_{t+1} + \\gamma R(\\tau)_{t=t+1}^{T}]$$\n",
    "\n",
    "$$q^{\\pi} (s,a) = \\sum_{s'} P(s'|s,a) \\sum_{\\tau} P(\\tau|\\pi)_{t=t+1}^{T-1}[ R_{t+1} + \\gamma R(\\tau)_{t=t+1}^{T}]$$\n",
    "\n",
    "$$q^{\\pi} (s,a) = \\sum_{s'} P(s'|s,a) [R_{t+1} + \\gamma \\sum_{\\tau} P(\\tau|\\pi)_{t=t+1}^{T-1} R(\\tau)_{t=t+1}^{T}]$$\n",
    "\n",
    "$$q^{\\pi} (s, a) = \\sum_{s'} P(s'|s,a) [R_{t+1} + \\gamma v^{\\pi} (s')]$$\n",
    "\n",
    "$$q^{\\pi} (s, a) = \\sum_{s'} P(s'|s,a) [R_{t+1} + \\gamma \\sum_{a'} \\pi(a'|s') q^{\\pi} (s',a')]$$\n",
    "\n",
    "<!-- #### Bellman Ecuation and Optimality\n",
    "\n",
    "Assumptions\n",
    "- Let $x_t$ be the state at same time $t$\n",
    "- The initial decision begin at $t=0$, so the intial state is $x_0$\n",
    "- The set of available actions  that depends on current state $a_t \\in \\Gamma(x_t)$\n",
    "- The next state after taken the action $a_t$ is $x_{t+1}=T(x_t, a_t)$\n",
    "- The payoff from taking the action $a_t$ is $F(x_t,a_t )$\n",
    "- Discount factor $0< \\beta<1$\n",
    "\n",
    "$V(x_0)$ denote the *optimal value* that can be obtained by maximizing this *objetive function* subject to contraints.\n",
    "\n",
    "$$V(x_0) = \\max_{\\{ a_t\\}_{t=0}^{\\infty}} \\sum_{t=0}^{\\infty} \\beta^t F(x_t, a_t)$$\n",
    "\n",
    "*Principle of Optimality*: An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. (See Bellman, 1957, Chap. III.3.)\n",
    "$$V(x_0) = \\max_{\\{ a_0\\}}[ F(x_0, a_0) +  \\max_{\\{ a_t\\}_{t=1}^{\\infty}}\\sum_{t=1}^{\\infty} \\beta^t F(x_t, a_t)]$$\n",
    "\n",
    "$$V(x_0) = \\max_{\\{ a_0\\}}[F(x_0, a_0)+V(x_1)]$$\n",
    "\n",
    "It reads from inner to outer, first maximize from the step $t=1$ to next, then add the payoff of the initial state and maximize it.  -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Value Functions\n",
    "\n",
    "Since on-policy **value functions** are estimated with respect to the policy\n",
    "itself, different policies will lead to different value functions, even for the\n",
    "same set of states and actions. Among all those different value functions,\n",
    "we define the optimal value function as\n",
    "\n",
    "$$v_*(s) = \\max_{\\pi} v_{\\pi}(s)$$\n",
    "\n",
    "For **action-value function**\n",
    "\n",
    "$$q_{*}(s, a) = \\max_{\\pi} q_{\\pi}(s, a)  $$\n",
    "\n",
    "We will update the policy $\\pi$ such that it converges to the optimal policy. We will update the matrix transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### Bellman Optimality Equation\n",
    "\n",
    "Bellman equation for optimal value functions (*Optimal value function* and *Optimal action-value function*)\n",
    "\n",
    "We choose the action (in which we can choose) that maximizes the expected return.\n",
    "\n",
    "Bellman equation for optimal value function\n",
    "\n",
    "$$v^{\\pi}_{*} (s) = \\max_{a} \\sum_{s'} P(s'|s,a) \\sum_{a} \\pi(a|s)[R_{t+1} + \\gamma \\max_{a'} v^{\\pi}_{*} (s')]$$\n",
    "\n",
    "Bellman equation for optimal action-value function\n",
    "\n",
    "The action $a$ is given for time $t$ so we can not maximize it.\n",
    "\n",
    "$$q^{\\pi}_{*} (s, a) = \\sum_{s'} P(s'|s,a) [R_{t+1} + \\gamma \\sum_{a'} \\pi(a'|s') \\max_{a'} q^{\\pi}_{*} (s',a')]$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Dynamic\" means that the problem has sequential or temporal components.\n",
    "\n",
    "\"Programming\" \n",
    "\n",
    "DP solves a complex problem by breaking it down into smaller subproblems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### üìò Full Explanation of **Dynamic Programming (DP)** in Reinforcement Learning (RL)\n",
    "\n",
    "**Dynamic Programming (DP)** is a fundamental technique in **Reinforcement Learning (RL)** used to solve problems where the agent can model the environment completely ‚Äî that is, when the agent knows all the **transition probabilities** and **reward functions**.\n",
    "\n",
    "It is one of the earliest and most classical approaches to solving **Markov Decision Processes (MDPs)**. -->\n",
    "\n",
    "<!-- ---\n",
    "\n",
    "## üîç What Is Dynamic Programming?\n",
    "\n",
    "In the context of RL:\n",
    "\n",
    "> **Dynamic Programming (DP)** refers to a collection of algorithms that can compute optimal policies **given a perfect model of the environment**, described as a **Markov Decision Process (MDP)**.\n",
    "\n",
    "--- -->\n",
    "\n",
    "Prerequisites for Using DP\n",
    "\n",
    "To apply DP methods in RL, you need:\n",
    "\n",
    "1. **Full knowledge of the environment**\n",
    "   - Transition probability $ P(s' | s, a) $\n",
    "   - Reward function $ R(s, a, s') $\n",
    "2. The problem must be expressible as an **MDP**\n",
    "3. Sufficient computational resources (DP scales poorly with large state spaces)\n",
    "\n",
    "Two properties that a *problem* must have for DP to be applicable:\n",
    "1. *Optimal substructure*: Optimal solution can be decomposed into optimal solutions for its sub-problems:\n",
    "2. *Overlapping sub-problems*: implies that the number of sub-problems is finite and the problem ocurr recursively so sub-solutions can be cached and reused.\n",
    "\n",
    "> Recursion is a programming technique that uses functions to solve problems by breaking them down into smaller, more manageable problems. It's a method that involves a function calling itself repeatedly until a base case is reached\n",
    "\n",
    "Model a problem as an MDP and using Bellman's recursion equation allow us meet the two properties of DP.\n",
    "\n",
    "\n",
    "\n",
    "<!-- --- -->\n",
    "\n",
    "<!-- ## üß† Core Concepts in DP for RL\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **Policy Evaluation** | Estimating the value function $ V^\\pi(s) $ for a given policy $ \\pi $ |\n",
    "| **Policy Improvement** | Improving a policy using the value function |\n",
    "| **Policy Iteration** | Alternating between policy evaluation and improvement |\n",
    "| **Value Iteration** | Directly computing the optimal value function without explicitly maintaining a policy | -->\n",
    "\n",
    "<!-- Markov Decision Process (MDP) Recap\n",
    "\n",
    "An MDP is defined by:\n",
    "\n",
    "- A set of **states**: $ \\mathcal{S} $\n",
    "- A set of **actions**: $ \\mathcal{A} $\n",
    "- A **transition probability function**:  \n",
    "  $ P(s' | s, a) = \\text{Probability of moving to state } s' \\text{ from state } s \\text{ after action } a $\n",
    "- A **reward function**:  \n",
    "  $ R(s, a, s') = \\text{Expected reward received after transitioning to } s' \\text{ from } s \\text{ via } a $\n",
    "- A **discount factor**:  \n",
    "  $ \\gamma \\in [0, 1] $ ‚Äî how much future rewards are valued compared to immediate ones\n",
    "\n",
    "---\n",
    "\n",
    "Bellman Equations: Foundation of DP\n",
    "\n",
    "DP relies on recursive equations known as **Bellman equations**.\n",
    "\n",
    "Value Function Under a Policy $ \\pi $\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma V^\\pi(s') \\right]\n",
    "$$\n",
    "\n",
    "This equation expresses the value of being in a state $ s $ under policy $ \\pi $ as the expected sum of current and discounted future rewards. -->\n",
    "\n",
    "<!-- --- -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of Dynamic Programming Algorithms\n",
    "\n",
    "**Policy Evaluation**\n",
    "\n",
    "- **Goal**: Evaluate how good a policy is ‚Äî estimate its value function.\n",
    "- **Method**: Iteratively apply the Bellman expectation backup until convergence:\n",
    "  <!-- $$\n",
    "  V_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a)\\left[R_0 + \\gamma V_k(s')\\right]\n",
    "  $$ -->\n",
    "\n",
    "  $$\n",
    "  V(s) \\Leftarrow \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a)\\left[R_{t+1} + \\gamma V(s')\\right]\n",
    "  $$\n",
    "\n",
    "  If the taken action only have one possible next state, then the Bellman expectation backup can be simplified to:\n",
    "\n",
    "  $$\n",
    "  V(s) \\Leftarrow  \\sum_{s'} P(s'|s, a)\\left[R_{t+1} + \\gamma V(s')\\right]\n",
    "  $$\n",
    "\n",
    "\n",
    "**Policy Improvement**\n",
    "\n",
    "- **Goal**: Improve a policy based on its value function.\n",
    "- **Method**: For each state $ s $, choose the action that maximizes expected return:\n",
    "  $$\n",
    "  \\pi'(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a)\\left[R_{t+1} + \\gamma V^\\pi(s')\\right]\n",
    "  $$\n",
    "\n",
    "**Policy Iteration** \n",
    "<!-- or *generalize policy iteration* (GPI) -->\n",
    "\n",
    "- **Goal**: Find the optimal policy through repeated policy evaluation and improvement.\n",
    "- **Steps**:\n",
    "  1. Initialize a random policy $ \\pi_0 $\n",
    "  2. While policy not converged:\n",
    "     - Evaluate $ V^{\\pi}(s) $\n",
    "     - Improve $ \\pi $ using $ V^{\\pi} $\n",
    "- **Result**: Converges to the **optimal policy** $ \\pi^* $\n",
    "\n",
    "**Value Iteration**\n",
    "\n",
    "- **Goal**: Compute the optimal value function directly without needing to evaluate intermediate policies.\n",
    "- **Method**:\n",
    "  <!-- $$\n",
    "  V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a)\\left[R_0 + \\gamma V_k(s')\\right]\n",
    "  $$ -->\n",
    "\n",
    "  $$\n",
    "  V(s) \\Leftarrow \\max_a \\sum_{s'} P(s'|s,a)\\left[R_{t+1} + \\gamma V(s')\\right]\n",
    "  $$\n",
    "- After convergence, extract the optimal policy:\n",
    "  $$\n",
    "  \\pi^*(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a)\\left[R_{t+1} + \\gamma V^*(s')\\right]\n",
    "  $$\n",
    "\n",
    "\n",
    "Advantages of Dynamic Programming\n",
    "\n",
    "| Advantage | Description |\n",
    "|----------|-------------|\n",
    "| **Guaranteed Convergence** | If the MDP is finite and the updates are done correctly, DP converges to the optimal solution |\n",
    "| **Mathematically Exact** | Solves Bellman equations exactly (within numerical precision) |\n",
    "| **Basis for Approximate Methods** | Many modern RL algorithms (like Q-learning, DQN) are inspired by DP concepts |\n",
    "\n",
    "\n",
    "Disadvantages of Dynamic Programming\n",
    "\n",
    "| Disadvantage | Description |\n",
    "|--------------|-------------|\n",
    "| **Requires Full Model** | Needs full knowledge of transitions and rewards ‚Äî rarely available in real-world scenarios |\n",
    "| **Computationally Expensive** | Infeasible for large state spaces (curse of dimensionality) |\n",
    "| **Not Suitable for Partial Observability** | Assumes the environment is fully observable (MDP), not applicable to POMDPs |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from pprint import pprint\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Example of a grid world environment\n",
    "# Method: Every Visit\n",
    "\n",
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.n_episodes = 1000\n",
    "        self.n_states = 16\n",
    "        self.n_actions = 4\n",
    "        self.gamma = 0.9\n",
    "        self.theta = 1e-6\n",
    "        self.target_state = (0, 2)\n",
    "        self.value = np.zeros((self.n_states//4, self.n_states//4))\n",
    "        self.prob = np.ones((self.n_states//4, self.n_states//4, self.n_actions), \n",
    "                            dtype=np.int32)\n",
    "        self.states = [(i, j) for i in range(4) for j in range(4)]\n",
    "        self.actions = np.arange(self.n_actions)\n",
    "        self.blocked_states_01 = [(0, 1), (0, 2), (0, 3)]\n",
    "        self.blocked_states_02 = [(2, 0), (2, 1), (2, 2)]\n",
    "        self.state = (3, 0)\n",
    "        self.rewards = np.ones((self.n_states//4, self.n_states//4), dtype=np.int32) * -1\n",
    "        self.rewards[self.target_state] = 5\n",
    "        # for state in self.blocked_states_01:\n",
    "        #    self.rewards[state] = -10\n",
    "        # for state in  self.blocked_states_02:\n",
    "        #    self.rewards[state] = -10\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = (3, 0)\n",
    "        # self.value = np.zeros((self.n_states//5, self.n_states//5))\n",
    "        # return self.state\n",
    "    \n",
    "    def show_action(self, action:int):\n",
    "        if action == 0: \n",
    "            return \"‚Üí\"\n",
    "        elif action == 1: \n",
    "            return \"‚Üê\"\n",
    "        elif action == 2: \n",
    "            return \"‚Üì\"\n",
    "        elif action == 3: \n",
    "            return \"‚Üë\"\n",
    "\n",
    "    def policy(self):\n",
    "        return random.choice(range(4))\n",
    "        \n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0: y += 1  # Right\n",
    "        elif action == 1: y -= 1  # Left\n",
    "        elif action == 2: x += 1  # Down\n",
    "        elif action == 3: x -= 1  # Up\n",
    "        # The the agent keep in the same state if \n",
    "        # the action inplies over the max or min values of the enviroment\n",
    "        next_state = (max(0, min(x, 3)), max(0, min(y, 3)))\n",
    "\n",
    "        # next_reward = 1 if next_state == self.goal else -0.01\n",
    "        # reward = 1 if self.state == self.goal else -0.01\n",
    "        reward = self.rewards[next_state]\n",
    "        # reward = self.rewards[self.state]\n",
    "        # reward = self.rewards[next_state] - 5 if self.state == next_state else self.rewards[next_state]\n",
    "        # reward, next_reward,\n",
    "        # done = next_state == self.goal\n",
    "        return next_state, reward #,  done\n",
    "    \n",
    "    def extract_optimal_policy(self):\n",
    "        data = np.empty_like(self.value)\n",
    "        data = data.astype(np.object_)\n",
    "        for state in self.states:\n",
    "            prob = self.prob[state]/sum(self.prob[state])\n",
    "            action = np.argmax(prob)\n",
    "            data[state] = self.show_action(action)\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_iteration():\n",
    "    env = Env()\n",
    "    converge = False\n",
    "    start = time.time()\n",
    "    \n",
    "    # 1. Policy Evaluation\n",
    "    # Evaluate for all states and actions\n",
    "    while not converge:\n",
    "        \n",
    "        delta = 0\n",
    "        for state in env.states:\n",
    "            values = []\n",
    "            prob = env.prob[state]/np.sum(env.prob[state])\n",
    "            env.state = state\n",
    "            for action in env.actions:\n",
    "                next_state, reward = env.step(action)\n",
    "                values.append(prob[action]*( \n",
    "                    reward + \n",
    "                    env.gamma * env.value[next_state]))\n",
    "            \n",
    "            delta = max(delta, abs(sum(values) - env.value[state]))\n",
    "            env.value[state] = sum(values)\n",
    "        if delta < env.theta:\n",
    "            # print('Break Evaluation')\n",
    "            # break\n",
    "            # 2. Policy Improvement\n",
    "            # 2.1. Update Policy\n",
    "            delta = 0\n",
    "            \n",
    "            for state in env.states:\n",
    "                values = []\n",
    "                prob = env.prob[state]/sum(env.prob[state])\n",
    "                env.state = state\n",
    "                for action in env.actions:\n",
    "                    \n",
    "                    next_state, reward = env.step(action)\n",
    "                    values.append(prob[action]*( \n",
    "                            reward +  \n",
    "                            env.gamma * env.value[next_state]))\n",
    "                action_max = np.argmax(values)\n",
    "                env.prob[state][action_max]+=1\n",
    "                delta = max(delta, abs(\n",
    "                    env.prob[state][action_max]/sum(env.prob[state]) - prob[action_max])\n",
    "                    )\n",
    "            \n",
    "            if delta < 0.001:\n",
    "                # print('Break Policy')\n",
    "                converge=True\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    end = time.time()\n",
    "    print('Time: {:0>2}:{:0>2}:{:0>2}'.format(round((end-start)//60, 0),  round(((end-start)%60)//1, 0), round((end-start)%1, 0)))\n",
    "    return env\n",
    "\n",
    "def get_value_iterarion():\n",
    "    env = Env()\n",
    "    converge = False\n",
    "    \n",
    "    # 1. Policy Evaluation\n",
    "    # Evaluate for all states and actions\n",
    "    start = time.time()\n",
    "    while not converge:\n",
    "        \n",
    "        delta = 0\n",
    "        for state in env.states:\n",
    "            values = []\n",
    "            prob = env.prob[state]/np.sum(env.prob[state])\n",
    "            env.state = state\n",
    "            for action in env.actions:\n",
    "                # env.state = state\n",
    "                next_state, reward = env.step(action)\n",
    "                values.append(prob[action]*( \n",
    "                    reward + \n",
    "                    env.gamma * env.value[next_state]))\n",
    "                \n",
    "            action_max=np.argmax(values)\n",
    "            next_state, reward = env.step(action_max)\n",
    "            value = prob[action_max]*(\n",
    "                    reward + \n",
    "                    env.gamma * env.value[next_state])\n",
    "            \n",
    "            delta = max(delta, abs(value - env.value[state]))\n",
    "            env.value[state] = value\n",
    "            env.prob[state][action_max]+=1\n",
    "        if delta < env.theta:\n",
    "            converge=True\n",
    "            # print('Break Evaluation')\n",
    "            break\n",
    "    end = time.time()\n",
    "    print('Time: {:0>2}:{:0>2}:{:0>2}'.format(int(round((end-start)//60, 0)),  \n",
    "                                               int(round(((end-start)%60)//1,0)), \n",
    "                                               int(round((end-start)%1, 0))))\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.0:0.0:1.0\n",
      "[['‚Üí' '‚Üí' '‚Üë' '‚Üê']\n",
      " ['‚Üí' '‚Üë' '‚Üë' '‚Üë']\n",
      " ['‚Üë' '‚Üë' '‚Üë' '‚Üë']\n",
      " ['‚Üë' '‚Üë' '‚Üë' '‚Üë']]\n",
      "Time: 00:08:01\n",
      "[['‚Üí' '‚Üí' '‚Üë' '‚Üê']\n",
      " ['‚Üë' '‚Üë' '‚Üë' '‚Üê']\n",
      " ['‚Üë' '‚Üë' '‚Üë' '‚Üê']\n",
      " ['‚Üë' '‚Üë' '‚Üë' '‚Üê']]\n"
     ]
    }
   ],
   "source": [
    "policy_iteration = get_policy_iteration()\n",
    "policy_iteration.extract_optimal_policy()\n",
    "\n",
    "value_iteration = get_value_iterarion()\n",
    "value_iteration.extract_optimal_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo\n",
    "\n",
    "- MC doesn't require full knowledge of the environment, only a little prior knowledge about it, then using sampling (implies it has a component of uncertainty) we can create experiences to generate learning.\n",
    "- When using MC in reinforcement learning, we will\n",
    "average the returns for each state-action pair from different episodes\n",
    "- Initial estimations are not good, but they improve as we get more data\n",
    "- Also, we assume that the problem is episodic and an episode will always terminate regardless of the actions taken.\n",
    "\n",
    "##### Incremental Monte Carlo Prediction\n",
    "\n",
    "Exists a more efficient computational method that allow us get rid of the list of observed returns and simplify the mean calculation step.\n",
    "\n",
    "Let $Q(S_k, A_k)$ be the estimation of the state-action value after it ( the tuple $(S, A)$) has been selected for $k-1$ times can be rewitten as:\n",
    "\n",
    "$$Q_{k} = \\frac{G_1 + G_2 + ..., G_{k-1}}{k-1} $$\n",
    "\n",
    "<!-- It is only the estimation of $Q_{k}$, since the actual value is $Q_{k} = \\frac{G_1 + G_2 + ..., G_{k}}{k} $ -->\n",
    "\n",
    "This can be computed by the following:\n",
    "\n",
    "$$Q_{k+1} = \\frac{1}{k} \\sum_{i=1}^{k}G_i$$\n",
    "$$=\\frac{1}{k}(G_k +  \\frac{(k-1)}{(k-1)}\\sum_{i=1}^{k-1}G_i)$$\n",
    "$$=\\frac{1}{k}(G_k +  (k-1)Q_{k})$$\n",
    "$$=Q_{k} + \\frac{1}{k}(G_k - Q_{k})$$\n",
    "\n",
    "More general form:\n",
    "\n",
    "$$ \\text{NewEstimate} \\Leftarrow \\text{OldEstimate} + \\text{StepSize}.(\\text{Target} - \\text{OldEstimate}) $$\n",
    "\n",
    "The ‚ÄúStepSize‚Äù is a parameter that controls how fast the estimate is being updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### State-Value Prediction\n",
    "\n",
    "*Task*: estimate the state-value function for a given policy $œÄ$, that means averaging the returns from a particular policy. But we can use incremental update to estimate the state-value function.\n",
    "\n",
    "$$Q_{k+1} = Q_{k} + \\frac{1}{k}(G_{k} - Q_{k})$$\n",
    "\n",
    "There are two types of estimations, *First-visit MC* and *every-visit MC*. The *First-visit MC* only considers the return of the first visit to state $s$ in the whole episode, however, every-visit MC considers every visit to state $s$ in the episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy Iteration** (Monte Carlo Control) for MC\n",
    "\n",
    "Policy iteration is a method for finding the optimal policy in a Markov decision process (MDP). It works by iteratively improving the policy until it converges to the optimal policy.\n",
    "\n",
    "Policy iteration has two steps:\n",
    "\n",
    "1. Policy Evaluation: Compute the value function for the current policy.\n",
    "2. Policy Improvement: Improve the policy based on the value function.\n",
    "\n",
    "To improve the policy we will use the greedy policy. The greedy policy will always choose the *action* that\n",
    "has maximal action-value function for a given *state*.\n",
    "\n",
    "$$\\pi(s) = \\argmax_a q(s, a) $$\n",
    "\n",
    "For each policy improvement, we will need to construct $œÄ_{t+1}$ based on $\\pi_t$ .\n",
    "\n",
    "\n",
    "$$ \\pi_{k+1}(s) =\\argmax_a q_{\\pi_k}(s, \\pi_k(s)) $$\n",
    "\n",
    "<!-- $$q_{\\pi_t}(s, \\pi_{t+1}(s)) = \\argmax_a q(s, a) \\ge q_{\\pi_t}(s, \\pi_t(s)) $$ -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monte_carlo():\n",
    "    # Training parameters\n",
    "    \n",
    "    env = Env()\n",
    "    q_table = np.zeros((env.n_states//4, env.n_states//4, env.n_actions))\n",
    "    counts = np.zeros((env.n_states//4, env.n_states//4,env.n_actions))\n",
    "    n_episodes = 1000\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        env.reset()\n",
    "        trajectory = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.policy()\n",
    "            next_state, reward = env.step(action)\n",
    "            done = True if next_state == env.target_state else False\n",
    "            trajectory.append((env.state, action, reward, next_state))\n",
    "            env.state = next_state\n",
    "\n",
    "        # 1. First visit only consider the first time we visit \n",
    "        # a state in an episode\n",
    "        # 2. Every visit consider all the times we visit a state\n",
    "        G = 0\n",
    "        for step in reversed(trajectory):\n",
    "            # We use (2. every visit) to update the Q-table\n",
    "            state, action, reward, next_state = step\n",
    "            G = reward + env.gamma * G\n",
    "            counts[state][action] += 1\n",
    "            q_table[state][action] += (1/counts[state][action]) * (G - q_table[state][action])\n",
    "\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) ‚Üí\n",
      "(0, 1) ‚Üí\n",
      "(0, 2) ‚Üí\n",
      "(0, 3) ‚Üê\n",
      "(1, 0) ‚Üë\n",
      "(1, 1) ‚Üë\n",
      "(1, 2) ‚Üë\n",
      "(1, 3) ‚Üë\n",
      "(2, 0) ‚Üë\n",
      "(2, 1) ‚Üë\n",
      "(2, 2) ‚Üë\n",
      "(2, 3) ‚Üë\n",
      "(3, 0) ‚Üë\n",
      "(3, 1) ‚Üë\n",
      "(3, 2) ‚Üë\n",
      "(3, 3) ‚Üë\n"
     ]
    }
   ],
   "source": [
    "q_table = get_monte_carlo()\n",
    "env = Env()\n",
    "\n",
    "for state in env.states:\n",
    "    best_action = np.argmax(q_table[state])\n",
    "    print(state, env.show_action(best_action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal Difference Learning\n",
    "\n",
    "- Temporal Difference (TD) combines ideas from DP and MC.\n",
    "\n",
    "- Similar to DP, TD use bootstrapping in the estimation (updating estimates based on other estimates, rather than waiting for the final outcome), however, like MC, it does not require full knowegde of the enviroment in the learning process, but applies a sampling-based optimization approach.\n",
    "\n",
    "TD utilizes the error, the difference between the *target value* and the *estimate value* at different time step TD(0).\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$$\n",
    "\n",
    "If we observe carefully, the target value during update for MC is $G_t$ which is known only after one episode, since we don't want to wait for the final outcome and additionally $G_t = R_{t+1} + \\gamma G_{t+1}$, but $G_{t+1}$ is known after one episode as well, instead, we can use an metric that summarize the values ($G_{0}, G_{1}, ..., G_{k-1}$) for the next step $t+1$ like the average $V(S_{t+1})_k$, but due to $V_{t+1}$ can be computed step by step don't depend on $k$, so for TD the target\n",
    "value is $R_{t+1}‚Äâ+‚ÄâŒ≥V‚Äâ(S_{t+1})$ which can be computed step by step and $V(S_{t+1})$ is the average of $G$ at state $S_{t+1}$. This reduce variance of the estimator\n",
    "\n",
    "\n",
    "**Sarsa: On-Policy TD Control**\n",
    "\n",
    "The update rule can, therefore, be framed as\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha[R_{t+1} + \\gamma Q(S_{t+1}, A_t) - Q(S_t, A_t)]$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><thead><tr><th>Aspect</th><th>On-Policy (e.g., SARSA)</th><th>Off-Policy (e.g., Q-Learning)</th></tr></thead><tbody><tr><td><strong>Behavior vs. Target Policy</strong></td><td>Same policy for learning and acting</td><td>Different policies for learning and acting</td></tr><tr><td><strong>Exploration</strong></td><td>Directly updates using exploratory actions</td><td>Updates using the greedy (optimal) action</td></tr><tr><td><strong>Learning Stability</strong></td><td>More stable and robust</td><td>Can be unstable without proper safeguards</td></tr><tr><td><strong>Learning Speed</strong></td><td>Slower due to learning from exploratory actions</td><td>Faster because it learns the optimal policy</td></tr><tr><td><strong>Sample Efficiency</strong></td><td>Lower</td><td>Higher</td></tr><tr><td><strong>Use Case</strong></td><td>Evaluating or improving the current behavior</td><td>Learning the best policy, even while exploring</td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Learning: Off-Policy TD Control**\n",
    "\n",
    "Q-learning is an off-policy TD method that is very similar to Sarsa and plays\n",
    "an important role in deep reinforcement learning application such as the\n",
    "deep Q-network\n",
    "\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha[R_{t+1} + \\gamma \\max_a(Q(S_{t+1}, a)) - Q(S_t, A_t)]$$\n",
    "\n",
    "The main difference that Q-learning has from Sarsa is that the target value now is no longer dependent on the policy being used (how the action is chosen) but only on the state-action function (max)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Trained Q-Table:'\n",
      "{(0, 0): [0.4261265899999825,\n",
      "          0.20430841017696696,\n",
      "          0.26988027762074734,\n",
      "          0.24933425939289922],\n",
      " (0, 1): [0.24272096171333535,\n",
      "          0.2070055395543572,\n",
      "          0.4845850999999959,\n",
      "          0.23608708290459007],\n",
      " (0, 2): [-0.009619450402373109,\n",
      "          0.033501489096055555,\n",
      "          0.4941230037676856,\n",
      "          -0.00956179249911955],\n",
      " (0, 3): [-0.00702190387,\n",
      "          -0.006375628531738719,\n",
      "          -0.006222977271038711,\n",
      "          -0.0058519850599],\n",
      " (0, 4): [-0.00490099501, -0.00520441888699, -0.0053416828, -0.00490099501],\n",
      " (1, 0): [0.4670034850861101,\n",
      "          0.02690248265109381,\n",
      "          -0.014941567169801955,\n",
      "          -0.01420430159947346],\n",
      " (1, 1): [0.5495389999999978,\n",
      "          0.281588449069307,\n",
      "          0.37096672165404077,\n",
      "          0.3054947869955743],\n",
      " (1, 2): [0.11302993450282967,\n",
      "          0.3709290976292174,\n",
      "          0.621709999999998,\n",
      "          0.20316436608149402],\n",
      " (1, 3): [-0.0036991000000000003,\n",
      "          -0.004250727739,\n",
      "          0.3781869317438103,\n",
      "          -0.00402772591],\n",
      " (1, 4): [-0.0029701, -0.0029701, 0.024190372277221837, -0.003058309],\n",
      " (2, 0): [0.07609659991200401,\n",
      "          -0.009542898323042606,\n",
      "          -0.009595852804621,\n",
      "          -0.011428653290287489],\n",
      " (2, 1): [0.5998485782689155,\n",
      "          -0.004545687104277069,\n",
      "          0.07069148555930925,\n",
      "          0.07861340292910267],\n",
      " (2, 2): [0.4215636020645643,\n",
      "          0.3691427178622258,\n",
      "          0.7018999999999982,\n",
      "          0.290836797781339],\n",
      " (2, 3): [0.07143523386830536, -0.00199, 0.7683762471250394, -0.002071],\n",
      " (2, 4): [0.07942005553355647,\n",
      "          0.02533641141377781,\n",
      "          0.7664255038043417,\n",
      "          -0.0014717479028328998],\n",
      " (3, 0): [-0.0050931710988178945,\n",
      "          -0.0057817049401,\n",
      "          -0.0062242831,\n",
      "          -0.006561343269907079],\n",
      " (3, 1): [0.5138162307533584,\n",
      "          -0.00405629913061,\n",
      "          -0.003668482,\n",
      "          -0.0044005881961],\n",
      " (3, 2): [0.7909999999999986,\n",
      "          0.17676568485171706,\n",
      "          0.18616909175910626,\n",
      "          0.425146475849594],\n",
      " (3, 3): [0.889999999999999,\n",
      "          0.5258452110008007,\n",
      "          0.45704092398048385,\n",
      "          0.37580740251935046],\n",
      " (3, 4): [0.751507892521671,\n",
      "          0.3700978679371475,\n",
      "          0.9999999999999996,\n",
      "          0.43435694187189844],\n",
      " (4, 0): [-0.0041338027000000005, -0.00385219, -0.003940399, -0.004245826672],\n",
      " (4, 1): [0.007183574000000002, -0.002071, -0.0029701, -0.0020791000000000004],\n",
      " (4, 2): [0.506204461060602, -0.001, -0.001, 0.06927613142263524],\n",
      " (4, 3): [0.9353891811077333, 0, 0.023390000000000005, -0.001],\n",
      " (4, 4): [0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "class CustomEnv:\n",
    "    def __init__(self):\n",
    "        self.states = [(i, j) for i in range(5) for j in range(5)]\n",
    "        self.goal = (4, 4)\n",
    "        self.state = (0, 0)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0: y += 1  # Right\n",
    "        elif action == 1: y -= 1  # Left\n",
    "        elif action == 2: x += 1  # Down\n",
    "        elif action == 3: x -= 1  # Up\n",
    "        # The the agent keep in the same state if \n",
    "        # the action inplies over the max or min values of the enviroment\n",
    "        self.state = (max(0, min(x, 4)), max(0, min(y, 4)))\n",
    "\n",
    "        reward = 1 if self.state == self.goal else -0.01\n",
    "        done = self.state == self.goal\n",
    "        return self.state, reward, done\n",
    "\n",
    "# Initialize environment and Q-table\n",
    "env = CustomEnv()\n",
    "\n",
    "# Initialize Q-table\n",
    "# state:[0, 0, 0, 0], point out the Q-values are\n",
    "# zeros for each state and for each action.\n",
    "# The index of [0, 0, 0, 0] represent the actions\n",
    "q_table = {state: [0, 0, 0, 0] for state in env.states}  \n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.1\n",
    "\n",
    "def policy(state):\n",
    "    if random.uniform(0, 1) <= epsilon:\n",
    "        # print(\"Explore\")\n",
    "        return random.choice(range(4))\n",
    "    else:\n",
    "        # print(\"Explotation\")\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "for episode in range(500):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = policy(state)\n",
    "        # action = np.argmax(\n",
    "        #     q_table[state]) if random.uniform(0, 1) > epsilon else random.choice(range(4))\n",
    "        # 1. Let `state` be the current state\n",
    "        # 2. Based on this we take the action `action`\n",
    "        # 3. This return `next_state` and its `reward` \n",
    "        # 4. Instead take the same action (like SARSA), take the action that max Q\n",
    "        #    not like current policy\n",
    "        next_state, reward, done = env.step(action)\n",
    "        # print(state, action, next_state)\n",
    "        q_table[state][action] += learning_rate * (\n",
    "            reward + discount_factor * max(q_table[next_state]) - q_table[state][action])\n",
    "        state = next_state\n",
    "\n",
    "pprint(\"Trained Q-Table:\")\n",
    "pprint(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Learning (DQL)\n",
    "\n",
    "Deep Q-Learning replace Q-Table with a Deep Neural Network (DNN) that approximates the *Q-function*\n",
    "\n",
    "Instead storing Q-values in a table, a NN takes a state a input and output Q-values for all possibles actions.\n",
    "\n",
    "$$Q(s,a, \\theta)=Q^*(s,a)$$\n",
    "\n",
    "where $\\theta$ represent the weights.\n",
    "\n",
    "DQL Algorithm steps:\n",
    "1. Initialize a NN $Q(s, a, \\theta)$ randomly\n",
    "2. For each episode:\n",
    "   * Observe the current state $s$\n",
    "   * Choose an action using an $\\epsilon -\\text{greedy}\\; \\text{policy}$ (explotation vs exploration)\n",
    "   * Excecute $a$, receive reward $r$ and the next state $s'$ \n",
    "   * Sample a batch from the Replay Buffer.\n",
    "   * Compute the Target-Q-value: $y = r + \\gamma \\max_{a'} Q(s', a';\\theta)$\n",
    "   * Compute the Loss: $L = (y - Q(s, a,;\\theta))^2$\n",
    "   * Update NN weights $\\theta$ using backpropagation\n",
    "   * Periodically update target NN $Q(s, a;\\theta)$ to stabilize learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Loss: 0.0000\n",
      "Episode 10, Loss: 0.0188\n",
      "Episode 20, Loss: 0.0001\n",
      "Episode 30, Loss: 0.0001\n",
      "Episode 40, Loss: 0.0010\n",
      "Episode 50, Loss: 0.0011\n",
      "Episode 60, Loss: 0.0002\n",
      "Episode 70, Loss: 0.0002\n",
      "Episode 80, Loss: 0.0003\n",
      "Episode 90, Loss: 0.0002\n",
      "Episode 100, Loss: 0.0002\n",
      "Episode 110, Loss: 0.0000\n",
      "Episode 120, Loss: 0.0000\n",
      "Episode 130, Loss: 0.0000\n",
      "Episode 140, Loss: 0.0001\n",
      "Episode 150, Loss: 0.0000\n",
      "Episode 160, Loss: 0.0000\n",
      "Episode 170, Loss: 0.0000\n",
      "Episode 180, Loss: 0.0000\n",
      "Episode 190, Loss: 0.0000\n",
      "Episode 200, Loss: 0.0000\n",
      "Episode 210, Loss: 0.0000\n",
      "Episode 220, Loss: 0.0000\n",
      "Episode 230, Loss: 0.0000\n",
      "Episode 240, Loss: 0.0000\n",
      "Episode 250, Loss: 0.0000\n",
      "Episode 260, Loss: 0.0000\n",
      "Episode 270, Loss: 0.0000\n",
      "Episode 280, Loss: 0.0000\n",
      "Episode 290, Loss: 0.0000\n",
      "Episode 300, Loss: 0.0000\n",
      "Episode 310, Loss: 0.0000\n",
      "Episode 320, Loss: 0.0000\n",
      "Episode 330, Loss: 0.0000\n",
      "Episode 340, Loss: 0.0000\n",
      "Episode 350, Loss: 0.0000\n",
      "Episode 360, Loss: 0.0000\n",
      "Episode 370, Loss: 0.0000\n",
      "Episode 380, Loss: 0.0000\n",
      "Episode 390, Loss: 0.0000\n",
      "Episode 400, Loss: 0.0000\n",
      "Episode 410, Loss: 0.0000\n",
      "Episode 420, Loss: 0.0000\n",
      "Episode 430, Loss: 0.0000\n",
      "Episode 440, Loss: 0.0000\n",
      "Episode 450, Loss: 0.0000\n",
      "Episode 460, Loss: 0.0000\n",
      "Episode 470, Loss: 0.0000\n",
      "Episode 480, Loss: 0.0000\n",
      "Episode 490, Loss: 0.0000\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Define Deep Q-Network (DQN)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Custom environment\n",
    "class CustomEnv:\n",
    "    def __init__(self):\n",
    "        self.states = [(i, j) for i in range(5) for j in range(5)]\n",
    "        self.goal = (4, 4)\n",
    "        self.state = (0, 0)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self._state_to_tensor()\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0: y += 1  # Right\n",
    "        elif action == 1: y -= 1  # Left\n",
    "        elif action == 2: x += 1  # Down\n",
    "        elif action == 3: x -= 1  # Up\n",
    "        self.state = (max(0, min(x, 4)), max(0, min(y, 4)))\n",
    "        reward = 1 if self.state == self.goal else -0.01\n",
    "        done = self.state == self.goal\n",
    "        return self._state_to_tensor(), reward, done\n",
    "    \n",
    "    def _state_to_tensor(self):\n",
    "        return torch.tensor([self.state[0] / 4, self.state[1] / 4], dtype=torch.float32)\n",
    "\n",
    "# Hyperparameters\n",
    "epsilon = 0.1  # Exploration factor\n",
    "learning_rate = 0.001\n",
    "discount_factor = 0.9\n",
    "batch_size = 32\n",
    "buffer_capacity = 1000\n",
    "episodes = 500\n",
    "\n",
    "# Initialize environment and networks\n",
    "env = CustomEnv()\n",
    "state_dim = 2  # (x, y) coordinates\n",
    "num_actions = 4\n",
    "\n",
    "policy_net = DQN(state_dim, num_actions)\n",
    "target_net = DQN(state_dim, num_actions)\n",
    "\n",
    "# Both start with the same parameters\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "replay_buffer = deque(maxlen=buffer_capacity)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(range(num_actions))\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = torch.argmax(policy_net(state)).item()\n",
    "        \n",
    "        next_state, reward, done = env.step(action)\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        \n",
    "        # Training the network\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            \n",
    "            states = torch.stack(states)\n",
    "            actions = torch.tensor(actions, dtype=torch.long)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "            next_states = torch.stack(next_states)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32)\n",
    "            # state => Q_values => Q_value of chosen action (target action)\n",
    "            # Similar to Q_values[state][action]\n",
    "            q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "            # state => Q_values => max Q_value \n",
    "            # Similar to max(Q_values[next_state])\n",
    "            \n",
    "            next_q_values = target_net(next_states).max(1)[0]\n",
    "            # As reward and discount_factor are constant the only parameter\n",
    "            # to update is theta\n",
    "            target_q_values = rewards + discount_factor * next_q_values * (1 - dones)\n",
    "            \n",
    "            # Try to fit the parameter in order to q_values => target_q_values.detach()\n",
    "            # as target_q_values contains the target actions\n",
    "            loss = loss_fn(q_values, target_q_values.detach())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Update target network every 10 episodes\n",
    "    if episode % 10 == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        print(f\"Episode {episode}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2012.13773\n",
    "\n",
    "https://github.com/wassname/rl-portfolio-management?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "----G\n",
      "\n",
      "-----\n",
      "A----\n",
      "-----\n",
      "-----\n",
      "----G\n",
      "\n",
      "-----\n",
      "-----\n",
      "A----\n",
      "-----\n",
      "----G\n",
      "\n",
      "-----\n",
      "-----\n",
      "-A---\n",
      "-----\n",
      "----G\n",
      "\n",
      "-----\n",
      "-----\n",
      "--A--\n",
      "-----\n",
      "----G\n",
      "\n",
      "-----\n",
      "-----\n",
      "---A-\n",
      "-----\n",
      "----G\n",
      "\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "---A-\n",
      "----G\n",
      "\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "----A\n",
      "----G\n",
      "\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "----A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the Gridworld environment\n",
    "class Gridworld:\n",
    "    def __init__(self, grid_size, start, goal):\n",
    "        self.grid_size = grid_size\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.state = start\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0:  # Up\n",
    "            next_state = (max(x - 1, 0), y)\n",
    "        elif action == 1:  # Right\n",
    "            next_state = (x, min(y + 1, self.grid_size - 1))\n",
    "        elif action == 2:  # Down\n",
    "            next_state = (min(x + 1, self.grid_size - 1), y)\n",
    "        elif action == 3:  # Left\n",
    "            next_state = (x, max(y - 1, 0))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        reward = 1 if next_state == self.goal else -1\n",
    "        done = next_state == self.goal\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.zeros((self.grid_size, self.grid_size), dtype=str)\n",
    "        grid[:, :] = '-'\n",
    "        grid[self.goal] = 'G'\n",
    "        grid[self.state] = 'A'\n",
    "        print(\"\\n\".join([\"\".join(row) for row in grid]) + \"\\n\")\n",
    "\n",
    "# SARSA implementation\n",
    "def sarsa(env, episodes, alpha, gamma, epsilon):\n",
    "    q_table = np.zeros((env.grid_size, env.grid_size, 4))\n",
    "\n",
    "    def choose_action(state, epsilon):\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            return random.choice(range(4))  # Explore\n",
    "        else:\n",
    "            x, y = state\n",
    "            return np.argmax(q_table[x, y])  # Exploit\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        action = choose_action(state, epsilon)\n",
    "\n",
    "        while True:\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_action = choose_action(next_state, \n",
    "            )\n",
    "\n",
    "            x, y = state\n",
    "            nx, ny = next_state\n",
    "\n",
    "            # SARSA update rule\n",
    "            q_table[x, y, action] += alpha * (\n",
    "                reward + gamma * q_table[nx, ny, next_action] - q_table[x, y, action]\n",
    "            )\n",
    "\n",
    "            state, action = next_state, next_action\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    return q_table\n",
    "\n",
    "# Parameters\n",
    "grid_size = 5\n",
    "start = (0, 0)\n",
    "goal = (4, 4)\n",
    "episodes = 500\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "\n",
    "# Initialize and train\n",
    "env = Gridworld(grid_size, start, goal)\n",
    "q_table = sarsa(env, episodes, alpha, gamma, epsilon)\n",
    "\n",
    "# Render final policy\n",
    "state = env.reset()\n",
    "env.render()\n",
    "for _ in range(10):\n",
    "    action = np.argmax(q_table[state])\n",
    "    state, _, done = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done\n",
    "\n",
    "1. *Episode Terminatio*n: It signifies that the current sequence of interactions between the agent and the environment has concluded. An \"episode\" is a complete run from an initial state until a terminal state is reached.\n",
    "\n",
    "2. *Achieving a Goal*: In many tasks, \"done\" means the agent has successfully achieved its objective. For example:\n",
    "\n",
    "    * In a game of Chess, reaching checkmate.\n",
    "\n",
    "    * In a robotic arm task, picking up the object.\n",
    "\n",
    "    * In Frozen Lake, reaching the \"Goal\" (G) tile.\n",
    "\n",
    "3. *Failing the Task*: \"Done\" can also mean the agent has failed or entered an undesirable state, leading to the end of the episode. For example:\n",
    "\n",
    "    * Falling into a hole in Frozen Lake.\n",
    "\n",
    "    * Crashing a self-driving car.\n",
    "\n",
    "4. *Running out of moves or time in a game*.\n",
    "\n",
    "    * Reaching a Maximum Limit: Environments often have a maximum number of steps or a time limit per episode. If the agent hasn't reached a goal or failed state but hits this limit, the episode is also considered \"done.\" This is often distinguished as \"truncated\" in newer Gym/Gymnasium APIs (more on that below).\n",
    "\n",
    "\n",
    "*Done* vs. *Truncated* (in Gymnasium / Newer Gym versions):\n",
    "With the evolution of OpenAI Gym (now Gymnasium), the concept of \"done\" has been split into two distinct signals:\n",
    "\n",
    "* *done* (or terminated): This specifically indicates that the episode ended because the agent reached a natural terminal state (e.g., goal achieved, fell in a hole, crashed).\n",
    "\n",
    "* *truncated*: This indicates that the episode ended because a time limit or step limit was reached, even if the agent was still in a valid (non-terminal) state. The agent could have continued if the limit wasn't imposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, the ultimate goal of the agent is to improve its policy to acquire better rewards. *Policy improvement* in the optimization domain is called *policy optimization*.\n",
    "\n",
    "Apart from some linear or non linear methods, the *parameterization* of *value functions* with *deep neural networks* is one way of achieving **value function approximation**, and it‚Äôs the most popular way in the modern deep reinforcement learning domain.\n",
    "\n",
    "Value function approximation is useful because we cannot always acquire the true value function easily, and\n",
    "actually we cannot get the true function for most cases in practice.\n",
    "\n",
    "The gradient-based optimization methods can be used for improving parameterized policies, usually through the method called **policy gradient** in reinforcement learning terminology.\n",
    "\n",
    "The methods in policy optimization fall into two main categories: **(1) value-based optimization** methods like Q-learning, DQN, etc., which optimize the action-value function to obtain the preferences for the action choice, and **(2) policy-based optimization methods** like REINFORCE, the cross-entropy method, etc., which directly\n",
    "optimize the policy according to the sampled reward values.\n",
    "\n",
    "A combination of these two categories was found to be a more effective approach by people which forms one of the most widely used architecture in model-free reinforcement learning called **actor-critic**. Actor-critic methods employ the optimization of value function as the guidance of policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value-Based Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value-based optimization method always needs to alternate between\n",
    "value function estimation under the current policy and policy improvement with the estimated value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous sections, we see that the Q-learning can be used for\n",
    "solving some simple tasks in reinforcement learning. However, the realworld applications or even the quasi-real-world applications may have\n",
    "much larger and complicated state and action spaces, and the action is\n",
    "usually continuous in practice\n",
    "\n",
    "For example, the Go game has 10170 states. In these cases, the traditional lookup table method in Q-learning cannot\n",
    "work well with the *limitation of its scalability*, because each state will have an entry V‚Äâ(s) and each state-action pair will need an entry Q(s, a). The values in the table are updated one-by-one in practice. Therefore the *requirement of the memory and computational resources will be huge with tabular-based Q-learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply the value-based reinforcement learning in relatively largescale tasks, function approximators are applied to handle the above\n",
    "limitations\n",
    "- **Linear Methods**:\n",
    "    - Linear function approximators are used to approximate the value function, which is a linear combination of the features extracted from the state or state-action pairs.\n",
    "    - The linear function approximator can be expressed as:\n",
    "    \n",
    "    $$\n",
    "    V(s) = \\theta^T \\phi(s)\n",
    "    $$\n",
    "    \n",
    "    where $\\theta$ is the weight vector and $\\phi(s)$ is the feature vector for state $s$.\n",
    "    - Polinomial function approximators are used to approximate the value function, which is a polynomial function of the features extracted from the state or state-action pairs.\n",
    "    - The polynomial function approximator can be expressed as:\n",
    "    $$    V(s) = \\sum_{i=0}^{n} \\theta_i \\phi_i(s)$$\n",
    "\n",
    "    - Foureier basis function approximators are used to approximate the value function, which is a linear combination of the Fourier basis functions.\n",
    "    - The Fourier basis function approximator can be expressed as:\n",
    "    $$    V(s) = \\sum_{i=0}^{n} \\theta_i \\cos(\\omega_i s + \\phi_i)$$\n",
    "    for $s‚Äâ‚àà‚Äâ[0, 1]$ and $i‚Äâ=‚Äâ0, ‚Ä¶, n$\n",
    "\n",
    "    - Coarse coding is a method that uses a coarse representation of the state space to approximate the value function. It is used to reduce the dimensionality of the state space and make the value function approximation more efficient.\n",
    "    - The coarse coding method can be expressed as:\n",
    "    $$    V(s) = \\sum_{i=0}^{n} \\theta_i \\phi_i(s)$$\n",
    "    where $\\phi_i(s)$ is the coarse representation of the state $s$ and $\\theta_i$ is the weight vector. And the coase representation is defined as:\n",
    "    $$\\phi_i(s) = \\begin{cases}\n",
    "        1 & \\text{if } s \\in C_i \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases}$$\n",
    "    \n",
    "    - Radial basis functions (RBF) are used to approximate the value function, which is a linear combination of the radial basis functions.\n",
    "    - The RBF function approximator can be expressed as:\n",
    "    $$    V(s) = \\sum_{i=0}^{n} \\theta_i \\phi_i(s)$$\n",
    "    where $\\phi_i(s)$ is the radial basis function for state $s$ and $\\theta_i$ is the weight vector. The radial basis function is defined as:\n",
    "    $$\\phi_i(s) = \\exp\\left(-\\frac{||s - c_i||^2}{2\\sigma_i^2}\\right)$$\n",
    "    where $c_i$ is the center of the radial basis function and $\\sigma_i$ is the width of the radial basis function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Non-linear methods**\n",
    "  - Artificial neural networks: different from the above function approximation methods, artificial neural networks are widely used as\n",
    "non-linear function approximators, which are proven to have universal approximation ability under certain conditions.\n",
    "\n",
    "* **Other methods**\n",
    "  - Decision trees: decision trees can be used to approximate the value function by partitioning the state space into different regions and assigning a value to each region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of using function approximation:\n",
    "- **Scalability**: Function approximation can handle large state and action spaces, which is essential for real-world applications.\n",
    "- **Generalization**: Function approximation can generalize the value function to unseen states, which is crucial for tasks with continuous or high-dimensional state spaces.\n",
    "- **Efficiency**: Function approximation can reduce the memory and computational requirements, making it feasible to apply reinforcement learning to complex tasks.\n",
    "\n",
    "(ANN  also redice the need for manually defined features, which is a common requirement in traditional machine learning methods)\n",
    "\n",
    "Types of Value-Based Optimization Methods\n",
    "- **Model-free methods**: These methods do not require a model of the environment and learn the value function directly from the data. They are often used in reinforcement learning tasks where the environment is complex and difficult to model. The parameters w of the approximators can be updated with Monte Carlo (MC) or TD learning\n",
    "- **Model-based methods**: These methods require a model of the environment and use it to learn the value function. They are often used in reinforcement learning tasks where the environment is simple and easy to model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *most practical approximation method* for present DRL algorithms is using the *neural network*, for its great **scalability** and **generalization** for\n",
    "various specific functions. \n",
    "\n",
    "A neural network is a *differential method* with *gradient-based optimization*, which has a guarantee of *convergence* to optimum within convex cases and can achieve near-optimal solutions for some non-convex functions approximation.\n",
    "\n",
    "Drawbacks: it may require a large amount of data for training in practice and may cause other difficulties.\n",
    "\n",
    "Challenges of useing Value Function Approximation\n",
    "- Most supervised learning methods are not suitable for reinforcement learning tasks, because the data is not independent and identically distributed (i.i.d.) and the data is not labeled.\n",
    "- The data is generated by the agent's interaction with the environment, which is not i.i.d. (The next row depends on the previous rows).\n",
    "- The data is not stationary, which means the distribution of the data changes over time. The value function is estimated with current policy, or at least the state-visitfrequency determined by current policy, and the policy is updated all the\n",
    "time during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Reward: -37.50, Explore: 0.30\n",
      "Episode: 50, Reward: -37.30, Explore: 0.23\n",
      "Episode: 100, Reward: 0.40, Explore: 0.18\n",
      "Episode: 150, Reward: -134.30, Explore: 0.14\n",
      "Episode: 200, Reward: -165.90, Explore: 0.11\n",
      "Episode: 250, Reward: 7.60, Explore: 0.09\n",
      "Episode: 300, Reward: -88.50, Explore: 0.07\n",
      "Episode: 350, Reward: -185.20, Explore: 0.05\n",
      "Episode: 400, Reward: 3.50, Explore: 0.04\n",
      "Episode: 450, Reward: 0.70, Explore: 0.03\n",
      "Episode: 500, Reward: -131.80, Explore: 0.02\n",
      "Episode: 550, Reward: -4.10, Explore: 0.02\n",
      "Episode: 600, Reward: 1.80, Explore: 0.01\n",
      "Episode: 650, Reward: -55.90, Explore: 0.01\n",
      "Episode: 700, Reward: -295.70, Explore: 0.01\n",
      "Episode: 750, Reward: -91.70, Explore: 0.01\n",
      "Episode: 800, Reward: 2.00, Explore: 0.01\n",
      "Episode: 850, Reward: -18.60, Explore: 0.01\n",
      "Episode: 900, Reward: 0.90, Explore: 0.01\n",
      "Episode: 950, Reward: -210.70, Explore: 0.01\n",
      "Test 1: Reward: -24.60\n",
      "Test 2: Reward: -195.80\n",
      "Test 3: Reward: -18.50\n",
      "Test 4: Reward: 3.70\n",
      "Test 5: Reward: 1.60\n",
      "Test 6: Reward: 1.50\n",
      "Test 7: Reward: 0.30\n",
      "Test 8: Reward: 0.40\n",
      "Test 9: Reward: -37.60\n",
      "Test 10: Reward: 5.40\n",
      "Average test reward: -26.36\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy-Based Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of Policy-Based and Value-Based Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning (RL), using decision trees for value function approximation is an alternative to neural networks, particularly useful in scenarios where interpretability or computational efficiency is desired. Decision trees can approximate the Q-value function \\( Q(s, a) \\) or the value function \\( V(s) \\) by discretizing the state-action space and predicting values based on feature splits. This approach is less common in modern deep RL but can be effective in environments with discrete or low-dimensional state spaces.\n",
    "\n",
    "Below, I provide a **real example** of an RL algorithm using decision trees for Q-value approximation, applied to the **CartPole-v1** environment from Gymnasium. The algorithm is based on **Q-learning** with a decision tree regressor to approximate the Q-function. Since the query mentions QMIX (a multi-agent RL algorithm), I‚Äôll focus on a single-agent case for simplicity but note that extending this to multi-agent settings (like QMIX) would involve using trees for each agent‚Äôs Q-function and a mixing mechanism, which I can elaborate on if needed.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Decision Trees in RL**:\n",
    "   - A decision tree splits the state-action space into regions based on feature thresholds.\n",
    "   - Each leaf node represents a Q-value for a state-action pair.\n",
    "   - The tree is trained on (state, action, target Q-value) tuples, where targets are computed using the Q-learning update:\n",
    "     \\[\n",
    "     Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)\n",
    "     \\]\n",
    "   - Features for the tree include the state variables and the action (discretized or encoded).\n",
    "\n",
    "2. **Advantages**:\n",
    "   - **Interpretability**: The tree structure shows how states and actions map to Q-values.\n",
    "   - **Simplicity**: Suitable for discrete or low-dimensional spaces.\n",
    "   - **Efficiency**: Faster to train than neural networks for small datasets.\n",
    "\n",
    "3. **Challenges**:\n",
    "   - **Scalability**: Trees struggle with high-dimensional or continuous state spaces due to exponential growth in splits.\n",
    "   - **Overfitting**: Trees can overfit noisy RL data, requiring regularization (e.g., max depth, minimum samples per leaf).\n",
    "   - **Multi-Agent Extension**: For QMIX, each agent would need a tree-based Q-function, and the mixing network would combine their outputs, potentially using a neural network or another tree.\n",
    "\n",
    "4. **CartPole-v1 Environment**:\n",
    "   - **State**: Continuous, 4-dimensional (cart position, cart velocity, pole angle, pole angular velocity).\n",
    "   - **Actions**: Discrete, 2 actions (push left, push right).\n",
    "   - **Reward**: +1 for each step the pole remains balanced, up to 500 steps.\n",
    "   - **Termination**: Episode ends if the pole angle exceeds ¬±12 degrees, cart position exceeds ¬±2.4, or 500 steps are reached.\n",
    "\n",
    "### Python Example: Q-Learning with Decision Tree Approximation\n",
    "\n",
    "This example implements Q-learning with a decision tree regressor from scikit-learn to approximate the Q-function in the CartPole-v1 environment. The decision tree takes the state and action as input features and predicts the Q-value. The code includes training and evaluation to demonstrate a practical RL setup.\n",
    "\n",
    "\n",
    "\n",
    "### Explanation of the Code\n",
    "\n",
    "- **QLearningTreeAgent**:\n",
    "  - **Initialization**: Sets up a decision tree regressor (`DecisionTreeRegressor`) with a maximum depth and minimum samples per leaf to prevent overfitting. The replay buffer stores experiences.\n",
    "  - **Features**: The input to the tree is a vector combining the state (4D for CartPole) and action (scalar, 0 or 1), resulting in a 5D feature vector.\n",
    "  - **Act**: Uses Œµ-greedy exploration to select actions. Q-values are predicted by querying the tree for each possible action.\n",
    "  - **Predict Q-Values**: For a given state, predicts Q-values for all actions by appending each action to the state and querying the tree.\n",
    "  - **Store**: Saves experiences (state, action, reward, next_state, done) in the replay buffer.\n",
    "  - **Train**: Samples a batch, computes Q-learning targets using the Bellman equation, and updates the training data. The decision tree is retrained on all collected data to approximate the Q-function.\n",
    "  - **Epsilon Decay**: Reduces exploration over time to focus on exploitation.\n",
    "\n",
    "- **Environment**:\n",
    "  - **CartPole-v1**: A standard Gymnasium environment where the agent balances a pole on a cart. The state is continuous, but the decision tree handles it by learning splits on the state variables.\n",
    "\n",
    "- **Training Loop**:\n",
    "  - Runs for 500 episodes, with a maximum of 500 steps per episode.\n",
    "  - Collects experiences, trains the tree, and decays epsilon.\n",
    "  - Prints the average reward over the last 50 episodes every 50 episodes.\n",
    "  - Plots a smoothed reward curve to visualize learning progress.\n",
    "\n",
    "- **Decision Tree**:\n",
    "  - The `DecisionTreeRegressor` predicts Q-values based on state-action pairs.\n",
    "  - `max_depth=5` and `min_samples_leaf=5` control tree complexity to avoid overfitting.\n",
    "  - The tree is retrained periodically on all collected data, which is a simple but effective approach for small datasets.\n",
    "\n",
    "### Output and Expected Behavior\n",
    "\n",
    "- **Performance**: The agent should learn to balance the pole, achieving rewards close to 500 (the maximum episode length) after 200‚Äì300 episodes. The decision tree‚Äôs performance may be less stable than a neural network due to its discrete splits, but it can achieve decent results in CartPole.\n",
    "- **Reward Plot**: The plot shows the 50-episode moving average of rewards, indicating learning progress. Expect rewards to increase from ~20 (random policy) to ~400‚Äì500 as the agent learns.\n",
    "- **Interpretability**: You can inspect the trained decision tree (e.g., using `sklearn.tree.plot_tree`) to see how it splits the state-action space, offering insights into the learned policy.\n",
    "\n",
    "### Extending to QMIX\n",
    "\n",
    "To adapt this to a multi-agent QMIX setting with decision trees:\n",
    "1. **Agent Q-Functions**: Use a separate `DecisionTreeRegressor` for each agent‚Äôs Q-function, taking the agent‚Äôs observation and action as input.\n",
    "2. **Mixing Network**: Retain a neural network for the mixing function to combine individual Q-values, as in standard QMIX, since trees are less suited for combining continuous outputs while enforcing monotonicity.\n",
    "3. **Training**: Update each agent‚Äôs tree using the QMIX TD loss, where the target is computed via the mixing network:\n",
    "   \\[\n",
    "   L = \\left( r + \\gamma \\max_{\\mathbf{a}'} Q_{tot}(s', \\mathbf{a}') - Q_{tot}(s, \\mathbf{a}) \\right)^2\n",
    "   \\]\n",
    "4. **Challenges**: Trees may struggle with high-dimensional observations or complex coordination tasks, requiring careful feature engineering or ensemble methods (e.g., random forests).\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Limitations**: Decision trees are less flexible than neural networks for continuous or high-dimensional spaces. For complex environments, consider ensemble methods (e.g., random forests) or neural networks.\n",
    "- **Environment Choice**: CartPole is a simple environment for demonstration. For multi-agent tasks, use environments like the StarCraft Multi-Agent Challenge (SMAC) with a modified QMIX setup.\n",
    "- **Tuning**: Adjust `max_depth`, `min_samples_leaf`, and `epsilon_decay` based on the environment‚Äôs complexity.\n",
    "- **Visualization**: To interpret the tree, you can use `sklearn.tree.plot_tree(agent.q_tree)` or export the tree structure for analysis.\n",
    "\n",
    "If you want to extend this to a multi-agent environment, integrate with a specific multi-agent task, or explore other tree-based methods (e.g., random forests), let me know, and I can provide a tailored example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Avg Reward (last 50): 19.00, Epsilon: 0.995\n",
      "Episode 50, Avg Reward (last 50): 21.70, Epsilon: 0.774\n",
      "Episode 100, Avg Reward (last 50): 18.80, Epsilon: 0.603\n",
      "Episode 150, Avg Reward (last 50): 14.50, Epsilon: 0.469\n",
      "Episode 200, Avg Reward (last 50): 12.64, Epsilon: 0.365\n",
      "Episode 250, Avg Reward (last 50): 11.50, Epsilon: 0.284\n",
      "Episode 300, Avg Reward (last 50): 10.82, Epsilon: 0.221\n",
      "Episode 350, Avg Reward (last 50): 10.44, Epsilon: 0.172\n",
      "Episode 400, Avg Reward (last 50): 10.06, Epsilon: 0.134\n",
      "Episode 450, Avg Reward (last 50): 9.78, Epsilon: 0.104\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Q-learning agent with decision tree approximation\n",
    "class QLearningTreeAgent:\n",
    "    def __init__(self, state_dim, n_actions, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1, max_depth=5):\n",
    "        self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.q_tree = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=5, random_state=42)\n",
    "        self.replay_buffer = deque(maxlen=10000)\n",
    "        self.X_train = []  # State-action pairs\n",
    "        self.y_train = []  # Q-value targets\n",
    "    \n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        q_values = self._predict_q_values(state)\n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "    def _predict_q_values(self, state):\n",
    "        # Predict Q-values for all actions\n",
    "        q_values = np.zeros(self.n_actions)\n",
    "        for action in range(self.n_actions):\n",
    "            state_action = np.append(state, action).reshape(1, -1)\n",
    "            q_values[action] = self.q_tree.predict(state_action)[0] if len(self.X_train) > 0 else 0.0\n",
    "        return q_values\n",
    "    \n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train(self, batch_size=32):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.replay_buffer, batch_size)\n",
    "        \n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            # Compute target Q-value\n",
    "            next_q_values = self._predict_q_values(next_state)\n",
    "            target = reward if done else reward + self.gamma * np.max(next_q_values)\n",
    "            current_q = self._predict_q_values(state)[action]\n",
    "            updated_q = current_q + self.alpha * (target - current_q)\n",
    "            \n",
    "            # Add to training data\n",
    "            state_action = np.append(state, action)\n",
    "            X_batch.append(state_action)\n",
    "            y_batch.append(updated_q)\n",
    "        \n",
    "        # Update training data\n",
    "        self.X_train.extend(X_batch)\n",
    "        self.y_train.extend(y_batch)\n",
    "        \n",
    "        # Retrain decision tree\n",
    "        if len(self.X_train) > 0:\n",
    "            self.q_tree.fit(np.array(self.X_train), np.array(self.y_train))\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "# Training and evaluation\n",
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_dim = env.observation_space.shape[0]  # 4 for CartPole\n",
    "    n_actions = env.action_space.n  # 2 for CartPole\n",
    "    agent = QLearningTreeAgent(state_dim=state_dim, n_actions=n_actions)\n",
    "    \n",
    "    n_episodes = 500\n",
    "    max_steps = 500\n",
    "    rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            agent.store(state, action, reward, next_state, done)\n",
    "            agent.train(batch_size=32)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        rewards.append(total_reward)\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            avg_reward = np.mean(rewards[-50:]) if rewards else 0\n",
    "            print(f\"Episode {episode}, Avg Reward (last 50): {avg_reward:.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Plot average rewards\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(np.convolve(rewards, np.ones(50)/50, mode='valid'))\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward (50-episode moving avg)')\n",
    "    plt.title('Q-Learning with Decision Tree on CartPole-v1')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.55*3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning\n",
    "https://arxiv.org/pdf/1803.11485"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Reinforcement Learning 5*: Function Approximation and Deep Reinforcement Learning:\n",
    "    https://www.youtube.com/watch?v=wAk1lxmiW4c&list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb&index=5\n",
    "- *Reinforcement Learning 10*: Classic Games Case Study:\n",
    "    https://www.youtube.com/watch?v=ld28AU7DDB4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
