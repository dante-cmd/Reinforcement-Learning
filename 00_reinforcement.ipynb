{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **agent** and **enviroment**\n",
    "\n",
    "The environment is an entity that the agent can interact with, e.g. Pong game.\n",
    "\n",
    "The agent controls the paddle to hit the ball back and forth. An agent can ‚Äúinteract‚Äù with the environment by using a predefined **action set**: $A = \\{A_1, A_2, ... \\}$ (all possible actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p>The goal of reinforcement learning algorithms is to teach the agent how to interact ‚Äúwell‚Äù with the environment so that the agent is able to obtain a good score under a predefined evaluation metric</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent receive a reward $r$ of $1$ when the ball hits the wall on the opposite side. \n",
    "\n",
    "At an arbitrary time step (a point at which observations can be made), $t$, the agent first observes the current state of the environment, $S_t$, and the corresponding reward value, $R_t$.\n",
    "\n",
    "The agent then decides what to do next based on the state and reward information. The action the agent intends to perform, $A_t$, gets fed back into the environment such that we can obtain the new state $S_{t+1}$ and reward $R_{t+1}$.\n",
    "\n",
    "$$(S_t,R_t) \\rightarrow A_t \\rightarrow (S_{t+1},R_{t+1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation of the environment state $s$($s$ is a general representation of state regardless of time step $t$) from the agent‚Äôs perspective does not always contain all the information about the environment. \n",
    "\n",
    "If the observation only contains partial state information, the environment is *partially observable*. Nevertheless, if the observation contains the complete state information of the environment, the environment is *fully observable*.\n",
    "\n",
    "The action $a$ ($a$ is a general representation of action regardless of time step $t$) is usually conditioned on the state $s$ to represent the behavior of the agent (Under assumption of fully observable environments.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide feedback from the environment to the agent, a reward function $R$ generates an immediate reward $R_t$ according to the environment status and sends it to the agent at every time step. $R_t=R(S_t)$.\n",
    "\n",
    "> Trajectory: \n",
    "> \n",
    "> A **trajectory** is defined: \n",
    "> \n",
    "> $\\tau = (S_0, R_0,  A_0, S_1, R_1, A_1,...)$\n",
    "> \n",
    "> A *trajectory*, being referred to also as an *episode*, is a sequence that goes from the initial state to the terminal state (for finite cases)\n",
    "\n",
    "The initial state in a trajectory, $S_0$, is randomly sampled from the *start-state distribution*, denoted by $œÅ_0$, in which:\n",
    "\n",
    "$$S_0 \\sim œÅ_0(.)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transition from a state (after taking an action) to the next state can be either *deterministic transition process* or *stochastic transition process*\n",
    "\n",
    "For the *deterministic transition*, the next state $S_{t+1}$ is governed by a deterministic function:\n",
    "\n",
    "$$S_{t+1} = f(S_t, A_t)$$\n",
    "\n",
    "For the *stochastic transition* process, the next state $S_{t+1}$ is described as a probabilistic distribution:\n",
    "\n",
    "$$S_{t+1} \\sim p(S_{t+1}|S_t, A_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploitation** means maximizing the agent performance using the existing knowledge, and its performance is usually evaluated by the expected reward. Given the actual knowledge, the agent doesn't take risk to explore.\n",
    "\n",
    "The policy he took here is the **greedy policy**, which means the agent constantly performs the action that yields the highest expected reward based on current information, rather than taking risky trials which may lead to lower expected rewards.\n",
    "\n",
    "**Exploration** means increasing existing knowledge by taking actions and interacting with the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov process (MP) is a *discrete stochastic process* with *Markov property*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"assets/markov-process-example.png\" height=360>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph simulates how a person works on two tasks and goes to bed in the end.\n",
    "\n",
    "If I'm doing the \"Task 1\" and exists the `30%` probability of stoping and go to play a \"Game\" and then, the probability of returning to do the \"Task 1\" is only `10%` and the probability of keep playing is `90%`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphical Model of a Markov Process\n",
    "<center>\n",
    "<img src=\"assets/graphical-model-mp.png\" height=80> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a \\rightarrow b$ indicates the variable $b$ is depended on a vaiable $a$\n",
    "\n",
    "The probabilistic graphical model can help us to have a more intuitive sense of the relationships between variables in reinforcement learning, as well as providing rigorous references when we derive the gradients with respect to different variables along the MP chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MP follows the assumption of **Markov chain** (*memoryless property*) where the next state $S_{t+1}$\n",
    "is only dependent on the current state $S_t$, with the probability of a state\n",
    "jumping to the next state described as follows:\n",
    "\n",
    "$$P(S_{t+1}|S_{t}) = P(S_{t+1}|S_0, ..., S_{t})$$\n",
    "\n",
    "Also the *time homogeneus property*\n",
    "\n",
    "$$P(S_{t+1}|S_{t}) = P(S_{t+2}|S_{t+1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a finite *state set* $S$, we can have *state transition matrix* $P$\n",
    "\n",
    "$ Ôº≥= \\{g, t_1, t_2, r, p, b\\}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"assets/matrix-transition.PNG\" height=170>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $P_{i,j}$ represents the probability of transferring the current state $S_i$ to the next state $S_j$.\n",
    "\n",
    "$$ P(s=t_1|s=g)=10\\%$$\n",
    "\n",
    "The sum of each row must be equal to $1$ and the $P$ is always a square matrix.\n",
    "\n",
    "A Markov Process can be represented by a tuple $<Ôº≥, P>$\n",
    "\n",
    "The next state is sample from $P$:\n",
    "\n",
    "$$S_{t+1} \\sim P_{S_t} $$\n",
    "\n",
    "For continuous case a finite matrix can not be used to represent the transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Reward Process\n",
    "\n",
    "We need to add feedback from the enviroment to the agent, so we extent\n",
    "\n",
    "$<Ôº≥, P>$ to $<Ôº≥, P, R, \\gamma>$, in which $R$ represent the *reward function* and $\\gamma$ *reward discount factor*.\n",
    "\n",
    "$$R_t = R(S_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Graphical Model of a Markov Process is updated to \n",
    "<center>\n",
    "<img src=\"assets/graphical-model-mp-reward.png\" height=170>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, if we are considering to move to the next state, we should take account the rewards of the following next states that we will take in order to reach the $T$.\n",
    "\n",
    "If a single trajectory $\\tau$ has $T$ time steps, and the current time step $t=0$, then **the return** is the cummulative reward discounted by $\\gamma \\in (0,1)$  of a trajectory.\n",
    "\n",
    "$$G_{t=0:T} = G_{t=0}^{(T)} = R(\\tau)_{t=0}^{(T)} =  R_1 + \\gamma R_2 + ... + \\gamma^{T-1} R_T$$\n",
    "\n",
    "<!-- $\\gamma R_1 + ... + \\gamma^{T} R_T$ is the discount factor of the future rewards that we get from the current time step $t=0$ following a set of actions ($a_1, a_2, ..., a_{T}$) -->\n",
    "\n",
    "where $R_t$ is the **immediate reward** at time step $t$, and $T$ represents the time step of the terminal state and $r$ as a general representation of immediate reward value.\n",
    "\n",
    "The discounted factor is especially critical when handling with infinite MRP cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function $V(s)$ represents the expected return from the state $S_0$ that take the state value $s$ until final state.\n",
    "\n",
    "<!-- $$V(s) = E[R_t|S_0=s]$$ -->\n",
    "$$V(s) = E[G_{t=0}^{(T)}|S_0=s]$$\n",
    "\n",
    "A simple way to estimate the $V(s)$ is Monte Carlo method, we can randomly sample a large number of trajectories starting from state $s$ according to the given state transition matrix $P$.\n",
    "\n",
    "If the agent acts according to the policy $œÄ$, we denote the *value function* as $V^œÄ(s)$\n",
    "\n",
    "\n",
    "<!-- tasks = ['Bed', 'Game', 'Pass', 'Rest', 'Task1', 'Task2']\n",
    "rewards = {'Bed':0, 'Game':-1, 'Pass':10, 'Rest':1, 'Task1':-2, 'Task2':-2}\n",
    "\n",
    "transition = np.array(\n",
    "    [\n",
    "        [1.0, 0.0, 1.0, 0.0, 0.0, 0.3],\n",
    "        [0.0, 0.9, 0.0, 0.0, 0.3, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.6],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.1],\n",
    "        [0.0, 0.1, 0.0, 0.1, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.9, 0.7, 0.0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for tk in  tasks:\n",
    "    n_iter = 1\n",
    "    print(\"V(S={})=\".format(tk), end='')\n",
    "    while n_iter<=10_000:    \n",
    "        array = []\n",
    "        task = tk\n",
    "        target = 'Bed'\n",
    "        G = 0\n",
    "        gamma = 0.9\n",
    "        t = 0\n",
    "        while True:\n",
    "            idx = tasks.index(task)\n",
    "            reward = rewards[task]\n",
    "            G = G + (gamma** t)*reward\n",
    "            t+=1\n",
    "            prob = transition[:, idx].copy()\n",
    "            task = np.random.choice(tasks, p=prob)\n",
    "            if task == target:\n",
    "                array.append(G)\n",
    "                n_iter+=1\n",
    "                break\n",
    "\n",
    "    print(\"{:.3f}\".format(sum(array)/len(array)))} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "tasks = ['Bed', 'Game', 'Pass', 'Rest', 'Task1', 'Task2']\n",
    "rewards = {'Bed':0, 'Game':-1, 'Pass':10, 'Rest':1, 'Task1':-2, 'Task2':-2}\n",
    "value_function = {'Bed':0, 'Game':0, 'Pass':0, 'Rest':0, 'Task1':0, 'Task2':0}\n",
    "\n",
    "transition = np.array(\n",
    "    [\n",
    "        [1.0, 0.0, 1.0, 0.0, 0.0, 0.3],\n",
    "        [0.0, 0.9, 0.0, 0.0, 0.3, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.6],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.1],\n",
    "        [0.0, 0.1, 0.0, 0.1, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.9, 0.7, 0.0]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(task:str, target:str):\n",
    "    def trajectory(task:str, target:str, data:list):\n",
    "        data.append(task)\n",
    "        if task == target:\n",
    "            return data\n",
    "        else:\n",
    "            idx = tasks.index(task)\n",
    "            next_task =np.random.choice(tasks, p=transition[:, idx])\n",
    "            return trajectory(next_task, target, data)\n",
    "\n",
    "    return trajectory(task, target, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tasks:\n",
    "    n_iter = 10_000\n",
    "    v = 0\n",
    "    while n_iter > 0:\n",
    "        # 1. Simulating trajectory\n",
    "        simulated_trajectory = get_trajectory(task, 'Bed')\n",
    "\n",
    "        # 2. Simulating rewards\n",
    "        simulated_rewards = np.array([rewards[x] for x in simulated_trajectory])\n",
    "\n",
    "        # 3. Computing return\n",
    "        calculated_return = np.sum(\n",
    "            np.cumprod(\n",
    "                np.ones_like(simulated_rewards)*gamma)/gamma *simulated_rewards)\n",
    "        v = v + calculated_return\n",
    "        n_iter -= 1\n",
    "    n_iter = 10_000\n",
    "    value_function[task] = v/n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bed': 0.0,\n",
       " 'Game': -5.922879947584755,\n",
       " 'Pass': 10.0,\n",
       " 'Rest': 3.9446244226534253,\n",
       " 'Task1': -1.199615908058622,\n",
       " 'Task2': 3.7855726236614347}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent policy usually selects the next state with higher value.\n",
    "\n",
    "E.g. if we are in the *Game* state, we only have, two options: *Game* or *Task1*, we should choice *Task1* (since the higher value of $V(S=\\text{Task1})$). And if we are in th *Task1* state, we would have two options: *Game* or *Task2*.  We should choice *Task2* (since the higher value of $V(S=\\text{Task2})$) and so on.\n",
    "\n",
    "$$\\text{Game} \\rightarrow \\text{Task1} \\rightarrow \\text{Task2} \\rightarrow \\text{Pass} \\rightarrow \\text{Bed}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Decision Process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actions taken above only depend on the expected value of discounted reward  given a state. The action that maximize this expected value is taken. \n",
    "\n",
    "But we do more granularity this expected value and compute at level of action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tuple $$< Ôº≥, P, R, \\gamma>$$ we add $Ôº°$\n",
    "\n",
    "$$< Ôº≥, Ôº°, P, R, \\gamma>$$\n",
    "\n",
    "$$ P(s'|s,a) = P(S_{t+1} = s'|S_{t}=s, A_t=a)$$\n",
    "\n",
    "Where $Ôº°$ represent *finite action set* $\\{a_0, a_1, a_2, a_3, ..., a_{T-1}\\}$ and the immediate reward becomes:\n",
    "\n",
    "$$R_{t+1} = R(S_t=s, A_t=a, S_{t+1}=s')$$\n",
    "\n",
    "<!-- Reward Function ($ R(s, a, s') $):  -->\n",
    "The immediate reward received when transitioning from state $ s $ to state $ s' $ after action $ a $.\n",
    "\n",
    "\n",
    "A *policy* $\\pi$ represents the way in which the agent behaves based on its observations of the enviroment.\n",
    "\n",
    "$$\\pi(a|s) = p(A_t=a|S_t = s)$$\n",
    "\n",
    "**Expected return** is the expectation of returns over all possible trajectories under a policy. Therefore, **the goal of reinforcement learning is to find the higher expected return by optimizing the policy**.\n",
    "\n",
    "The probability of the T-step trajectory for MDP is:\n",
    "Based on a behavior of the agent will generate a path or trajectory.\n",
    "\n",
    "$$p(\\tau|\\pi)_{t=0}^{T-1} = p_0(S_0) \\prod_{t=0}^{T-1} p(S_{t+1}|S_t, A_t)\\pi(A_t|S_t)$$\n",
    "\n",
    "If is given the initial state $S_0$, the probability of the T-step trajectory for MDP will updated to:\n",
    "\n",
    "$$ p(\\tau|\\pi)_{t=0}^{T-1} = \\prod_{t=0}^{T-1} p(S_{t+1}|S_t, A_t)\\pi(A_t|S_t)$$\n",
    "\n",
    "\n",
    "Given state $S_t$ and action $A_t$, if only exists way to go state $S_{t+1}$ the probability of the T-step trajectory is reduced to:\n",
    "\n",
    "$$ \\prod_{t=0}^{T-1}p(S_{t+1}|S_t, A_t)\\pi( A_t|S_t) =  \\prod_{t=0}^{T-1}p(S_{t+1}|S_t, A_t)$$\n",
    "\n",
    "<!-- $$p(S_{t+1}|S_t, A_t)\\pi( A_t|S_t) = \\prod_{t=0}^{T-1} \\pi( A_t|S_t)$$ -->\n",
    "\n",
    "Given the reward function $R$ and all possible trajectories $œÑ$, the expected return $J(œÄ)$ starting from time step $t=0$ is defined as follows\n",
    "\n",
    "\n",
    "$$J(\\pi) = \\sum_{\\tau} P(\\tau|\\pi)_{t=0}^{T-1} R(\\tau)_{t=0}^{T} = E_{\\tau \\sim \\pi}[R(\\tau)_{t=0}^{T}]$$\n",
    "\n",
    "\n",
    "The RL optimization problem is to improve the policy for maximizing the expected return with optimization methods. \n",
    "\n",
    "The **optimal policy** $œÄ^‚àó$ can be expressed as\n",
    "\n",
    "$$\\pi^* = \\argmax_{\\pi} J(\\pi)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given policy $\\pi$, the **value function** $V(s)$ can be defined as:\n",
    "\n",
    "$$V^{\\pi} (s) = E_{\\tau \\sim \\pi}[R(\\tau)_{t}^{T}|S_{t}=s]$$\n",
    "\n",
    "where $œÑ‚Äâ‚àº‚ÄâœÄ$ means the trajectories $œÑ$ are sampled given the policy $œÄ$\n",
    "\n",
    "In MDP, given an action, we have the **action-value function** $Q^\\pi(s, a)$, which depends on both the state and the action just taken\n",
    "\n",
    "$$Q^{\\pi} (s, a) = E_{\\tau \\sim \\pi}[R(\\tau)_{t}^{T}|S_{t}=s, A_{t}=a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to keep in mind that the $Q^œÄ(s, a)$ depends on $œÄ$, as the\n",
    "estimation of the value is an expectation over the trajectories by the policy\n",
    "$œÄ$. This also indicates if the $œÄ$ changes, the corresponding $Q^œÄ(s, a)$ will also change.\n",
    "\n",
    "We therefore usually call the value function estimated with a specific policy *the on-policy value function* (with lower case $q$), for the distinction from\n",
    "the optimal value function estimated with the optimal policy.\n",
    "\n",
    "$$q_{\\pi}(s,a) = E_{\\tau \\sim \\pi}[R(\\tau)_{t}^{T}|S_t=s, A_t=a]$$\n",
    "\n",
    "$$v_{\\pi}(s) = E_{a \\sim \\pi}[q_{\\pi}(s,a)] $$\n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_{a} \\pi(a|s)  q_{\\pi}(s,a) $$\n",
    "<!-- =E_{\\tau \\sim \\pi}[R(\\tau)|S_0=s] -->\n",
    "\n",
    "We can use *Monte Carlo method* to estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman Equation for **Value Function**\n",
    "\n",
    "Given state $s$ in time step $t$ ($s'$ in time step $t+1$) with a policy $\\pi$, and the all trajectories $\\tau$ upto time step $T$ with respective rewards, we can define  the value function  $v^{\\pi} (s)$\n",
    "\n",
    "$$v^{\\pi} (s) = E_{\\tau \\sim \\pi}[R(\\tau)_{t=t}^{T}|S_{t}=s] = \\sum_{\\tau} P(\\tau|\\pi)_{t=t}^{T-1} R(\\tau)_{t=t}^{T} $$\n",
    "\n",
    "\n",
    "\n",
    "We can split the above into \n",
    "1.  $P(\\tau|\\pi)_{t=t}^{T-1} =  P(s'|s, a) \\pi(a|s) P(\\tau|\\pi)_{t=t+1}^{T-1}$\n",
    "\n",
    "2. $R(\\tau)_{t=t}^{T} = R_{t+1} + \\gamma R(\\tau)_{t=t+1}^{T}$ (*Return* after take action $a$ in the time step $t$)\n",
    "\n",
    "\n",
    "$$v^{\\pi} (s) = \\sum_{\\tau} P(s'|s,a) \\pi(a|s) P(\\tau|\\pi)_{t=t+1}^{T-1} [R_{t+1} + \\gamma R(\\tau)_{t=t+1}^{T}]$$\n",
    "\n",
    "$$v^{\\pi} (s) = \\sum_{s'} P(s'|s,a) \\sum_{a} \\pi(a|s) \\sum_{\\tau} P(\\tau|\\pi)_{t=t+1}^{T-1} [R_{t+1} + \\gamma R(\\tau)_{t=t+1}^{T}]$$\n",
    "\n",
    "$$v^{\\pi} (s) = \\sum_{s'} P(s'|s,a) \\sum_{a} \\pi(a|s)[R_{t+1} + \\gamma \\sum_{\\tau} P(\\tau|\\pi)_{t=t+1}^{T-1} R(\\tau)_{t=t+1}^{T}]$$\n",
    "\n",
    "$$v^{\\pi} (s) = \\sum_{s'} P(s'|s,a) \\sum_{a} \\pi(a|s)[R_{t+1} + \\gamma v^{\\pi} (s')]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman Equation for **Value Action Function**\n",
    "\n",
    "Let be state $s$ in time step $t$ (state $s'$ and action $a'$ in time step $t$) with a policy $\\pi$, and the all trajectories $\\tau$ upto time step $T$ with respective rewards, we can define  the value function  $q^{\\pi} (s, a)$\n",
    "\n",
    "$$q^{\\pi} (s, a) = E_{\\tau \\sim \\pi}[R(\\tau)_{t=t+1}^{T}|S_{t}=s, A_{t}=a] = \\sum_{\\tau} P(\\tau|\\pi)_{t=t+1}^{T-1} R(\\tau)_{t=t+1}^{T} $$\n",
    "\n",
    "\n",
    "\n",
    "We can split the above into \n",
    "1.  $P(\\tau|\\pi)_{t=t+1}^{T-1} =  P(s'|s,a) \\pi(a|s) P(\\tau|\\pi)_{t=t+1}^{T-1}$\n",
    "\n",
    "2. $R(\\tau)_{t=t+1}^{T} = R_0 + \\gamma R(\\tau)_{t=t+1}^{T}$\n",
    "\n",
    "3. Since $s$  and $a$ are given, $\\pi(a|s) = 1$\n",
    "\n",
    "\n",
    "$$q^{\\pi} (s,a) = \\sum_{\\tau} P(s'|s,a) \\pi(a|s) P(\\tau|\\pi)_{t=t+1}^{T-1} [R_{t+1} + \\gamma R(\\tau)_{t=t+1}^{T}]$$\n",
    "\n",
    "$$q^{\\pi} (s,a) = \\sum_{s'} P(s'|s,a) \\sum_{\\tau} P(\\tau|\\pi)_{t=t+1}^{T-1}[ R_{t+1} + \\gamma R(\\tau)_{t=t+1}^{T}]$$\n",
    "\n",
    "$$q^{\\pi} (s,a) = \\sum_{s'} P(s'|s,a) [R_{t+1} + \\gamma \\sum_{\\tau} P(\\tau|\\pi)_{t=t+1}^{T-1} R(\\tau)_{t=t+1}^{T}]$$\n",
    "\n",
    "$$q^{\\pi} (s, a) = \\sum_{s'} P(s'|s,a) [R_{t+1} + \\gamma v^{\\pi} (s')]$$\n",
    "\n",
    "$$q^{\\pi} (s, a) = \\sum_{s'} P(s'|s,a) [R_{t+1} + \\gamma \\sum_{a'} \\pi(a'|s') q^{\\pi} (s',a')]$$\n",
    "\n",
    "<!-- #### Bellman Ecuation and Optimality\n",
    "\n",
    "Assumptions\n",
    "- Let $x_t$ be the state at same time $t$\n",
    "- The initial decision begin at $t=0$, so the intial state is $x_0$\n",
    "- The set of available actions  that depends on current state $a_t \\in \\Gamma(x_t)$\n",
    "- The next state after taken the action $a_t$ is $x_{t+1}=T(x_t, a_t)$\n",
    "- The payoff from taking the action $a_t$ is $F(x_t,a_t )$\n",
    "- Discount factor $0< \\beta<1$\n",
    "\n",
    "$V(x_0)$ denote the *optimal value* that can be obtained by maximizing this *objetive function* subject to contraints.\n",
    "\n",
    "$$V(x_0) = \\max_{\\{ a_t\\}_{t=0}^{\\infty}} \\sum_{t=0}^{\\infty} \\beta^t F(x_t, a_t)$$\n",
    "\n",
    "*Principle of Optimality*: An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. (See Bellman, 1957, Chap. III.3.)\n",
    "$$V(x_0) = \\max_{\\{ a_0\\}}[ F(x_0, a_0) +  \\max_{\\{ a_t\\}_{t=1}^{\\infty}}\\sum_{t=1}^{\\infty} \\beta^t F(x_t, a_t)]$$\n",
    "\n",
    "$$V(x_0) = \\max_{\\{ a_0\\}}[F(x_0, a_0)+V(x_1)]$$\n",
    "\n",
    "It reads from inner to outer, first maximize from the step $t=1$ to next, then add the payoff of the initial state and maximize it.  -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Value Functions\n",
    "\n",
    "Since on-policy **value functions** are estimated with respect to the policy\n",
    "itself, different policies will lead to different value functions, even for the\n",
    "same set of states and actions. Among all those different value functions,\n",
    "we define the optimal value function as\n",
    "\n",
    "$$v_*(s) = \\max_{\\pi} v_{\\pi}(s)$$\n",
    "\n",
    "For **action-value function**\n",
    "\n",
    "$$q_{*}(s, a) = \\max_{\\pi} q_{\\pi}(s, a)  $$\n",
    "\n",
    "We will update the policy $\\pi$ such that it converges to the optimal policy. We will update the matrix transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### Bellman Optimality Equation\n",
    "\n",
    "Bellman equation for optimal value functions (*Optimal value function* and *Optimal action-value function*)\n",
    "\n",
    "We choose the action (in which we can choose) that maximizes the expected return.\n",
    "\n",
    "Bellman equation for optimal value function\n",
    "\n",
    "$$v^{\\pi}_{*} (s) = \\max_{a} \\sum_{s'} P(s'|s,a) \\sum_{a} \\pi(a|s)[R_{t+1} + \\gamma \\max_{a'} v^{\\pi}_{*} (s')]$$\n",
    "\n",
    "Bellman equation for optimal action-value function\n",
    "\n",
    "The action $a$ is given for time $t$ so we can not maximize it.\n",
    "\n",
    "$$q^{\\pi}_{*} (s, a) = \\sum_{s'} P(s'|s,a) [R_{t+1} + \\gamma \\sum_{a'} \\pi(a'|s') \\max_{a'} q^{\\pi}_{*} (s',a')]$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Dynamic\" means that the problem has sequential or temporal components.\n",
    "\n",
    "\"Programming\" \n",
    "\n",
    "DP solves a complex problem by breaking it down into smaller subproblems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### üìò Full Explanation of **Dynamic Programming (DP)** in Reinforcement Learning (RL)\n",
    "\n",
    "**Dynamic Programming (DP)** is a fundamental technique in **Reinforcement Learning (RL)** used to solve problems where the agent can model the environment completely ‚Äî that is, when the agent knows all the **transition probabilities** and **reward functions**.\n",
    "\n",
    "It is one of the earliest and most classical approaches to solving **Markov Decision Processes (MDPs)**. -->\n",
    "\n",
    "<!-- ---\n",
    "\n",
    "## üîç What Is Dynamic Programming?\n",
    "\n",
    "In the context of RL:\n",
    "\n",
    "> **Dynamic Programming (DP)** refers to a collection of algorithms that can compute optimal policies **given a perfect model of the environment**, described as a **Markov Decision Process (MDP)**.\n",
    "\n",
    "--- -->\n",
    "\n",
    "Prerequisites for Using DP\n",
    "\n",
    "To apply DP methods in RL, you need:\n",
    "\n",
    "1. **Full knowledge of the environment**\n",
    "   - Transition probability $ P(s' | s, a) $\n",
    "   - Reward function $ R(s, a, s') $\n",
    "2. The problem must be expressible as an **MDP**\n",
    "3. Sufficient computational resources (DP scales poorly with large state spaces)\n",
    "\n",
    "Two properties that a *problem* must have for DP to be applicable:\n",
    "1. *Optimal substructure*: Optimal solution can be decomposed into optimal solutions for its sub-problems:\n",
    "2. *Overlapping sub-problems*: implies that the number of sub-problems is finite and the problem ocurr recursively so sub-solutions can be cached and reused.\n",
    "\n",
    "> Recursion is a programming technique that uses functions to solve problems by breaking them down into smaller, more manageable problems. It's a method that involves a function calling itself repeatedly until a base case is reached\n",
    "\n",
    "Model a problem as an MDP and using Bellman's recursion equation allow us meet the two properties of DP.\n",
    "\n",
    "\n",
    "\n",
    "<!-- --- -->\n",
    "\n",
    "<!-- ## üß† Core Concepts in DP for RL\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **Policy Evaluation** | Estimating the value function $ V^\\pi(s) $ for a given policy $ \\pi $ |\n",
    "| **Policy Improvement** | Improving a policy using the value function |\n",
    "| **Policy Iteration** | Alternating between policy evaluation and improvement |\n",
    "| **Value Iteration** | Directly computing the optimal value function without explicitly maintaining a policy | -->\n",
    "\n",
    "<!-- Markov Decision Process (MDP) Recap\n",
    "\n",
    "An MDP is defined by:\n",
    "\n",
    "- A set of **states**: $ \\mathcal{S} $\n",
    "- A set of **actions**: $ \\mathcal{A} $\n",
    "- A **transition probability function**:  \n",
    "  $ P(s' | s, a) = \\text{Probability of moving to state } s' \\text{ from state } s \\text{ after action } a $\n",
    "- A **reward function**:  \n",
    "  $ R(s, a, s') = \\text{Expected reward received after transitioning to } s' \\text{ from } s \\text{ via } a $\n",
    "- A **discount factor**:  \n",
    "  $ \\gamma \\in [0, 1] $ ‚Äî how much future rewards are valued compared to immediate ones\n",
    "\n",
    "---\n",
    "\n",
    "Bellman Equations: Foundation of DP\n",
    "\n",
    "DP relies on recursive equations known as **Bellman equations**.\n",
    "\n",
    "Value Function Under a Policy $ \\pi $\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma V^\\pi(s') \\right]\n",
    "$$\n",
    "\n",
    "This equation expresses the value of being in a state $ s $ under policy $ \\pi $ as the expected sum of current and discounted future rewards. -->\n",
    "\n",
    "<!-- --- -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of Dynamic Programming Algorithms\n",
    "\n",
    "**Policy Evaluation**\n",
    "\n",
    "- **Goal**: Evaluate how good a policy is ‚Äî estimate its value function.\n",
    "- **Method**: Iteratively apply the Bellman expectation backup until convergence:\n",
    "  <!-- $$\n",
    "  V_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a)\\left[R_0 + \\gamma V_k(s')\\right]\n",
    "  $$ -->\n",
    "\n",
    "  $$\n",
    "  V(s) \\Leftarrow \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a)\\left[R_{t+1} + \\gamma V(s')\\right]\n",
    "  $$\n",
    "\n",
    "  If the taken action only have one possible next state, then the Bellman expectation backup can be simplified to:\n",
    "\n",
    "  $$\n",
    "  V(s) \\Leftarrow  \\sum_{s'} P(s'|s, a)\\left[R_{t+1} + \\gamma V(s')\\right]\n",
    "  $$\n",
    "\n",
    "\n",
    "**Policy Improvement**\n",
    "\n",
    "- **Goal**: Improve a policy based on its value function.\n",
    "- **Method**: For each state $ s $, choose the action that maximizes expected return:\n",
    "  $$\n",
    "  \\pi'(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a)\\left[R_{t+1} + \\gamma V^\\pi(s')\\right]\n",
    "  $$\n",
    "\n",
    "**Policy Iteration** \n",
    "<!-- or *generalize policy iteration* (GPI) -->\n",
    "\n",
    "- **Goal**: Find the optimal policy through repeated policy evaluation and improvement.\n",
    "- **Steps**:\n",
    "  1. Initialize a random policy $ \\pi_0 $\n",
    "  2. While policy not converged:\n",
    "     - Evaluate $ V^{\\pi}(s) $\n",
    "     - Improve $ \\pi $ using $ V^{\\pi} $\n",
    "- **Result**: Converges to the **optimal policy** $ \\pi^* $\n",
    "\n",
    "**Value Iteration**\n",
    "\n",
    "- **Goal**: Compute the optimal value function directly without needing to evaluate intermediate policies.\n",
    "- **Method**:\n",
    "  <!-- $$\n",
    "  V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a)\\left[R_0 + \\gamma V_k(s')\\right]\n",
    "  $$ -->\n",
    "\n",
    "  $$\n",
    "  V(s) \\Leftarrow \\max_a \\sum_{s'} P(s'|s,a)\\left[R_{t+1} + \\gamma V(s')\\right]\n",
    "  $$\n",
    "- After convergence, extract the optimal policy:\n",
    "  $$\n",
    "  \\pi^*(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a)\\left[R_{t+1} + \\gamma V^*(s')\\right]\n",
    "  $$\n",
    "\n",
    "\n",
    "Advantages of Dynamic Programming\n",
    "\n",
    "| Advantage | Description |\n",
    "|----------|-------------|\n",
    "| **Guaranteed Convergence** | If the MDP is finite and the updates are done correctly, DP converges to the optimal solution |\n",
    "| **Mathematically Exact** | Solves Bellman equations exactly (within numerical precision) |\n",
    "| **Basis for Approximate Methods** | Many modern RL algorithms (like Q-learning, DQN) are inspired by DP concepts |\n",
    "\n",
    "\n",
    "Disadvantages of Dynamic Programming\n",
    "\n",
    "| Disadvantage | Description |\n",
    "|--------------|-------------|\n",
    "| **Requires Full Model** | Needs full knowledge of transitions and rewards ‚Äî rarely available in real-world scenarios |\n",
    "| **Computationally Expensive** | Infeasible for large state spaces (curse of dimensionality) |\n",
    "| **Not Suitable for Partial Observability** | Assumes the environment is fully observable (MDP), not applicable to POMDPs |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from pprint import pprint\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Example of a grid world environment\n",
    "# Method: Every Visit\n",
    "\n",
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.n_episodes = 1000\n",
    "        self.n_states = 16\n",
    "        self.n_actions = 4\n",
    "        self.gamma = 0.9\n",
    "        self.theta = 1e-6\n",
    "        self.target_state = (0, 2)\n",
    "        self.value = np.zeros((self.n_states//4, self.n_states//4))\n",
    "        self.prob = np.ones((self.n_states//4, self.n_states//4, self.n_actions), \n",
    "                            dtype=np.int32)\n",
    "        self.states = [(i, j) for i in range(4) for j in range(4)]\n",
    "        self.actions = np.arange(self.n_actions)\n",
    "        self.blocked_states_01 = [(0, 1), (0, 2), (0, 3)]\n",
    "        self.blocked_states_02 = [(2, 0), (2, 1), (2, 2)]\n",
    "        self.state = (3, 0)\n",
    "        self.rewards = np.ones((self.n_states//4, self.n_states//4), dtype=np.int32) * -1\n",
    "        self.rewards[self.target_state] = 5\n",
    "        # for state in self.blocked_states_01:\n",
    "        #    self.rewards[state] = -10\n",
    "        # for state in  self.blocked_states_02:\n",
    "        #    self.rewards[state] = -10\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = (3, 0)\n",
    "        # self.value = np.zeros((self.n_states//5, self.n_states//5))\n",
    "        # return self.state\n",
    "    \n",
    "    def show_action(self, action:int):\n",
    "        if action == 0: \n",
    "            return \"‚Üí\"\n",
    "        elif action == 1: \n",
    "            return \"‚Üê\"\n",
    "        elif action == 2: \n",
    "            return \"‚Üì\"\n",
    "        elif action == 3: \n",
    "            return \"‚Üë\"\n",
    "\n",
    "    def policy(self):\n",
    "        return random.choice(range(4))\n",
    "        \n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0: y += 1  # Right\n",
    "        elif action == 1: y -= 1  # Left\n",
    "        elif action == 2: x += 1  # Down\n",
    "        elif action == 3: x -= 1  # Up\n",
    "        # The the agent keep in the same state if \n",
    "        # the action inplies over the max or min values of the enviroment\n",
    "        next_state = (max(0, min(x, 3)), max(0, min(y, 3)))\n",
    "\n",
    "        # next_reward = 1 if next_state == self.goal else -0.01\n",
    "        # reward = 1 if self.state == self.goal else -0.01\n",
    "        reward = self.rewards[next_state]\n",
    "        # reward = self.rewards[self.state]\n",
    "        # reward = self.rewards[next_state] - 5 if self.state == next_state else self.rewards[next_state]\n",
    "        # reward, next_reward,\n",
    "        # done = next_state == self.goal\n",
    "        return next_state, reward #,  done\n",
    "    \n",
    "    def extract_optimal_policy(self):\n",
    "        data = np.empty_like(self.value)\n",
    "        data = data.astype(np.object_)\n",
    "        for state in self.states:\n",
    "            prob = self.prob[state]/sum(self.prob[state])\n",
    "            action = np.argmax(prob)\n",
    "            data[state] = self.show_action(action)\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_iteration():\n",
    "    env = Env()\n",
    "    converge = False\n",
    "    start = time.time()\n",
    "    \n",
    "    # 1. Policy Evaluation\n",
    "    # Evaluate for all states and actions\n",
    "    while not converge:\n",
    "        \n",
    "        delta = 0\n",
    "        for state in env.states:\n",
    "            values = []\n",
    "            prob = env.prob[state]/np.sum(env.prob[state])\n",
    "            env.state = state\n",
    "            for action in env.actions:\n",
    "                next_state, reward = env.step(action)\n",
    "                values.append(prob[action]*( \n",
    "                    reward + \n",
    "                    env.gamma * env.value[next_state]))\n",
    "            \n",
    "            delta = max(delta, abs(sum(values) - env.value[state]))\n",
    "            env.value[state] = sum(values)\n",
    "        if delta < env.theta:\n",
    "            # print('Break Evaluation')\n",
    "            # break\n",
    "            # 2. Policy Improvement\n",
    "            # 2.1. Update Policy\n",
    "            delta = 0\n",
    "            \n",
    "            for state in env.states:\n",
    "                values = []\n",
    "                prob = env.prob[state]/sum(env.prob[state])\n",
    "                env.state = state\n",
    "                for action in env.actions:\n",
    "                    \n",
    "                    next_state, reward = env.step(action)\n",
    "                    values.append(prob[action]*( \n",
    "                            reward +  \n",
    "                            env.gamma * env.value[next_state]))\n",
    "                action_max = np.argmax(values)\n",
    "                env.prob[state][action_max]+=1\n",
    "                delta = max(delta, abs(\n",
    "                    env.prob[state][action_max]/sum(env.prob[state]) - prob[action_max])\n",
    "                    )\n",
    "            \n",
    "            if delta < 0.001:\n",
    "                # print('Break Policy')\n",
    "                converge=True\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    end = time.time()\n",
    "    print('Time: {:0>2}:{:0>2}:{:0>2}'.format(round((end-start)//60, 0),  round(((end-start)%60)//1, 0), round((end-start)%1, 0)))\n",
    "    return env\n",
    "\n",
    "def get_value_iterarion():\n",
    "    env = Env()\n",
    "    converge = False\n",
    "    \n",
    "    # 1. Policy Evaluation\n",
    "    # Evaluate for all states and actions\n",
    "    start = time.time()\n",
    "    while not converge:\n",
    "        \n",
    "        delta = 0\n",
    "        for state in env.states:\n",
    "            values = []\n",
    "            prob = env.prob[state]/np.sum(env.prob[state])\n",
    "            env.state = state\n",
    "            for action in env.actions:\n",
    "                # env.state = state\n",
    "                next_state, reward = env.step(action)\n",
    "                values.append(prob[action]*( \n",
    "                    reward + \n",
    "                    env.gamma * env.value[next_state]))\n",
    "                \n",
    "            action_max=np.argmax(values)\n",
    "            next_state, reward = env.step(action_max)\n",
    "            value = prob[action_max]*(\n",
    "                    reward + \n",
    "                    env.gamma * env.value[next_state])\n",
    "            \n",
    "            delta = max(delta, abs(value - env.value[state]))\n",
    "            env.value[state] = value\n",
    "            env.prob[state][action_max]+=1\n",
    "        if delta < env.theta:\n",
    "            converge=True\n",
    "            # print('Break Evaluation')\n",
    "            break\n",
    "    end = time.time()\n",
    "    print('Time: {:0>2}:{:0>2}:{:0>2}'.format(int(round((end-start)//60, 0)),  \n",
    "                                               int(round(((end-start)%60)//1,0)), \n",
    "                                               int(round((end-start)%1, 0))))\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.0:0.0:1.0\n",
      "[['‚Üí' '‚Üí' '‚Üë' '‚Üê']\n",
      " ['‚Üí' '‚Üë' '‚Üë' '‚Üë']\n",
      " ['‚Üë' '‚Üë' '‚Üë' '‚Üë']\n",
      " ['‚Üë' '‚Üë' '‚Üë' '‚Üë']]\n",
      "Time: 00:08:01\n",
      "[['‚Üí' '‚Üí' '‚Üë' '‚Üê']\n",
      " ['‚Üë' '‚Üë' '‚Üë' '‚Üê']\n",
      " ['‚Üë' '‚Üë' '‚Üë' '‚Üê']\n",
      " ['‚Üë' '‚Üë' '‚Üë' '‚Üê']]\n"
     ]
    }
   ],
   "source": [
    "policy_iteration = get_policy_iteration()\n",
    "policy_iteration.extract_optimal_policy()\n",
    "\n",
    "value_iteration = get_value_iterarion()\n",
    "value_iteration.extract_optimal_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo\n",
    "\n",
    "- MC doesn't require full knowledge of the environment, only a little prior knowledge about it, then using sampling (implies it has a component of uncertainty) we can create experiences to generate learning.\n",
    "- When using MC in reinforcement learning, we will\n",
    "average the returns for each state-action pair from different episodes\n",
    "- Initial estimations are not good, but they improve as we get more data\n",
    "- Also, we assume that the problem is episodic and an episode will always terminate regardless of the actions taken.\n",
    "\n",
    "#### Sample Average Method\n",
    "\n",
    "The estimated value of action a at time $t$, denoted $Q_t(s, a)$, is computed as the average of all rewards received so far when action $a$ was taken.\n",
    "\n",
    "$Q_t(s, a) = (\\text{Sum of rewards when action a was taken prior to time t}) / (\\text{Number of times action a was taken prior to time t}) $\n",
    "\n",
    "<!-- $$Q_k(s,a) = \\frac{G_1 + G_2 + ..., G_{k-1}}{k-1} $$ -->\n",
    "$$Q_t(s,a) = \\frac{G‚ÇÅ + G‚ÇÇ + ... + G_{t-1}}{N_{t-1}(s,a)}$$\n",
    "\n",
    "‚ùì Why do we say ‚Äúprior to time $t$‚Äù when defining $Q_t(s,a)$?\n",
    "\n",
    "This is not arbitrary ‚Äî it‚Äôs a deliberate convention to ensure we‚Äôre using only past data to make decisions at time $t$.\n",
    "\n",
    "When the agent is about to act at time $t$, it must choose $A_t$ using the knowledge it has so far ‚Äî that is, from time steps $1$ through $t‚àí1$. It hasn‚Äôt seen $R_t$ yet ‚Äî that reward comes after taking the action.\n",
    "\n",
    "Why Use Sample Average?\n",
    "\n",
    "> - *Unbiased Estimator*: By the Law of Large Numbers, as the number of samples ‚Üí ‚àû, Q_t(a) ‚Üí Q*(a).\n",
    "> - *No Hyperparameters (for step size)*: The step size 1/n is determined by the number of samples ‚Äî no need to tune a learning rate.\n",
    "\n",
    "##### Incremental Monte Carlo Prediction\n",
    "\n",
    "Exists a more efficient computational method that allow us get rid of the list of observed returns and simplify the mean calculation step.\n",
    "\n",
    "Using the estimation $Q_t(s,a)$ we can chose the best action $a$ at time step $t$, and then update the estimation $Q_{t+1}(s,a)$ after receiving the reward $R_t$.\n",
    "<!-- Let $Q(S_k, A_k)$ be the estimation of the state-action value after it ( the tuple $(S, A)$) has been selected for $k-1$ times can be rewitten as: -->\n",
    "\n",
    "$$Q‚Çú‚Çä‚ÇÅ(s,a) = Q‚Çú(s,a) + (1/N_{t}(s,a)) √ó (G‚Çú ‚àí Q‚Çú(s,a))$$\n",
    "\n",
    "<!-- $$Q_{k} = \\frac{G_1 + G_2 + ..., G_{k-1}}{k-1} $$ -->\n",
    "\n",
    "<!-- It is only the estimation of $Q_{k}$, since the actual value is $Q_{k} = \\frac{G_1 + G_2 + ..., G_{k}}{k} $ -->\n",
    "\n",
    "<!-- This can be computed by the following:\n",
    "\n",
    "$$Q_{k+1} = \\frac{1}{k} \\sum_{i=1}^{k}G_i$$\n",
    "$$=\\frac{1}{k}(G_k +  \\frac{(k-1)}{(k-1)}\\sum_{i=1}^{k-1}G_i)$$\n",
    "$$=\\frac{1}{k}(G_k +  (k-1)Q_{k})$$\n",
    "$$=Q_{k} + \\frac{1}{k}(G_k - Q_{k})$$\n",
    "\n",
    "More general form: -->\n",
    "\n",
    "$$ \\text{NewEstimate} \\Leftarrow \\text{OldEstimate} + \\text{StepSize}.(\\text{Target} - \\text{OldEstimate}) $$\n",
    "\n",
    "The ‚ÄúStepSize‚Äù is a parameter that controls how fast the estimate is being updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### State-Value Prediction\n",
    "\n",
    "*Task*: estimate the state-value function for a given policy $œÄ$, that means averaging the returns from a particular policy. But we can use incremental update to estimate the state-value function.\n",
    "\n",
    "There are two types of estimations, *First-visit MC* and *every-visit MC*. The *First-visit MC* only considers the return of the first visit to state $s$ in the whole episode, however, every-visit MC considers every visit to state $s$ in the episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy Iteration** (Monte Carlo Control) for MC\n",
    "\n",
    "Policy iteration is a method for finding the optimal policy in a Markov decision process (MDP). It works by iteratively improving the policy until it converges to the optimal policy.\n",
    "\n",
    "*Policy iteration* has two steps:\n",
    "\n",
    "1. *Policy Evaluation*: Compute the value function for the current policy (using sample average method, ).\n",
    "2. *Policy Improvement*: Improve the policy based on the value function.\n",
    "\n",
    "To improve the policy we will use the greedy policy. The greedy policy will always choose the *action* that\n",
    "has maximal action-value function for a given *state*.\n",
    "\n",
    "$$\\pi(s) = \\argmax_a q(s, a) $$\n",
    "\n",
    "For each policy improvement, we will need to construct $œÄ_{t+1}$ based on $\\pi_t$ .\n",
    "\n",
    "\n",
    "$$ \\pi_{t+1}(s) =\\argmax_a q_{\\pi_t}(s, a) $$\n",
    "\n",
    "<!-- $$q_{\\pi_t}(s, \\pi_{t+1}(s)) = \\argmax_a q(s, a) \\ge q_{\\pi_t}(s, \\pi_t(s)) $$ -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from pprint import pprint\n",
    "import time\n",
    "import pandas as pd\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.n_episodes = 1000\n",
    "        self.n_states = 16\n",
    "        self.n_actions = 5\n",
    "        self.gamma = 0.9\n",
    "        self.theta = 1e-6\n",
    "        self.epsilon_decay_rate = 0.995\n",
    "        self.epsilon = 1.0\n",
    "        self.states = [(i, j) for i in range(4) for j in range(4)]\n",
    "        self.actions = np.arange(self.n_actions)\n",
    "        self.target_state = (0, 2)\n",
    "        self.state = (\n",
    "            random.choice(range(0,3)), \n",
    "            random.choice(range(0,3))\n",
    "            )\n",
    "        # (3, 0)\n",
    "        self.q_values = np.zeros((self.n_states//4, self.n_states//4, self.n_actions))\n",
    "        self.counts = np.ones((self.n_states//4, self.n_states//4, self.n_actions))\n",
    "        self.rewards = np.ones((self.n_states//4, self.n_states//4), dtype=np.int32) * -1\n",
    "        self.rewards[self.target_state] = 1\n",
    "        self.selected = np.ones(self.n_actions)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = (\n",
    "            random.choice(range(0,3)), \n",
    "            random.choice(range(0,3))\n",
    "            )\n",
    "    \n",
    "    def show_action(self, action:int):\n",
    "        if action == 0: \n",
    "            return \"‚Üí\"\n",
    "        elif action == 1: \n",
    "            return \"‚Üê\"\n",
    "        elif action == 2: \n",
    "            return \"‚Üì\"\n",
    "        elif action == 3: \n",
    "            return \"‚Üë\"\n",
    "        elif action == 4: \n",
    "            return \"*\"\n",
    "        \n",
    "    def available_actions(self):\n",
    "        x, y = self.state\n",
    "        actions = []\n",
    "        if (y+1)<=3:\n",
    "            actions.append(0)\n",
    "        if (y-1)>=0:\n",
    "            actions.append(1)\n",
    "        if (x+1)<=3:\n",
    "            actions.append(2)\n",
    "        if (x-1)>=0:\n",
    "            actions.append(3)\n",
    "        if self.state == self.target_state:\n",
    "            actions.append(4)\n",
    "        return actions\n",
    "\n",
    "    def policy(self):\n",
    "        actions = self.available_actions()\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(actions)\n",
    "        else:\n",
    "            # Policy Improment (Greedy)\n",
    "            q_values = self.q_values[self.state].copy()\n",
    "            q_values = [q_value if idx in actions else -float('inf') for idx, q_value in enumerate(q_values)]\n",
    "            action = int(\n",
    "                    round(np.argmax(q_values), 0))\n",
    "            # self.selected[action]+=1\n",
    "            # prob = self.selected/self.selected.sum()\n",
    "            return action # np.random.choice(self.actions, p=prob)\n",
    "        \n",
    "    def step(self, action:int):\n",
    "        x, y = self.state\n",
    "        if action == 0: y += 1  # Right\n",
    "        elif action == 1: y -= 1  # Left\n",
    "        elif action == 2: x += 1  # Down\n",
    "        elif action == 3: x -= 1  # Up\n",
    "        elif action == 4: pass # Keep\n",
    "        # The the agent keep in the same state if \n",
    "        # the action inplies over the max or min values of the enviroment\n",
    "        next_state = x, y # (max(0, min(x, 3)), max(0, min(y, 3)))\n",
    "        reward = self.rewards[next_state]\n",
    "        # if x>3 or y>3 or x<0 or y<0:\n",
    "        #    reward-=1\n",
    "        if (self.state == self.target_state) and (action == 4):\n",
    "            reward+=0.5\n",
    "        done = False\n",
    "        if next_state == self.target_state:\n",
    "            done=True\n",
    "        return next_state, reward,  done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['‚Üí' '‚Üí' '*' '‚Üê']\n",
      " ['‚Üë' '‚Üë' '‚Üë' '‚Üë']\n",
      " ['‚Üë' '‚Üí' '‚Üë' '‚Üë']\n",
      " ['‚Üí' '‚Üí' '‚Üë' '‚Üë']]\n"
     ]
    }
   ],
   "source": [
    "env = Env()\n",
    "n_episodes = env.n_episodes\n",
    "trajectory = []\n",
    "\n",
    "while n_episodes > 0:\n",
    "    env.reset()\n",
    "    # state = env.state\n",
    "    while True:\n",
    "        action = env.policy()\n",
    "        # print(env.state, action, env.available_actions())\n",
    "        next_state, reward, done = env.step(action)\n",
    "        trajectory.append((env.state ,action, reward))\n",
    "        env.state = next_state # type: ignore\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Policy Evaluation (Update the Q-values)\n",
    "    G = 0\n",
    "    visited = []\n",
    "    for item in reversed(trajectory):\n",
    "        state, action, reward = item\n",
    "        G = reward + env.gamma * G\n",
    "        if (state, action) not in visited:\n",
    "            env.q_values[state][action] =  (\n",
    "                env.q_values[state][action] + (\n",
    "                    1/env.counts[state][action])*(G - env.q_values[state][action]))\n",
    "            env.counts[state][action] +=1\n",
    "    n_episodes-=1\n",
    "    env.epsilon*=env.epsilon_decay_rate\n",
    "    \n",
    "zeros = np.empty((env.n_states//4, env.n_states//4), dtype=np.object_)\n",
    "\n",
    "for state in product([0, 1, 2, 3],\n",
    "                     [0, 1, 2, 3]):\n",
    "    \n",
    "    env.state = state\n",
    "    actions = env.available_actions()\n",
    "    q_values = env.q_values[state].copy()\n",
    "    q_values = [q_value if idx in actions else -float('inf') for idx, q_value in enumerate(q_values)]\n",
    "    action = int(round(np.argmax(q_values),0))\n",
    "    zeros[state] = env.show_action(action)\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal Difference Learning\n",
    "\n",
    "- Temporal Difference (TD) combines ideas from DP and MC.\n",
    "\n",
    "- Similar to DP, TD use bootstrapping in the estimation (updating estimates based on other estimates (we use $R_{t} + \\gamma V_t({s'})$ instead of $G_t$), rather than waiting for the final outcome), however, like MC, it does not require full knowegde of the enviroment in the learning process, but applies a sampling-based optimization approach.\n",
    "\n",
    "TD utilizes the error, the difference between the *target value* and the *estimate value* at different time step TD(0).\n",
    "\n",
    "$$V_{t}(s) \\leftarrow V_t(s) + \\alpha[R_t(s,a) + \\gamma V_t({s'}) - V_t(s)]$$\n",
    "\n",
    "If we observe carefully, the target value during update for MC is $G_t$ which is known only after one episode, so we need to wait for the final outcome to update the estimation, however, for TD we use bootstrapping (*updating estimates based on other estimates*).\n",
    "\n",
    "An estimate can be an metric that summarize the values of $G$ previously at $t$ and previous episodes.\n",
    "like $R_t(s,s') + \\gamma V_t(s')$. when I take action $a$ at state $s$ we receive the immediate reward $R_t(s,s')$ and move to state $s'$, then we can use the estimation of $V_t(s')$ to estimate the return from state $s'$ and no wait for the final outcome.\n",
    "\n",
    "This reduce variance of the estimator\n",
    "\n",
    "\n",
    "**Sarsa: On-Policy TD Control**\n",
    "\n",
    "The update rule can, therefore, be framed as\n",
    "\n",
    "$$Q_t(s, a) \\leftarrow Q_t(s, a) + \\alpha[R_{t}(s,s') + \\gamma Q_t(s', a') - Q_t(s, a)]$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Aspect</th>\n",
    "            <th>On-Policy (e.g., SARSA)</th>\n",
    "            <th>Off-Policy (e.g., Q-Learning)</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td><strong>Behavior vs. Target Policy</strong></td>\n",
    "            <td>Same policy for learning and acting (&#949-greede policy for both, for example)</td>\n",
    "            <td>Different policies for learning and acting (&#949-greede policy for one and Max for another, for example)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><strong>Exploration</strong></td>\n",
    "            <td>Directly updates using exploratory actions</td>\n",
    "            <td>Updates using the greedy (optimal) action</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><strong>Learning Stability</strong></td>\n",
    "            <td>More stable and robust</td>\n",
    "            <td>Can be unstable without proper safeguards</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><strong>Learning Speed</strong></td>\n",
    "            <td>Slower due to learning from exploratory actions</td>\n",
    "            <td>Faster because it learns the optimal policy</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><strong>Sample Efficiency</strong></td>\n",
    "            <td>Lower</td>\n",
    "            <td>Higher</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><strong>Use Case</strong></td>\n",
    "            <td>Evaluating or improving the current behavior</td>\n",
    "            <td>Learning the best policy, even while exploring</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Learning: Off-Policy TD Control**\n",
    "\n",
    "Q-learning is an off-policy TD method that is very similar to Sarsa and plays\n",
    "an important role in deep reinforcement learning application such as the\n",
    "deep Q-network\n",
    "\n",
    "At step $t$\n",
    "\n",
    "\n",
    "$$Q_t(s, a) \\leftarrow Q_t(s, a) + \\alpha[R_{t}(s,a) + \\gamma \\max_a(Q_t(s', a)) - Q_t(s, a)]$$\n",
    "\n",
    "The main difference that Q-learning has from Sarsa is that the target value now is no longer dependent on the policy being used (how the action is chosen) but only on the state-action function (max)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['‚Üí' '‚Üí' '*' '‚Üê']\n",
      " ['‚Üë' '‚Üë' '‚Üë' '‚Üê']\n",
      " ['‚Üë' '‚Üë' '‚Üë' '‚Üê']\n",
      " ['‚Üí' '‚Üí' '‚Üë' '‚Üê']]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------\n",
    "# ----------- Q-LEARNING IMPLEMENTATION --------------\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Initialize environment and Q-table\n",
    "env = Env()\n",
    "\n",
    "# Initialize Q-table\n",
    "# state:[0, 0, 0, 0], point out the Q-values are\n",
    "# zeros for each state and for each action.\n",
    "# The index of [0, 0, 0, 0] represent the actions\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.1\n",
    "\n",
    "for episode in range(500):\n",
    "    env.reset()\n",
    "    while True:\n",
    "        action = env.policy()\n",
    "        next_state, reward, done = env.step(action)\n",
    "        # 1. Let `state` be the current state\n",
    "        # 2. Based on this we take the action `action`\n",
    "        # 3. This return `next_state` and its `reward` \n",
    "        # 4. Instead take the same action (like SARSA), take the action that max Q\n",
    "        #    not like current policy\n",
    "        env.q_values[env.state][action] += learning_rate * (\n",
    "            reward + env.gamma * max(env.q_values[next_state]) - env.q_values[env.state][action])\n",
    "        env.state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    # print(episode)\n",
    "    env.epsilon*=env.epsilon_decay_rate\n",
    "\n",
    "zeros = np.empty((env.n_states//4, env.n_states//4), dtype=np.object_)\n",
    "\n",
    "for state in product([0, 1, 2, 3],\n",
    "                     [0, 1, 2, 3]):\n",
    "    \n",
    "    env.state = state\n",
    "    actions = env.available_actions()\n",
    "    q_values = env.q_values[state].copy()\n",
    "    q_values = [q_value if idx in actions else -float('inf') for idx, q_value in enumerate(q_values)]\n",
    "    action = int(round(np.argmax(q_values),0))\n",
    "    zeros[state] = env.show_action(action)\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['‚Üí' '‚Üí' '*' '‚Üê']\n",
      " ['‚Üí' '‚Üë' '‚Üë' '‚Üê']\n",
      " ['‚Üë' '‚Üë' '‚Üë' '‚Üë']\n",
      " ['‚Üí' '‚Üë' '‚Üë' '‚Üê']]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------\n",
    "# ----------- SARSA IMPLEMENTATION -------------------\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Initialize environment and Q-table\n",
    "env = Env()\n",
    "\n",
    "# Initialize Q-table\n",
    "# state:[0, 0, 0, 0], point out the Q-values are\n",
    "# zeros for each state and for each action.\n",
    "# The index of [0, 0, 0, 0] represent the actions\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.1\n",
    "\n",
    "for episode in range(500):\n",
    "    env.reset()\n",
    "    state = env.state\n",
    "    while True:\n",
    "        action = env.policy()\n",
    "        next_state, reward, done = env.step(action)\n",
    "        env.state = next_state\n",
    "        next_action = env.policy()\n",
    "        # 1. Let `state` be the current state\n",
    "        # 2. Based on this we take the action `action`\n",
    "        # 3. This return `next_state` and its `reward` \n",
    "        # 4. Instead take the same action (like SARSA), take the action that max Q\n",
    "        #    not like current policy\n",
    "        env.q_values[state][action] += learning_rate * (\n",
    "            reward + env.gamma * env.q_values[next_state][next_action] - env.q_values[state][action])\n",
    "        state= next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    # print(episode)\n",
    "    env.epsilon*=env.epsilon_decay_rate\n",
    "\n",
    "zeros = np.empty((env.n_states//4, env.n_states//4), dtype=np.object_)\n",
    "\n",
    "for state in product([0, 1, 2, 3],\n",
    "                     [0, 1, 2, 3]):\n",
    "    \n",
    "    env.state = state\n",
    "    actions = env.available_actions()\n",
    "    q_values = env.q_values[state].copy()\n",
    "    q_values = [q_value if idx in actions else -float('inf') for idx, q_value in enumerate(q_values)]\n",
    "    action = int(round(np.argmax(q_values),0))\n",
    "    zeros[state] = env.show_action(action)\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2012.13773\n",
    "\n",
    "https://github.com/wassname/rl-portfolio-management?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About *Done*\n",
    "\n",
    "1. *Episode Terminatio*n: It signifies that the current sequence of interactions between the agent and the environment has concluded. An \"episode\" is a complete run from an initial state until a terminal state is reached.\n",
    "\n",
    "2. *Achieving a Goal*: In many tasks, \"done\" means the agent has successfully achieved its objective. For example:\n",
    "\n",
    "    * In a game of Chess, reaching checkmate.\n",
    "\n",
    "    * In a robotic arm task, picking up the object.\n",
    "\n",
    "    * In Frozen Lake, reaching the \"Goal\" (G) tile.\n",
    "\n",
    "3. *Failing the Task*: \"Done\" can also mean the agent has failed or entered an undesirable state, leading to the end of the episode. For example:\n",
    "\n",
    "    * Falling into a hole in Frozen Lake.\n",
    "\n",
    "    * Crashing a self-driving car.\n",
    "\n",
    "4. *Running out of moves or time in a game*.\n",
    "\n",
    "    * Reaching a Maximum Limit: Environments often have a maximum number of steps or a time limit per episode. If the agent hasn't reached a goal or failed state but hits this limit, the episode is also considered \"done.\" This is often distinguished as \"truncated\" in newer Gym/Gymnasium APIs (more on that below).\n",
    "\n",
    "\n",
    "*Done* vs. *Truncated* (in Gymnasium / Newer Gym versions):\n",
    "With the evolution of OpenAI Gym (now Gymnasium), the concept of \"done\" has been split into two distinct signals:\n",
    "\n",
    "* *done* (or terminated): This specifically indicates that the episode ended because the agent reached a natural terminal state (e.g., goal achieved, fell in a hole, crashed).\n",
    "\n",
    "* *truncated*: This indicates that the episode ended because a time limit or step limit was reached, even if the agent was still in a valid (non-terminal) state. The agent could have continued if the limit wasn't imposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, the ultimate goal of the agent is to improve its policy to acquire better rewards. *Policy improvement* in the optimization domain is called *policy optimization*.\n",
    "\n",
    "Apart from some linear or non linear methods, the *parameterization* of *value functions* with *deep neural networks* is one way of achieving **value function approximation**, and it‚Äôs the most popular way in the modern deep reinforcement learning domain.\n",
    "\n",
    "Value function approximation is useful because we cannot always acquire the true value function easily, and\n",
    "actually we cannot get the true function for most cases in practice.\n",
    "\n",
    "The gradient-based optimization methods can be used for improving parameterized policies, usually through the method called **policy gradient** in reinforcement learning terminology.\n",
    "\n",
    "The methods in policy optimization fall into two main categories: **(1) value-based optimization** methods like Q-learning, DQN, etc., which optimize the action-value function to obtain the preferences for the action choice, and **(2) policy-based optimization methods** like REINFORCE, the cross-entropy method, etc., which directly\n",
    "optimize the policy according to the sampled reward values.\n",
    "\n",
    "A combination of these two categories was found to be a more effective approach by people which forms one of the most widely used architecture in model-free reinforcement learning called **actor-critic**. Actor-critic methods employ the optimization of value function as the guidance of policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value-Based Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value-based optimization method always needs to alternate between\n",
    "value function estimation under the current policy and policy improvement with the estimated value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous sections, we see that the Q-learning can be used for\n",
    "solving some simple tasks in reinforcement learning. However, the realworld applications or even the quasi-real-world applications may have\n",
    "much larger and complicated state and action spaces, and the action is\n",
    "usually continuous in practice\n",
    "\n",
    "For example, the Go game has 10170 states. In these cases, the traditional lookup table method in Q-learning cannot\n",
    "work well with the *limitation of its scalability*, because each state will have an entry V‚Äâ(s) and each state-action pair will need an entry Q(s, a). The values in the table are updated one-by-one in practice. Therefore the *requirement of the memory and computational resources will be huge with tabular-based Q-learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply the value-based reinforcement learning in relatively largescale tasks, function approximators are applied to handle the above\n",
    "limitations\n",
    "- **Linear Methods**:\n",
    "    - Linear function approximators are used to approximate the value function, which is a linear combination of the features extracted from the state or state-action pairs.\n",
    "    - The linear function approximator can be expressed as:\n",
    "    \n",
    "    $$\n",
    "    V(s) = \\theta^T \\phi(s)\n",
    "    $$\n",
    "    \n",
    "    where $\\theta$ is the weight vector and $\\phi(s)$ is the feature vector for state $s$.\n",
    "    - Polinomial function approximators are used to approximate the value function, which is a polynomial function of the features extracted from the state or state-action pairs.\n",
    "    - The polynomial function approximator can be expressed as:\n",
    "    $$    V(s) = \\sum_{i=0}^{n} \\theta_i \\phi_i(s)$$\n",
    "\n",
    "    - Foureier basis function approximators are used to approximate the value function, which is a linear combination of the Fourier basis functions.\n",
    "    - The Fourier basis function approximator can be expressed as:\n",
    "    $$    V(s) = \\sum_{i=0}^{n} \\theta_i \\cos(\\omega_i s + \\phi_i)$$\n",
    "    for $s‚Äâ‚àà‚Äâ[0, 1]$ and $i‚Äâ=‚Äâ0, ‚Ä¶, n$\n",
    "\n",
    "    - Coarse coding is a method that uses a coarse representation of the state space to approximate the value function. It is used to reduce the dimensionality of the state space and make the value function approximation more efficient.\n",
    "    - The coarse coding method can be expressed as:\n",
    "    $$    V(s) = \\sum_{i=0}^{n} \\theta_i \\phi_i(s)$$\n",
    "    where $\\phi_i(s)$ is the coarse representation of the state $s$ and $\\theta_i$ is the weight vector. And the coase representation is defined as:\n",
    "    $$\\phi_i(s) = \\begin{cases}\n",
    "        1 & \\text{if } s \\in C_i \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases}$$\n",
    "    \n",
    "    - Radial basis functions (RBF) are used to approximate the value function, which is a linear combination of the radial basis functions.\n",
    "    - The RBF function approximator can be expressed as:\n",
    "    $$    V(s) = \\sum_{i=0}^{n} \\theta_i \\phi_i(s)$$\n",
    "    where $\\phi_i(s)$ is the radial basis function for state $s$ and $\\theta_i$ is the weight vector. The radial basis function is defined as:\n",
    "    $$\\phi_i(s) = \\exp\\left(-\\frac{||s - c_i||^2}{2\\sigma_i^2}\\right)$$\n",
    "    where $c_i$ is the center of the radial basis function and $\\sigma_i$ is the width of the radial basis function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Non-linear methods**\n",
    "  - Artificial neural networks: different from the above function approximation methods, artificial neural networks are widely used as\n",
    "non-linear function approximators, which are proven to have universal approximation ability under certain conditions.\n",
    "\n",
    "* **Other methods**\n",
    "  - Decision trees: decision trees can be used to approximate the value function by partitioning the state space into different regions and assigning a value to each region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of using function approximation:\n",
    "- **Scalability**: Function approximation can handle large state and action spaces, which is essential for real-world applications.\n",
    "- **Generalization**: Function approximation can generalize the value function to unseen states, which is crucial for tasks with continuous or high-dimensional state spaces.\n",
    "- **Efficiency**: Function approximation can reduce the memory and computational requirements, making it feasible to apply reinforcement learning to complex tasks.\n",
    "\n",
    "(ANN  also redice the need for manually defined features, which is a common requirement in traditional machine learning methods)\n",
    "\n",
    "Types of Value-Based Optimization Methods\n",
    "- **Model-free methods**: These methods do not require a model of the environment and learn the value function directly from the data. They are often used in reinforcement learning tasks where the environment is complex and difficult to model. The parameters w of the approximators can be updated with Monte Carlo (MC) or TD learning\n",
    "- **Model-based methods**: These methods require a model of the environment and use it to learn the value function. They are often used in reinforcement learning tasks where the environment is simple and easy to model.\n",
    "\n",
    "\n",
    "Model-free agents learn the optimal policy through trial and error, while model-based agents first learn a model of the environment and then use that model to plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *most practical approximation method* for present DRL algorithms is using the *neural network*, for its great **scalability** and **generalization** for\n",
    "various specific functions. \n",
    "\n",
    "A neural network is a *differential method* with *gradient-based optimization*, which has a guarantee of *convergence* to optimum within convex cases and can achieve near-optimal solutions for some non-convex functions approximation.\n",
    "\n",
    "Drawbacks: it may require a large amount of data for training in practice and may cause other difficulties.\n",
    "\n",
    "Challenges of useing Value Function Approximation\n",
    "- Most supervised learning methods are not suitable for reinforcement learning tasks, because the data is not independent and identically distributed (i.i.d.) and the data is not labeled.\n",
    "- The data is generated by the agent's interaction with the environment, which is not i.i.d. (The next row depends on the previous rows).\n",
    "- The data is not stationary, which means the distribution of the data changes over time. The value function is estimated with current policy, or at least the state-visitfrequency determined by current policy, and the policy is updated all the\n",
    "time during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient-Based Value Function Approximation\n",
    "\n",
    "Considering the value function is parameterized as \n",
    "\n",
    "$$\n",
    "V^{\\pi}(s) = V^{\\pi}(s;\\theta)\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s,a) = Q^{\\pi}(s,a;\\theta)\n",
    "$$\n",
    "\n",
    "The optimization objective is set to MSE of approximated value function ($V^{\\pi}(s;\\theta)$ or $Q^{\\pi}(s,a;\\theta)$) and target value function ($V_{\\pi}(s)$ or $Q_{\\pi}(s,a)$)\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\pi} \\left[ (V^\\pi(s;\\theta) - V_\\pi(s))^2 \\right]\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\pi} \\left[ (Q^\\pi(s,a;\\theta) - Q_\\pi(s,a))^2 \\right]\n",
    "$$\n",
    "\n",
    "Therefore, the gradient of the objective function is given by\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\alpha (Q^\\pi(s,a;\\theta) - Q_\\pi(s,a)) \\nabla Q^\\pi(s,a;\\theta) \n",
    "$$\n",
    "or \n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\alpha (V^\\pi(s;\\theta) - V_\\pi(s)) \\nabla V^\\pi(s;\\theta) \n",
    "$$\n",
    "\n",
    "where the gradients are estimated with each sample in the batch and the\n",
    "weights are updated in a stochastic manner. \n",
    "\n",
    "The target (true) value\n",
    "functions $V^\\pi(s)$ or $Q^\\pi(s,a)$ in above equations are usually estimated, sometimes\n",
    "with a target network (DQN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Q-Network\n",
    "\n",
    "Let us consider the function approximation in Q-learning with some\n",
    "parameter $Œ∏$.\n",
    "\n",
    "The update rule  can be expressed as\n",
    "\n",
    "$$y = R(s,a) + Œ≥ \\max_{a'} Q(s',a';Œ∏)$$\n",
    "\n",
    "$$L(Q(s,a;\\theta), y)= \\frac{1}{2}(y - Q(s, a;\\theta))^2$$\n",
    "<!-- $$\\theta \\leftarrow \\argmin_{\\theta} L(Q(s,a;\\theta), y) $$ -->\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla L  $$\n",
    "\n",
    "where $y$ is the target Q-value and $L$ is the loss function, usually MSE.\n",
    "\n",
    "**Deep Q-Learning (DQL)**\n",
    "\n",
    "Deep Q-Learning replace Q-Table with a Deep Neural Network (DNN) that approximates the *Q-function*\n",
    "\n",
    "Instead storing Q-values in a table, a NN takes a state a input and output Q-values for all possibles actions.\n",
    "\n",
    "$$Q(s,a; \\theta)$$\n",
    "\n",
    "where $\\theta$ represent the weights.\n",
    "\n",
    "DQL Algorithm steps:\n",
    "1. Initialize a NN $Q(s, a; \\theta)$ randomly\n",
    "2. For each episode:\n",
    "   * Observe the current state $s$\n",
    "   * Choose an action using an $\\epsilon -\\text{greedy}\\; \\text{policy}$ (explotation vs exploration)\n",
    "   * Excecute $a$, receive reward $r$ and the next state $s'$ \n",
    "   * Sample a batch from the Replay Buffer.\n",
    "   * Compute the Target-Q-value: $y = r + \\gamma \\max_{a'} Q(s', a';\\theta)$\n",
    "   * Compute the Loss: $L = (y - Q(s, a;\\theta))^2$\n",
    "   * Update NN weights $\\theta$ using backpropagation\n",
    "   * Periodically update target NN $Q(s, a;\\theta)$ to stabilize learning.\n",
    "\n",
    "\n",
    "> **Replay Buffer**: A memory that stores past experiences $(s, a, r, s')$ to break correlation between consecutive samples and improve learning stability.It draws a mini-batch of samples from this buffer uniformly to apply the Q-learning update\n",
    "\n",
    "> **Target Network**: A separate NN $Q(s, a; \\theta^-)$ with weights $\\theta^-$ that are periodically updated from the main network. This helps to stabilize training by providing a fixed target for a number of steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvements over basic DQL:\n",
    "- Dueling DQN\n",
    "- Double DQN\n",
    "- Prioritized Experience Replay\n",
    "- Noisy Nets\n",
    "- Distributional RL\n",
    "- Rainbow DQN (combines several improvements)\n",
    "- Multi-step Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# -------------------------\n",
    "# Preprocessing wrapper\n",
    "# -------------------------\n",
    "class PreprocessFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(PreprocessFrame, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        # Convert to grayscale and resize\n",
    "        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "        obs = cv2.resize(obs, (84, 110))\n",
    "        obs = obs[18:102, :]   # crop\n",
    "        obs = np.expand_dims(obs, -1)\n",
    "        return obs.astype(np.uint8)\n",
    "\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        super(FrameStack, self).__init__(env)\n",
    "        self.k = k\n",
    "        self.frames = collections.deque(maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(shp[0], shp[1], k), dtype=np.uint8)\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()[0]\n",
    "        obs = self.env.observation(obs)\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        obs = self.env.observation(obs)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_obs(), reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.concatenate(list(self.frames), axis=-1)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Replay Buffer\n",
    "# -------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "        return (np.array(states),\n",
    "                np.array(actions),\n",
    "                np.array(rewards, dtype=np.float32),\n",
    "                np.array(next_states),\n",
    "                np.array(dones, dtype=np.uint8))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Deep Q-Network\n",
    "# -------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        c, h, w = input_shape\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self._feature_size(input_shape), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _feature_size(self, shape):\n",
    "        o = torch.zeros(1, *shape)\n",
    "        return self.net[:-3](o).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Training Loop\n",
    "# -------------------------\n",
    "def train_dqn(env, num_episodes=1000, buffer_size=100000, batch_size=32,\n",
    "              gamma=0.99, lr=1e-4, start_eps=1.0, end_eps=0.1,\n",
    "              eps_decay=1000000, target_update=1000):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    state_shape = (4, 84, 84)  # 4 stacked frames\n",
    "    policy_net = DQN(state_shape, n_actions).to(device)\n",
    "    target_net = DQN(state_shape, n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "\n",
    "    steps_done = 0\n",
    "\n",
    "    def select_action(state, eps_threshold):\n",
    "        if random.random() < eps_threshold:\n",
    "            return random.randrange(n_actions)\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(np.array([state]), device=device, dtype=torch.float32).permute(0,3,1,2)\n",
    "            return policy_net(state).argmax(1).item()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            eps_threshold = end_eps + (start_eps - end_eps) * \\\n",
    "                            np.exp(-1.0 * steps_done / eps_decay)\n",
    "\n",
    "            action = select_action(state, eps_threshold)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps_done += 1\n",
    "\n",
    "            # Train\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                states_v = torch.tensor(states, device=device, dtype=torch.float32).permute(0,3,1,2)\n",
    "                actions_v = torch.tensor(actions, device=device, dtype=torch.int64)\n",
    "                rewards_v = torch.tensor(rewards, device=device, dtype=torch.float32)\n",
    "                next_states_v = torch.tensor(next_states, device=device, dtype=torch.float32).permute(0,3,1,2)\n",
    "                dones_t = torch.tensor(dones, device=device, dtype=torch.float32)\n",
    "\n",
    "                state_action_values = policy_net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "                next_state_values = target_net(next_states_v).max(1)[0]\n",
    "                expected_state_action_values = rewards_v + gamma * next_state_values * (1 - dones_t)\n",
    "\n",
    "                loss = nn.MSELoss()(state_action_values, expected_state_action_values.detach())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Update target net\n",
    "            if steps_done % target_update == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        print(f\"Episode {episode}, reward {total_reward}\")\n",
    "\n",
    "    return policy_net\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode=None)\n",
    "    env = PreprocessFrame(env)\n",
    "    env = FrameStack(env, 4)\n",
    "\n",
    "    trained_model = train_dqn(env, num_episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Loss: 0.0000\n",
      "Episode 10, Loss: 0.0188\n",
      "Episode 20, Loss: 0.0001\n",
      "Episode 30, Loss: 0.0001\n",
      "Episode 40, Loss: 0.0010\n",
      "Episode 50, Loss: 0.0011\n",
      "Episode 60, Loss: 0.0002\n",
      "Episode 70, Loss: 0.0002\n",
      "Episode 80, Loss: 0.0003\n",
      "Episode 90, Loss: 0.0002\n",
      "Episode 100, Loss: 0.0002\n",
      "Episode 110, Loss: 0.0000\n",
      "Episode 120, Loss: 0.0000\n",
      "Episode 130, Loss: 0.0000\n",
      "Episode 140, Loss: 0.0001\n",
      "Episode 150, Loss: 0.0000\n",
      "Episode 160, Loss: 0.0000\n",
      "Episode 170, Loss: 0.0000\n",
      "Episode 180, Loss: 0.0000\n",
      "Episode 190, Loss: 0.0000\n",
      "Episode 200, Loss: 0.0000\n",
      "Episode 210, Loss: 0.0000\n",
      "Episode 220, Loss: 0.0000\n",
      "Episode 230, Loss: 0.0000\n",
      "Episode 240, Loss: 0.0000\n",
      "Episode 250, Loss: 0.0000\n",
      "Episode 260, Loss: 0.0000\n",
      "Episode 270, Loss: 0.0000\n",
      "Episode 280, Loss: 0.0000\n",
      "Episode 290, Loss: 0.0000\n",
      "Episode 300, Loss: 0.0000\n",
      "Episode 310, Loss: 0.0000\n",
      "Episode 320, Loss: 0.0000\n",
      "Episode 330, Loss: 0.0000\n",
      "Episode 340, Loss: 0.0000\n",
      "Episode 350, Loss: 0.0000\n",
      "Episode 360, Loss: 0.0000\n",
      "Episode 370, Loss: 0.0000\n",
      "Episode 380, Loss: 0.0000\n",
      "Episode 390, Loss: 0.0000\n",
      "Episode 400, Loss: 0.0000\n",
      "Episode 410, Loss: 0.0000\n",
      "Episode 420, Loss: 0.0000\n",
      "Episode 430, Loss: 0.0000\n",
      "Episode 440, Loss: 0.0000\n",
      "Episode 450, Loss: 0.0000\n",
      "Episode 460, Loss: 0.0000\n",
      "Episode 470, Loss: 0.0000\n",
      "Episode 480, Loss: 0.0000\n",
      "Episode 490, Loss: 0.0000\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Define Deep Q-Network (DQN)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Custom environment\n",
    "class CustomEnv:\n",
    "    def __init__(self):\n",
    "        self.states = [(i, j) for i in range(5) for j in range(5)]\n",
    "        self.goal = (4, 4)\n",
    "        self.state = (0, 0)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self._state_to_tensor()\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0: y += 1  # Right\n",
    "        elif action == 1: y -= 1  # Left\n",
    "        elif action == 2: x += 1  # Down\n",
    "        elif action == 3: x -= 1  # Up\n",
    "        self.state = (max(0, min(x, 4)), max(0, min(y, 4)))\n",
    "        reward = 1 if self.state == self.goal else -0.01\n",
    "        done = self.state == self.goal\n",
    "        return self._state_to_tensor(), reward, done\n",
    "    \n",
    "    def _state_to_tensor(self):\n",
    "        return torch.tensor([self.state[0] / 4, self.state[1] / 4], dtype=torch.float32)\n",
    "\n",
    "# Hyperparameters\n",
    "epsilon = 0.1  # Exploration factor\n",
    "learning_rate = 0.001\n",
    "discount_factor = 0.9\n",
    "batch_size = 32\n",
    "buffer_capacity = 1000\n",
    "episodes = 500\n",
    "\n",
    "# Initialize environment and networks\n",
    "env = CustomEnv()\n",
    "state_dim = 2  # (x, y) coordinates\n",
    "num_actions = 4\n",
    "\n",
    "policy_net = DQN(state_dim, num_actions)\n",
    "target_net = DQN(state_dim, num_actions)\n",
    "\n",
    "# Both start with the same parameters\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "replay_buffer = deque(maxlen=buffer_capacity)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(range(num_actions))\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = torch.argmax(policy_net(state)).item()\n",
    "        \n",
    "        next_state, reward, done = env.step(action)\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        \n",
    "        # Training the network\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            \n",
    "            states = torch.stack(states)\n",
    "            actions = torch.tensor(actions, dtype=torch.long)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "            next_states = torch.stack(next_states)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32)\n",
    "            # state => Q_values => Q_value of chosen action (target action)\n",
    "            # Similar to Q_values[state][action]\n",
    "            q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "            # state => Q_values => max Q_value \n",
    "            # Similar to max(Q_values[next_state])\n",
    "            \n",
    "            next_q_values = target_net(next_states).max(1)[0]\n",
    "            # As reward and discount_factor are constant the only parameter\n",
    "            # to update is theta\n",
    "            target_q_values = rewards + discount_factor * next_q_values * (1 - dones)\n",
    "            \n",
    "            # Try to fit the parameter in order to q_values => target_q_values.detach()\n",
    "            # as target_q_values contains the target actions\n",
    "            loss = loss_fn(q_values, target_q_values.detach())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Update target network every 10 episodes\n",
    "    if episode % 10 == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        print(f\"Episode {episode}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning with RNNs using PyTorch\n",
    "\n",
    "Reinforcement Learning (RL) combined with Recurrent Neural Networks (RNNs) is powerful for sequential decision-making tasks where the agent needs memory of past states. This combination is useful in environments with partial observability or when actions depend on historical patterns.\n",
    "\n",
    "Use Cases\n",
    "* Trading Algorithms: Remembering market trends over time\n",
    "\n",
    "* Robot Control: Handling sensor inputs with temporal dependencies\n",
    "\n",
    "* Game AI: For games where remembering past states is crucial\n",
    "\n",
    "* Conversational Agents: Maintaining context in dialogues\n",
    "\n",
    "* Inventory Management: Predicting demand patterns over time\n",
    "\n",
    "Implementation Example: Simple Trading Agent\n",
    "Let's implement a trading agent that uses RL with an RNN (LSTM) to decide whether to buy, hold, or sell an asset based on price history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -179.77, Portfolio Value: 20000.00\n",
      "Episode 1, Total Reward: -126.33, Portfolio Value: 20000.00\n",
      "Episode 2, Total Reward: -160.90, Portfolio Value: 10000.00\n",
      "Episode 3, Total Reward: -160.98, Portfolio Value: 10000.00\n",
      "Episode 4, Total Reward: -179.05, Portfolio Value: 20000.00\n",
      "Episode 5, Total Reward: -152.42, Portfolio Value: 20191.61\n",
      "Episode 6, Total Reward: -163.44, Portfolio Value: 20524.05\n",
      "Episode 7, Total Reward: -213.66, Portfolio Value: 20383.07\n",
      "Episode 8, Total Reward: -174.49, Portfolio Value: 10000.00\n",
      "Episode 9, Total Reward: -102.97, Portfolio Value: 10000.00\n",
      "Episode 10, Total Reward: -170.37, Portfolio Value: 20000.00\n",
      "Episode 11, Total Reward: -160.51, Portfolio Value: 10000.00\n",
      "Episode 12, Total Reward: -146.67, Portfolio Value: 10000.00\n",
      "Episode 13, Total Reward: -132.03, Portfolio Value: 20000.00\n",
      "Episode 14, Total Reward: -181.49, Portfolio Value: 19814.03\n",
      "Episode 15, Total Reward: -173.33, Portfolio Value: 19814.03\n",
      "Episode 16, Total Reward: -148.65, Portfolio Value: 10000.00\n",
      "Episode 17, Total Reward: -159.85, Portfolio Value: 10000.00\n",
      "Episode 18, Total Reward: -176.26, Portfolio Value: 20000.00\n",
      "Episode 19, Total Reward: -162.86, Portfolio Value: 20000.00\n",
      "Episode 20, Total Reward: -153.42, Portfolio Value: 10000.00\n",
      "Episode 21, Total Reward: -136.24, Portfolio Value: 20000.00\n",
      "Episode 22, Total Reward: -165.87, Portfolio Value: 19814.03\n",
      "Episode 23, Total Reward: -142.17, Portfolio Value: 20191.61\n",
      "Episode 24, Total Reward: -189.05, Portfolio Value: 10000.00\n",
      "Episode 25, Total Reward: -114.10, Portfolio Value: 10000.00\n",
      "Episode 26, Total Reward: -140.31, Portfolio Value: 20187.91\n",
      "Episode 27, Total Reward: -156.22, Portfolio Value: 10000.00\n",
      "Episode 28, Total Reward: -224.41, Portfolio Value: 20000.00\n",
      "Episode 29, Total Reward: -132.41, Portfolio Value: 19814.03\n",
      "Episode 30, Total Reward: -135.51, Portfolio Value: 10000.00\n",
      "Episode 31, Total Reward: -261.91, Portfolio Value: 10000.00\n",
      "Episode 32, Total Reward: -181.08, Portfolio Value: 10000.00\n",
      "Episode 33, Total Reward: -208.32, Portfolio Value: 10000.00\n",
      "Episode 34, Total Reward: -163.53, Portfolio Value: 20000.00\n",
      "Episode 35, Total Reward: -130.63, Portfolio Value: 10000.00\n",
      "Episode 36, Total Reward: -175.84, Portfolio Value: 10000.00\n",
      "Episode 37, Total Reward: -171.02, Portfolio Value: 19814.03\n",
      "Episode 38, Total Reward: -150.61, Portfolio Value: 10000.00\n",
      "Episode 39, Total Reward: -184.84, Portfolio Value: 10000.00\n",
      "Episode 40, Total Reward: -105.25, Portfolio Value: 19814.03\n",
      "Episode 41, Total Reward: -89.29, Portfolio Value: 10000.00\n",
      "Episode 42, Total Reward: -132.32, Portfolio Value: 10000.00\n",
      "Episode 43, Total Reward: -180.35, Portfolio Value: 20000.00\n",
      "Episode 44, Total Reward: -143.12, Portfolio Value: 10000.00\n",
      "Episode 45, Total Reward: -127.30, Portfolio Value: 20000.00\n",
      "Episode 46, Total Reward: -132.78, Portfolio Value: 20000.00\n",
      "Episode 47, Total Reward: -128.66, Portfolio Value: 20191.61\n",
      "Episode 48, Total Reward: -175.09, Portfolio Value: 10000.00\n",
      "Episode 49, Total Reward: -72.87, Portfolio Value: 10000.00\n",
      "Episode 50, Total Reward: -120.80, Portfolio Value: 10000.00\n",
      "Episode 51, Total Reward: -94.60, Portfolio Value: 20000.00\n",
      "Episode 52, Total Reward: -173.70, Portfolio Value: 20474.69\n",
      "Episode 53, Total Reward: -164.60, Portfolio Value: 10000.00\n",
      "Episode 54, Total Reward: -68.34, Portfolio Value: 10000.00\n",
      "Episode 55, Total Reward: -60.05, Portfolio Value: 10000.00\n",
      "Episode 56, Total Reward: -108.85, Portfolio Value: 20383.07\n",
      "Episode 57, Total Reward: -208.61, Portfolio Value: 20000.00\n",
      "Episode 58, Total Reward: -180.16, Portfolio Value: 20187.91\n",
      "Episode 59, Total Reward: -213.88, Portfolio Value: 20166.76\n",
      "Episode 60, Total Reward: -157.41, Portfolio Value: 10000.00\n",
      "Episode 61, Total Reward: -121.44, Portfolio Value: 20166.76\n",
      "Episode 62, Total Reward: -79.59, Portfolio Value: 20000.00\n",
      "Episode 63, Total Reward: -120.91, Portfolio Value: 10000.00\n",
      "Episode 64, Total Reward: -129.44, Portfolio Value: 20000.00\n",
      "Episode 65, Total Reward: -123.49, Portfolio Value: 10000.00\n",
      "Episode 66, Total Reward: -112.37, Portfolio Value: 10000.00\n",
      "Episode 67, Total Reward: -145.26, Portfolio Value: 20187.91\n",
      "Episode 68, Total Reward: -148.51, Portfolio Value: 19814.03\n",
      "Episode 69, Total Reward: -162.25, Portfolio Value: 10000.00\n",
      "Episode 70, Total Reward: -154.84, Portfolio Value: 20000.00\n",
      "Episode 71, Total Reward: -116.55, Portfolio Value: 10000.00\n",
      "Episode 72, Total Reward: -89.65, Portfolio Value: 10000.00\n",
      "Episode 73, Total Reward: -161.41, Portfolio Value: 20000.00\n",
      "Episode 74, Total Reward: -77.17, Portfolio Value: 20187.91\n",
      "Episode 75, Total Reward: -118.35, Portfolio Value: 20187.91\n",
      "Episode 76, Total Reward: -143.14, Portfolio Value: 20166.76\n",
      "Episode 77, Total Reward: -155.39, Portfolio Value: 20000.00\n",
      "Episode 78, Total Reward: -156.00, Portfolio Value: 10000.00\n",
      "Episode 79, Total Reward: -108.02, Portfolio Value: 20166.76\n",
      "Episode 80, Total Reward: -134.26, Portfolio Value: 10000.00\n",
      "Episode 81, Total Reward: -147.22, Portfolio Value: 10000.00\n",
      "Episode 82, Total Reward: -92.94, Portfolio Value: 10000.00\n",
      "Episode 83, Total Reward: -116.51, Portfolio Value: 19814.03\n",
      "Episode 84, Total Reward: -69.37, Portfolio Value: 19814.03\n",
      "Episode 85, Total Reward: -141.02, Portfolio Value: 10000.00\n",
      "Episode 86, Total Reward: -132.28, Portfolio Value: 10000.00\n",
      "Episode 87, Total Reward: -185.67, Portfolio Value: 20000.00\n",
      "Episode 88, Total Reward: -123.47, Portfolio Value: 10000.00\n",
      "Episode 89, Total Reward: -97.64, Portfolio Value: 10000.00\n",
      "Episode 90, Total Reward: -126.90, Portfolio Value: 10000.00\n",
      "Episode 91, Total Reward: -149.88, Portfolio Value: 19814.03\n",
      "Episode 92, Total Reward: -149.10, Portfolio Value: 19814.03\n",
      "Episode 93, Total Reward: -162.39, Portfolio Value: 10000.00\n",
      "Episode 94, Total Reward: -136.29, Portfolio Value: 10000.00\n",
      "Episode 95, Total Reward: -153.65, Portfolio Value: 10000.00\n",
      "Episode 96, Total Reward: -155.12, Portfolio Value: 10000.00\n",
      "Episode 97, Total Reward: -106.66, Portfolio Value: 10000.00\n",
      "Episode 98, Total Reward: -131.77, Portfolio Value: 10000.00\n",
      "Episode 99, Total Reward: -180.68, Portfolio Value: 10000.00\n",
      "Final Portfolio Value: 10000.00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Environment Simulation\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, data_length=1000):\n",
    "        # Generate synthetic price data (sin wave + noise + slight upward trend)\n",
    "        self.prices = [100 + 10 * np.sin(i/20) + np.random.normal(0, 2) + i/100 for i in range(data_length)]\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0: no position, 1: long\n",
    "        self.cash = 10000\n",
    "        self.portfolio_value = 10000\n",
    "        self.done = False\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        # Return last 10 prices and current position\n",
    "        start_idx = max(0, self.current_step - 10)\n",
    "        price_history = self.prices[start_idx:self.current_step]\n",
    "        # Pad with zeros if not enough history\n",
    "        if len(price_history) < 10:\n",
    "            price_history = [0]*(10-len(price_history)) + price_history\n",
    "        return torch.FloatTensor([price_history + [self.position]])\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\" action: 0=hold, 1=buy, 2=sell \"\"\"\n",
    "        current_price = self.prices[self.current_step]\n",
    "        reward = 0\n",
    "        \n",
    "        if action == 1 and self.position == 0:  # Buy\n",
    "            self.position = 1\n",
    "            self.buy_price = current_price\n",
    "            reward = -1  # Small penalty for transaction\n",
    "        \n",
    "        elif action == 2 and self.position == 1:  # Sell\n",
    "            self.position = 0\n",
    "            reward = (current_price - self.buy_price) / self.buy_price * 100  # % return\n",
    "        \n",
    "        # Move to next time step\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= len(self.prices) - 1:\n",
    "            self.done = True\n",
    "        \n",
    "        # Calculate portfolio value\n",
    "        if self.position == 1:\n",
    "            self.portfolio_value = self.cash + (current_price / self.buy_price) * self.cash\n",
    "        else:\n",
    "            self.portfolio_value = self.cash\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        return next_state, reward, self.done, {'portfolio_value': self.portfolio_value}\n",
    "\n",
    "# RNN-based Q-Network\n",
    "class RNNQNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNQNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # x shape: (batch, seq_len, input_size)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        if hidden is None:\n",
    "            h_0 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "            c_0 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "            hidden = (h_0, c_0)\n",
    "        \n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])  # Take last output\n",
    "        return out, hidden\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Training Parameters\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.995\n",
    "TARGET_UPDATE = 10\n",
    "LEARNING_RATE = 0.001\n",
    "MEMORY_CAPACITY = 1000\n",
    "NUM_EPISODES = 100\n",
    "\n",
    "# Initialize environment and networks\n",
    "env = TradingEnvironment()\n",
    "input_size = 11  # 10 prices + position\n",
    "hidden_size = 32\n",
    "output_size = 3  # 3 actions\n",
    "\n",
    "policy_net = RNNQNetwork(input_size, hidden_size, output_size)\n",
    "target_net = RNNQNetwork(input_size, hidden_size, output_size)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "memory = ReplayBuffer(MEMORY_CAPACITY)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    hidden = None\n",
    "    total_reward = 0\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * EPS_DECAY**episode\n",
    "    \n",
    "    while not env.done:\n",
    "        # Select action\n",
    "        if random.random() < eps_threshold:\n",
    "            action = random.randint(0, 2)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values, hidden = policy_net(state.unsqueeze(0), hidden)\n",
    "                action = q_values.argmax().item()\n",
    "        \n",
    "        # Execute action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Store transition\n",
    "        memory.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "        \n",
    "        # Train if enough samples\n",
    "        if len(memory) >= BATCH_SIZE:\n",
    "            transitions = memory.sample(BATCH_SIZE)\n",
    "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "            \n",
    "            batch_state = torch.stack(batch_state)\n",
    "            batch_next_state = torch.stack(batch_next_state)\n",
    "            batch_action = torch.tensor(batch_action, dtype=torch.long)\n",
    "            batch_reward = torch.tensor(batch_reward, dtype=torch.float)\n",
    "            batch_done = torch.tensor(batch_done, dtype=torch.float)\n",
    "            \n",
    "            # Current Q values\n",
    "            current_q, _ = policy_net(batch_state)\n",
    "            current_q = current_q.gather(1, batch_action.unsqueeze(1))\n",
    "            \n",
    "            # Target Q values\n",
    "            with torch.no_grad():\n",
    "                next_q, _ = target_net(batch_next_state)\n",
    "                max_next_q = next_q.max(1)[0]\n",
    "                target_q = batch_reward + GAMMA * max_next_q * (1 - batch_done)\n",
    "            \n",
    "            # Compute loss and optimize\n",
    "            loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Update target network\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    print(f\"Episode {episode}, Total Reward: {total_reward:.2f}, Portfolio Value: {env.portfolio_value:.2f}\")\n",
    "\n",
    "# Test the trained agent\n",
    "state = env.reset()\n",
    "hidden = None\n",
    "portfolio_values = []\n",
    "\n",
    "while not env.done:\n",
    "    with torch.no_grad():\n",
    "        q_values, hidden = policy_net(state.unsqueeze(0), hidden)\n",
    "        action = q_values.argmax().item()\n",
    "    \n",
    "    state, _, _, info = env.step(action)\n",
    "    portfolio_values.append(info['portfolio_value'])\n",
    "\n",
    "print(f\"Final Portfolio Value: {portfolio_values[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy-Based Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of Policy-Based and Value-Based Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning\n",
    "https://arxiv.org/pdf/1803.11485"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Reinforcement Learning 5*: Function Approximation and Deep Reinforcement Learning:\n",
    "    https://www.youtube.com/watch?v=wAk1lxmiW4c&list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb&index=5\n",
    "- *Reinforcement Learning 10*: Classic Games Case Study:\n",
    "    https://www.youtube.com/watch?v=ld28AU7DDB4\n",
    "\n",
    "- *Monte Carlo Tree Search*: https://www.youtube.com/results?search_query=mc+tree+search\n",
    "\n",
    "- *AlphaZero*: https://github.com/michaelnny/alpha_zero\n",
    "\n",
    "- *Actor-Critic*: https://www.youtube.com/watch?v=K2qjAixgLqk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search (MCTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo Tree Search (MCTS) is a **heuristic search algorithm** for decision-making in environments with large or infinite state spaces, particularly effective in **games with perfect information** (like Go, Chess, Hex) and increasingly used in imperfect information games and general planning problems.\n",
    "\n",
    "> In Reinforcement Learning (RL), planning problems refer to cases where the model of the environment is known ‚Äî i.e., you know the transition probabilities $ùëÉ(ùë†‚Ä≤‚à£ùë†,ùëé)P(s‚Ä≤‚à£s,a)$ and the reward function $ùëÖ(ùë†,ùëé)$.\n",
    "\n",
    "\n",
    "Instead of exhaustively exploring the entire game tree (which is infeasible for complex games), MCTS **selectively builds a search tree** by:\n",
    "1. **Randomly simulating** playouts from the current state\n",
    "2. **Gradually focusing** on the most promising branches using statistical estimates\n",
    "3. **Balancing exploration vs. exploitation** using the **Upper Confidence Bound (UCB)** principle\n",
    "\n",
    "\n",
    "**The Four Phases of MCTS**\n",
    "\n",
    "MCTS operates in iterative cycles, each consisting of four phases:\n",
    "\n",
    "1. **Selection**\n",
    "- Start from the root node (current game state)\n",
    "- Traverse down the tree by selecting child nodes using a **tree policy** (typically UCB1)\n",
    "- Continue until reaching a **leaf node** (a node that hasn't been fully expanded)\n",
    "\n",
    "**UCB1 Formula** (for maximizing player):\n",
    "```\n",
    "UCB1(node) = Q(node)/N(node) + c * ‚àö(ln(N(parent)) / N(node))\n",
    "```\n",
    "- `Q(node)`: Total reward from simulations through this node\n",
    "- `N(node)`: Number of visits to this node\n",
    "- `c`: Exploration parameter (typically ‚àö2 for games)\n",
    "- The first term = **exploitation** (average win rate)\n",
    "- The second term = **exploration** (uncertainty bonus)\n",
    "\n",
    "2. **Expansion**\n",
    "- If the leaf node represents a **non-terminal state**, add one or more child nodes (legal moves) to the tree\n",
    "- Select one of the newly added children for simulation\n",
    "\n",
    "3. **Simulation (Rollout)**\n",
    "- From the selected node, perform a **random playout** to a terminal state\n",
    "- Use a **default policy** (often completely random moves)\n",
    "- Record the outcome (win/loss/draw or reward value)\n",
    "\n",
    "4. **Backpropagation**\n",
    "- Propagate the simulation result back up the tree to the root\n",
    "- Update statistics for all nodes along the path:\n",
    "  - Increment visit count `N(node)`\n",
    "  - Add simulation result to total reward `Q(node)`\n",
    "\n",
    "\n",
    "Key Properties\n",
    "\n",
    "**Asymmetric Tree Growth**\n",
    "- Unlike minimax, MCTS doesn't explore all branches equally\n",
    "- Focuses computational resources on **promising lines of play**\n",
    "- Tree grows deeper in critical variations\n",
    "\n",
    "**Anytime Algorithm**\n",
    "- Can be stopped at any time and return the best move found so far\n",
    "- Quality of solution improves with more computation time\n",
    "- Perfect for real-time decision making\n",
    "\n",
    "**No Domain Knowledge Required** (in basic form)\n",
    "- Only needs:\n",
    "  - Game rules (to generate legal moves)\n",
    "  - Terminal state detection\n",
    "  - Outcome evaluation\n",
    "- However, **domain-specific enhancements** significantly improve performance\n",
    "\n",
    "\n",
    "**Convergence Properties**\n",
    "- **Theoretical guarantee**: With infinite simulations, MCTS converges to the optimal move\n",
    "- **Practical reality**: Works well with finite resources due to efficient exploration\n",
    "\n",
    "\n",
    "Famous Applications\n",
    "\n",
    " **AlphaGo (2016)**\n",
    "- Combined MCTS with **deep neural networks**\n",
    "- **Policy Network**: Guided tree expansion (reduced branching factor)\n",
    "- **Value Network**: Evaluated positions (reduced need for rollouts)\n",
    "- **Defeated world champion Lee Sedol** in Go\n",
    "\n",
    " **AlphaZero (2017)**\n",
    "- Generalized AlphaGo to **Chess, Shogi, and Go**\n",
    "- Learned entirely through **self-play**\n",
    "- Used MCTS for both **training** (generating data) and **inference** (making moves)\n",
    "\n",
    " **Other Applications**\n",
    "- **Video games**: NPC decision making, strategy games\n",
    "- **Automated planning**: Robotics, logistics\n",
    "- **Theorem proving**: Guiding proof search\n",
    "- **Optimization problems**: Combinatorial optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Enhancements & Variants**\n",
    "\n",
    "\n",
    "**Domain-Specific Improvements**\n",
    "- **Progressive Bias**: Incorporate heuristic evaluation into UCB\n",
    "- **RAVE (Rapid Action Value Estimation)**: Share statistics between similar moves\n",
    "- **Transposition Tables**: Cache previously evaluated positions\n",
    "\n",
    "**Neural Network Integration**\n",
    "- **Neural MCTS**: Use networks to guide selection and evaluation\n",
    "- **Policy Prior**: Replace uniform expansion with learned move probabilities\n",
    "- **Value Estimation**: Replace rollouts with neural network predictions\n",
    "\n",
    "**Parallel MCTS**\n",
    "- **Tree Parallelization**: Multiple threads share the same tree\n",
    "- **Root Parallelization**: Each thread builds separate tree, combine results\n",
    "- **Leaf Parallelization**: Parallelize rollouts from leaf nodes\n",
    "\n",
    "\n",
    "<!-- Advantages vs. Disadvantages\n",
    "\n",
    "**Advantages**\n",
    "‚úÖ **No evaluation function needed** (basic version)  \n",
    "‚úÖ **Anytime algorithm** - interruptible with good results  \n",
    "‚úÖ **Handles large branching factors** better than minimax  \n",
    "‚úÖ **Naturally handles stochastic environments**  \n",
    "‚úÖ **Memory efficient** - only stores explored portion of tree  \n",
    "\n",
    "**Disadvantages**\n",
    "‚ùå **Computationally expensive** per iteration  \n",
    "‚ùå **Can be misled by rare but critical events** (requires many simulations)  \n",
    "‚ùå **Struggles with \"trap\" positions** where one mistake loses immediately  \n",
    "‚ùå **Less effective in games with high randomness** (without modifications)  \n",
    "‚ùå **No guaranteed optimality** with finite computation   -->\n",
    "\n",
    "\n",
    "Modern Developments\n",
    "\n",
    "**MuZero (2019)**\n",
    "- Extends AlphaZero to **unknown environments**\n",
    "- Learns a **world model** (dynamics, reward, representation)\n",
    "- Uses MCTS with learned model for planning\n",
    "\n",
    "**MCTS in Large Language Models**\n",
    "\n",
    "- Used for **reasoning and planning** in LLMs\n",
    "- **Tree of Thoughts** prompting technique\n",
    "- **Self-refine** and **chain-of-thought** improvements\n",
    "\n",
    "**Real-World Planning**\n",
    "- **Autonomous vehicles**: Trajectory planning\n",
    "- **Healthcare**: Treatment planning\n",
    "- **Finance**: Portfolio optimization\n",
    "\n",
    "When to Use MCTS\n",
    "\n",
    "**Use MCTS when:**\n",
    "- State space is too large for exhaustive search\n",
    "- You can simulate the environment cheaply\n",
    "- You need an anytime algorithm\n",
    "- Perfect information is available (or can be handled)\n",
    "- No good heuristic evaluation function exists\n",
    "\n",
    "**Consider alternatives when:**\n",
    "- Computation time is extremely limited\n",
    "- The game has very high randomness\n",
    "- You have a very strong evaluation function (minimax might be better)\n",
    "- Memory is severely constrained\n",
    "\n",
    "<!-- Key Takeaways\n",
    "\n",
    "1. **MCTS is a simulation-based search algorithm** that builds a tree asymmetrically\n",
    "2. **UCB1 balances exploration and exploitation** during tree traversal\n",
    "3. **Four phases**: Selection ‚Üí Expansion ‚Üí Simulation ‚Üí Backpropagation\n",
    "4. **Revolutionized AI in games** through AlphaGo/AlphaZero\n",
    "5. **Highly flexible** and applicable beyond games to general planning problems\n",
    "6. **Combines well with machine learning** for state-of-the-art performance -->\n",
    "\n",
    "MCTS represents a beautiful marriage of **Monte Carlo methods** (random sampling) and **tree search** (structured exploration), making it one of the most important algorithms in modern AI decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure MCTS implementation\n",
    "This code will:\n",
    "1. Define a small Game interface (RoomLog implementation)\n",
    "2. Implement a pure MCTS algorithm (Selection, Expansion, Simulation, Backpropagation)\n",
    "3. Demonstrate a single MCTS decision from the empty board and play one game where MCTS plays vs Random\n",
    "\n",
    "Notes:\n",
    "- This is intentionally small and didactic, not hyper-optimized.\n",
    "- The rollout policy is purely random (pure MCTS).\n",
    "- The UCT formula uses an exploration constant c (default sqrt(2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "import math, random, time\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.dim_aulas = None\n",
    "        self.dim_periodo_franja = None\n",
    "        self.dim_frecuencia = None\n",
    "        self.dim_horario = None\n",
    "        self.items = None\n",
    "        self.items_bimestral = None\n",
    "        self.load_all()\n",
    "\n",
    "    def read_json(self, name: str):\n",
    "        with open(self.path / name, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def dim_aulas_loader(self):\n",
    "        self.dim_aulas = self.read_json(\"dim_aulas.json\")\n",
    "\n",
    "    def dim_periodo_franja_loader(self):\n",
    "        self.dim_periodo_franja = self.read_json(\"dim_periodo_franja.json\")\n",
    "\n",
    "    def dim_frecuencia_loader(self):\n",
    "        self.dim_frecuencia = self.read_json(\"dim_frecuencia.json\")\n",
    "\n",
    "    def dim_horario_loader(self):\n",
    "        self.dim_horario = self.read_json(\"dim_horario.json\")\n",
    "\n",
    "    def items_loader(self):\n",
    "        self.items = self.read_json(\"items.json\")\n",
    "\n",
    "    def items_bimestral_loader(self):\n",
    "        self.items_bimestral = self.read_json(\"items_bimestral.json\")\n",
    "\n",
    "    def load_all(self):\n",
    "        self.dim_horario_loader()\n",
    "        self.dim_aulas_loader()\n",
    "        self.dim_frecuencia_loader()\n",
    "        self.dim_periodo_franja_loader()\n",
    "        self.items_loader()\n",
    "        self.items_bimestral_loader()\n",
    "\n",
    "\n",
    "class RoomLog:\n",
    "    def __init__(self, dataset, sede: str):\n",
    "        self.dataset = dataset\n",
    "        self.sede = sede\n",
    "        self.dim_aulas = dataset.dim_aulas[sede].copy()\n",
    "        self.dim_periodo_franja = dataset.dim_periodo_franja.copy()\n",
    "        self.dim_frecuencia = dataset.dim_frecuencia.copy()\n",
    "        self.dim_horario = dataset.dim_horario.copy()\n",
    "        self.items = dataset.items[sede].copy()\n",
    "        self.items_bimestral = dataset.items_bimestral[sede].copy()\n",
    "        self.roomlog = self.get_roomlog()\n",
    "        self.idx_item = 0\n",
    "        self.n_items = len(self.items)\n",
    "        self.n_assignments = 0\n",
    "        self.stats = {'[Conflict]': 0,\n",
    "                      '[Aforo-Alum<0]': 0,\n",
    "                      '[Aforo=Alumn]|[Aforo-Alumn<=2]': 0,\n",
    "                      '[Aforo-Alumn>2]': 0}\n",
    "        self.n_aulas = len(self.roomlog.keys())\n",
    "\n",
    "    def get_roomlog(self):\n",
    "        # self = env_01\n",
    "        aulas = self.dim_aulas['AULA']\n",
    "        room_log = {}\n",
    "        for aula in aulas:\n",
    "            room_log[aula] = {}\n",
    "            for periodo_franja in self.dim_periodo_franja.keys():\n",
    "                franjas = self.dim_periodo_franja[periodo_franja]['FRANJAS']\n",
    "                dias = self.dim_periodo_franja[periodo_franja]['DIAS']\n",
    "                for dia in dias:\n",
    "                    room_log[aula][dia] = {}\n",
    "                    for franja in franjas:\n",
    "                        room_log[aula][dia][franja] = 0\n",
    "\n",
    "        for item in self.items_bimestral:\n",
    "            # conflict = 0\n",
    "            dias = self.dim_frecuencia[item['FRECUENCIA']]['DIAS']\n",
    "            franjas = self.dim_horario[item['HORARIO']]\n",
    "            for dia in dias:\n",
    "                for franja in franjas:\n",
    "                    room_log[item['AULA']][dia][franja] = 1\n",
    "        return room_log\n",
    "\n",
    "\n",
    "    def step(self, action: int):\n",
    "        if self.idx_item >= self.n_items:\n",
    "            return None, None\n",
    "\n",
    "        item = self.items[self.idx_item].copy()\n",
    "        aula = self.dim_aulas['AULA'][action]\n",
    "        aforo = self.dim_aulas['AFORO'][action]\n",
    "        roomlog = self.roomlog.copy()\n",
    "\n",
    "        if (aforo - item['ALUMN']) < 0:\n",
    "            reward = aforo - item['ALUMN'] - 2\n",
    "            response = '[Aforo-Alum<0]'\n",
    "            \n",
    "        elif ((aforo - item['ALUMN']) >= 0) and ((aforo - item['ALUMN']) <= 2):\n",
    "            reward = 1 + (item['ALUMN']/aforo)\n",
    "            response = '[Aforo=Alumn]|[Aforo-Alumn<=2]'\n",
    "        else:\n",
    "            reward = 0\n",
    "            response = '[Aforo-Alumn>2]'\n",
    "\n",
    "        dias = self.dim_frecuencia[item['FRECUENCIA']]['DIAS']\n",
    "        franjas = self.dim_horario[item['HORARIO']]\n",
    "\n",
    "        for dia in dias:\n",
    "            for franja in franjas:\n",
    "                roomlog[aula][dia][franja] = 1\n",
    "\n",
    "        self.roomlog = roomlog.copy()\n",
    "        self.idx_item += 1\n",
    "        return reward, response\n",
    "\n",
    "    def get_available_actions(self):\n",
    "        if self.idx_item >= self.n_items:\n",
    "            return []\n",
    "\n",
    "        item = self.items[self.idx_item].copy()\n",
    "        aulas = self.dim_aulas['AULA']\n",
    "        roomlog = self.roomlog.copy()\n",
    "        dias = self.dim_frecuencia[item['FRECUENCIA']]['DIAS']\n",
    "        franjas = self.dim_horario[item['HORARIO']]\n",
    "\n",
    "        available = []\n",
    "        for idx, aula in enumerate(aulas):\n",
    "            conflict = []\n",
    "            for dia in dias:\n",
    "                for franja in franjas:\n",
    "                    conflict.append(True if roomlog[aula][dia][franja] == 1 else False)\n",
    "            if not any(conflict):\n",
    "                available.append(idx)\n",
    "        return available\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return self.idx_item >= self.n_items | (len(self.get_available_actions()) == 0)\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, move=None, parent=None, untried_actions=None):\n",
    "        self.move = move                  # the move that led to this node (from parent)\n",
    "        self.parent = parent              # parent node\n",
    "        self.children = []                # list of child nodes\n",
    "        self.w = 0.0                   # number of wins for player_just_moved\n",
    "        self.visits = 0                   # visit count\n",
    "        self.untried_actions = [] if untried_actions is None else untried_actions[:]  # moves not expanded yet\n",
    "        # self.player_just_moved = player_just_moved  # who moved to get to this node\n",
    "\n",
    "    def uct_select_child(self, c_param=math.sqrt(2)):\n",
    "        # Select a child according to UCT (upper confidence bound applied to trees)\n",
    "        # If a child has 0 visits we consider its UCT value infinite to ensure it's visited.\n",
    "        best = max(self.children, key=lambda child: (\n",
    "            float('inf') if child.visits == 0 else\n",
    "            (child.w / child.visits) + c_param * math.sqrt(math.log(self.visits) / child.visits)\n",
    "        ))\n",
    "        return best\n",
    "\n",
    "    def add_child(self, move, untried_actions):\n",
    "        child = Node(move=move, parent=self, untried_actions=untried_actions)\n",
    "        self.untried_actions.remove(move)\n",
    "        self.children.append(child)\n",
    "        return child\n",
    "\n",
    "    def update(self, reward):\n",
    "        self.visits += 1\n",
    "        self.w += reward\n",
    "\n",
    "\n",
    "def UCT(state, iter_max=5000, c_param=math.sqrt(2)):\n",
    "    # PATH = Path(\"project\")\n",
    "    # SEDE = 'Ica'\n",
    "    # iter_max = 5000\n",
    "    # c_param=math.sqrt(2)\n",
    "    # dataset_01 = DataSet(PATH)\n",
    "    # state = RoomLog(dataset_01, SEDE)\n",
    "    root_node = Node(move=None,\n",
    "                     parent=None,\n",
    "                     untried_actions=state.get_available_actions())\n",
    "\n",
    "    idx_item = state.idx_item\n",
    "    roomlog = copy.deepcopy(state.roomlog.copy())\n",
    "\n",
    "    for i in range(iter_max):\n",
    "        # i = 0\n",
    "        rewards = []\n",
    "        node = root_node\n",
    "        state.idx_item = idx_item\n",
    "        state.roomlog = copy.deepcopy(roomlog.copy())\n",
    "\n",
    "        # 1. Selection: descend until we find a node with untried actions or a leaf (no children)\n",
    "        while node.untried_actions == [] and node.children:\n",
    "            node = node.uct_select_child(c_param)\n",
    "            reward, _ = state.step(node.move)\n",
    "            rewards.append(reward)\n",
    "\n",
    "        # 2. Expansion: if we can expand (i.e. state not terminal) pick an untried action\n",
    "        if node.untried_actions:\n",
    "            action = random.choice(node.untried_actions)\n",
    "            reward, _ = state.step(action)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "\n",
    "            child_untried = state.get_available_actions()\n",
    "            node = node.add_child(move=action,\n",
    "                                  untried_actions=child_untried)\n",
    "\n",
    "        # 3. Simulation: play randomly until the game ends\n",
    "        while not state.is_terminal():\n",
    "            possible_moves = state.get_available_actions()\n",
    "            reward, _ = state.step(random.choice(possible_moves))\n",
    "            rewards.append(reward)\n",
    "\n",
    "        # 4. Backpropagation: update node statistics with simulation result\n",
    "        n_items = min(max(state.idx_item, 1), state.n_items)\n",
    "\n",
    "        while node is not None:\n",
    "            node.update(sum(rewards)/n_items)\n",
    "            node = node.parent\n",
    "            # # for reward in rewards:\n",
    "\n",
    "    # return the move that was most visited\n",
    "    best_child = max(root_node.children, key=lambda c: c.visits)\n",
    "    state.idx_item = idx_item\n",
    "    state.roomlog = copy.deepcopy(roomlog.copy())\n",
    "\n",
    "    return best_child.move, root_node, state  # also return root node so the caller can inspect children statistics\n",
    "\n",
    "\n",
    "def UCT_worker(state, iter_max, c_param):\n",
    "    # Clone state for isolation\n",
    "    cloned_state = RoomLog(state.dataset, state.sede)\n",
    "    cloned_state.idx_item = state.idx_item\n",
    "    cloned_state.roomlog = copy.deepcopy(state.roomlog)\n",
    "\n",
    "    move, root, _ = UCT(cloned_state, iter_max=iter_max, c_param=c_param)\n",
    "    return root\n",
    "\n",
    "\n",
    "def parallel_UCT(state, iter_max=5000, c_param=math.sqrt(2), n_workers=4):\n",
    "    # split iterations across workers\n",
    "    iters_per_worker = iter_max // n_workers\n",
    "    roots = []\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = [executor.submit(UCT_worker, state, iters_per_worker, c_param)\n",
    "                   for _ in range(n_workers)]\n",
    "        for f in concurrent.futures.as_completed(futures):\n",
    "            roots.append(f.result())\n",
    "\n",
    "    # merge children statistics from workers\n",
    "    merged_root = Node(move=None, parent=None,\n",
    "                       untried_actions=state.get_available_actions())\n",
    "    move_to_node = {}\n",
    "\n",
    "    for r in roots:\n",
    "        for child in r.children:\n",
    "            if child.move not in move_to_node:\n",
    "                move_to_node[child.move] = Node(move=child.move,\n",
    "                                                parent=merged_root,\n",
    "                                                untried_actions=[])\n",
    "            move_to_node[child.move].visits += child.visits\n",
    "            move_to_node[child.move].w += child.w\n",
    "\n",
    "    merged_root.children = list(move_to_node.values())\n",
    "    best_child = max(merged_root.children, key=lambda c: c.visits)\n",
    "\n",
    "    return best_child.move, merged_root, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    assignamnent =namedtuple('Assignamnent', ['CODIGO_DE_CURSO', 'HORARIO', 'FRECUENCIA', 'AULA', 'ALUMN',\n",
    "       'ASSIGNMENTS'])\n",
    "    # --- Demo: use UCT on an empty RoomLog board ---\n",
    "    path = Path(\"project\")\n",
    "    sede = 'Ica'\n",
    "    dataset_01 = DataSet(path)\n",
    "    state = RoomLog(dataset_01, sede)\n",
    "    n_assigments = 0\n",
    "    n_no_assigments = 0\n",
    "    aulas = []\n",
    "    while state.idx_item < len(state.items):\n",
    "        start_time = time.time()\n",
    "        result = copy.deepcopy(state.items[state.idx_item].copy())\n",
    "        if len(state.get_available_actions()) == 0:\n",
    "            result['ASSIGNMENTS'] = {'AULA': None,\n",
    "                                     'AFORO': None}\n",
    "            \n",
    "            state.idx_item += 1\n",
    "            n_no_assigments +=1\n",
    "        else:\n",
    "            move, root_node, state = UCT(state, iter_max=5000)\n",
    "            # move, root_node, state = parallel_UCT(state, iter_max=5000, n_workers=7)\n",
    "            result['ASSIGNMENTS'] = {'AULA':state.dim_aulas['AULA'][move],\n",
    "                                     'AFORO':state.dim_aulas['AFORO'][move]}\n",
    "            n_assigments+=1\n",
    "            state.step(move)\n",
    "        # print(assignamnent(**result))\n",
    "        aulas.append(assignamnent(**result))\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"> Duration of Searching: {duration:.2f}s\",\n",
    "               f\"\\n # Assigments: {n_assigments} | # No Assigments: {n_no_assigments}\")\n",
    "    # df = pd.DataFrame(aulas)\n",
    "    # df.to_excel('sususu.xlsx', index=False)\n",
    "    # return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Duration of Searching: 14.80s \n",
      " # Assigments: 1 | # No Assigments: 0\n",
      "> Duration of Searching: 18.78s \n",
      " # Assigments: 2 | # No Assigments: 0\n",
      "> Duration of Searching: 17.95s \n",
      " # Assigments: 3 | # No Assigments: 0\n",
      "> Duration of Searching: 18.45s \n",
      " # Assigments: 4 | # No Assigments: 0\n",
      "> Duration of Searching: 16.80s \n",
      " # Assigments: 5 | # No Assigments: 0\n",
      "> Duration of Searching: 11.03s \n",
      " # Assigments: 6 | # No Assigments: 0\n",
      "> Duration of Searching: 11.98s \n",
      " # Assigments: 7 | # No Assigments: 0\n",
      "> Duration of Searching: 11.97s \n",
      " # Assigments: 8 | # No Assigments: 0\n",
      "> Duration of Searching: 10.72s \n",
      " # Assigments: 9 | # No Assigments: 0\n",
      "> Duration of Searching: 10.20s \n",
      " # Assigments: 10 | # No Assigments: 0\n",
      "> Duration of Searching: 10.31s \n",
      " # Assigments: 11 | # No Assigments: 0\n",
      "> Duration of Searching: 9.93s \n",
      " # Assigments: 12 | # No Assigments: 0\n",
      "> Duration of Searching: 9.73s \n",
      " # Assigments: 13 | # No Assigments: 0\n",
      "> Duration of Searching: 9.77s \n",
      " # Assigments: 14 | # No Assigments: 0\n",
      "> Duration of Searching: 9.84s \n",
      " # Assigments: 15 | # No Assigments: 0\n",
      "> Duration of Searching: 9.36s \n",
      " # Assigments: 16 | # No Assigments: 0\n",
      "> Duration of Searching: 9.05s \n",
      " # Assigments: 17 | # No Assigments: 0\n",
      "> Duration of Searching: 9.12s \n",
      " # Assigments: 18 | # No Assigments: 0\n",
      "> Duration of Searching: 9.04s \n",
      " # Assigments: 19 | # No Assigments: 0\n",
      "> Duration of Searching: 8.75s \n",
      " # Assigments: 20 | # No Assigments: 0\n",
      "> Duration of Searching: 8.83s \n",
      " # Assigments: 21 | # No Assigments: 0\n",
      "> Duration of Searching: 9.65s \n",
      " # Assigments: 22 | # No Assigments: 0\n",
      "> Duration of Searching: 9.23s \n",
      " # Assigments: 23 | # No Assigments: 0\n",
      "> Duration of Searching: 9.83s \n",
      " # Assigments: 24 | # No Assigments: 0\n",
      "> Duration of Searching: 9.54s \n",
      " # Assigments: 25 | # No Assigments: 0\n",
      "> Duration of Searching: 9.13s \n",
      " # Assigments: 26 | # No Assigments: 0\n",
      "> Duration of Searching: 7.26s \n",
      " # Assigments: 27 | # No Assigments: 0\n",
      "> Duration of Searching: 6.98s \n",
      " # Assigments: 28 | # No Assigments: 0\n",
      "> Duration of Searching: 6.72s \n",
      " # Assigments: 29 | # No Assigments: 0\n",
      "> Duration of Searching: 6.44s \n",
      " # Assigments: 30 | # No Assigments: 0\n",
      "> Duration of Searching: 6.26s \n",
      " # Assigments: 31 | # No Assigments: 0\n",
      "> Duration of Searching: 6.38s \n",
      " # Assigments: 32 | # No Assigments: 0\n",
      "> Duration of Searching: 6.26s \n",
      " # Assigments: 33 | # No Assigments: 0\n",
      "> Duration of Searching: 6.26s \n",
      " # Assigments: 34 | # No Assigments: 0\n",
      "> Duration of Searching: 6.03s \n",
      " # Assigments: 35 | # No Assigments: 0\n",
      "> Duration of Searching: 5.42s \n",
      " # Assigments: 36 | # No Assigments: 0\n",
      "> Duration of Searching: 4.95s \n",
      " # Assigments: 37 | # No Assigments: 0\n",
      "> Duration of Searching: 4.85s \n",
      " # Assigments: 38 | # No Assigments: 0\n",
      "> Duration of Searching: 4.53s \n",
      " # Assigments: 39 | # No Assigments: 0\n",
      "> Duration of Searching: 4.45s \n",
      " # Assigments: 40 | # No Assigments: 0\n",
      "> Duration of Searching: 4.30s \n",
      " # Assigments: 41 | # No Assigments: 0\n",
      "> Duration of Searching: 4.37s \n",
      " # Assigments: 42 | # No Assigments: 0\n",
      "> Duration of Searching: 4.30s \n",
      " # Assigments: 43 | # No Assigments: 0\n",
      "> Duration of Searching: 4.13s \n",
      " # Assigments: 44 | # No Assigments: 0\n",
      "> Duration of Searching: 4.05s \n",
      " # Assigments: 45 | # No Assigments: 0\n",
      "> Duration of Searching: 3.91s \n",
      " # Assigments: 46 | # No Assigments: 0\n",
      "> Duration of Searching: 3.86s \n",
      " # Assigments: 47 | # No Assigments: 0\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaZero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
